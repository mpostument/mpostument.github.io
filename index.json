[{"content":" ","permalink":"https://mpostument.com/posts/programming/godot/godot_movement_collision/","summary":" ","title":"Godot 2D Basics: Movement, Animation and Collision"},{"content":" ","permalink":"https://mpostument.com/posts/programming/godot/godot_intro/","summary":" ","title":"Godot Intro"},{"content":"Introduction Hello, In this post, I\u0026rsquo;ll guide you through the process of monitoring your Elasticsearch cluster using OpenTelemetry. The best part is that we won\u0026rsquo;t need to modify any Elasticsearch configurations or install additional exporters, as the Otel contrib version already contains an Elasticsearch receiver that we can use. We\u0026rsquo;ll also gather logs and system metrics, building upon what I\u0026rsquo;ve previously shown you in other articles.\nOTEL Version Make sure you have OTEL 0.79.0 installed, as it allows the use of dynamic indexes in Elasticsearch, which will come in handy later on.\nFilelog Receiver for Logs First, let\u0026rsquo;s set up the filelog receiver for handling logs. In the configuration, you\u0026rsquo;ll need to specify which files to read and how to extract the timestamp. Additionally, we\u0026rsquo;ll parse the log messages into separate fields in Elasticsearch.\nfilelog/elastic: include: - /var/log/elasticsearch/*.json include_file_name: false include_file_path: true operators: - type: json_parser timestamp: layout: \u0026#39;2006-01-02T15:04:05.999Z\u0026#39; parse_from: attributes[\u0026#34;@timestamp\u0026#34;] layout_type: gotime - field: attributes[\u0026#34;elasticsearch.index.suffix\u0026#34;] type: add value: elasticsearch - parse_from: body parse_to: attributes type: json_parser In this configuration, we start by specifying the files we want to read. Then, we utilize operators to perform specific tasks:\nThe first operator is used to extract the accurate timestamp from the log entry. This is achieved by specifying the timestamp\u0026rsquo;s layout and identifying the JSON field that represents the timestamp.\nAdditionally, we include an extra field in the log entry with the name \u0026quot;elasticsearch.index.suffix\u0026quot; and set its value to elasticsearch. This field plays a crucial role for the Elasticsearch exporter, as it reads this value and utilizes it in the index name, ensuring proper indexing.\nFinally, the last operator reads the JSON message and creates a field for each field found within the JSON data.\nThis configuration streamlines log processing and ensures that the Elasticsearch index is correctly formed, making it easier to manage and analyze the log data.\nMetrics Receiver Now that the logs are set up, let\u0026rsquo;s move on to metrics. We\u0026rsquo;ll use the Elasticsearch receiver to fetch Elasticsearch metrics. Specify the endpoints, username, password, and other metrics you want to receive in the configuration.\nelasticsearch: nodes: [\u0026#34;_local\u0026#34;] skip_cluster_metrics: false indices: [\u0026#34;_all\u0026#34;] endpoint: https://localhost:9200 username: ELASTIC_USERNAME password: ELASTIC_PASSWORD collection_interval: 10s tls: insecure_skip_verify: true metrics: elasticsearch.index.documents: enabled: true elasticsearch.index.operations.merge.docs_count: enabled: true elasticsearch.index.segments.count: enabled: true elasticsearch.index.segments.size: enabled: true The configuration is straightforward. I have Otel running on every Elasticsearch node, and the endpoint is set to localhost. As my Elasticsearch cluster has authentication enabled, I need to specify the username and password for access.\nnodes: [\u0026quot;local\u0026quot;]: This setting allows Otel to fetch metrics specifically for the local Elasticsearch node. indices: [\u0026quot;_all\u0026quot;]: It instructs Otel to gather data about each index in the cluster. In the metrics dictionary, I can specify additional metrics that I want to receive. However, not all metrics are enabled by default. Since I\u0026rsquo;m using these metrics in my Grafana dashboard, I have explicitly enabled them to ensure they are collected and displayed correctly.\nIf you want to explore which metrics are disabled by default, you can find the information here.\nFeel free to reach out if you need further clarification or assistance with anything else!\nHostmetrics Receiver For instance-level metrics like CPU and memory, we\u0026rsquo;ll use the hostmetrics receiver. Configure it with the desired metrics.\nhostmetrics: collection_interval: 15s scrapers: cpu: disk: filesystem: load: memory: network: paging: process: mute_process_exe_error: true mute_process_io_error: true Exporters Now let\u0026rsquo;s move on to the exporters section. Here, you\u0026rsquo;ll replace the endpoints and credentials with your own. Specify the logs_index, and OTEL will automatically add the value from the elasticsearch.index.suffix log entry to the data stream name.\nexporters: elasticsearch: endpoints: - ELASTIC_ENDPOINT logs_dynamic_index: enabled: true logs_index: logs- password: ELASTIC_PASSWORD user: ELASTIC_USER prometheusremotewrite: auth: authenticator: sigv4auth endpoint: PROMETHEUS_ENDPOINT resource_to_telemetry_conversion: enabled: true timeout: 30s Pipeline Create the OTEL pipeline, which defines where to take the values, performs some processing, and sends them to the exporter.\nservice: pipelines: metrics: receivers: - elasticsearch - hostmetrics processors: - batch exporters: - prometheusremotewrite logs/elasticsearch: exporters: - elasticsearch processors: - batch receivers: - filelog/elasticsearch Complete OTEL Config Here\u0026rsquo;s your complete OTEL configuration, including all the sections we\u0026rsquo;ve covered.\nreceivers: filelog/elastic: include: - /var/log/elasticsearch/*.json include_file_name: false include_file_path: true operators: - type: json_parser timestamp: layout: \u0026#39;2006-01-02T15:04:05.999Z\u0026#39; parse_from: attributes[\u0026#34;@timestamp\u0026#34;] layout_type: gotime - field: attributes[\u0026#34;elasticsearch.index.suffix\u0026#34;] type: add value: elasticsearch - parse_from: body parse_to: attributes type: json_parser elasticsearch: nodes: [\u0026#34;_local\u0026#34;] skip_cluster_metrics: false indices: [\u0026#34;_all\u0026#34;] endpoint: https://localhost:9200 username: ELASTIC_USERNAME password: ELASTIC_PASSWORD collection_interval: 10s tls: insecure_skip_verify: true metrics: elasticsearch.index.documents: enabled: true elasticsearch.index.operations.merge.docs_count: enabled: true elasticsearch.index.segments.count: enabled: true elasticsearch.index.segments.size: enabled: true hostmetrics: collection_interval: 10s scrapers: cpu: disk: filesystem: load: memory: network: paging: process: mute_process_exe_error: true mute_process_io_error: true processors: batch: exporters: elasticsearch: endpoints: - ELASTIC_ENDPOINT logs_dynamic_index: enabled: true logs_index: logs- password: ELASTIC_PASSWORD user: ELASTIC_USER prometheusremotewrite: auth: authenticator: sigv4auth endpoint: PROMETHEUS_ENDPOINT resource_to_telemetry_conversion: enabled: true timeout: 30s service: pipelines: metrics: receivers: - elasticsearch - hostmetrics processors: - batch exporters: - prometheusremotewrite logs/elasticsearch: exporters: - elasticsearch processors: - batch receivers: - filelog/elasticsearch Conclusion With this configuration in place, you should now be able to effectively monitor your Elasticsearch cluster with OpenTelemetry. Happy monitoring!\n","permalink":"https://mpostument.com/posts/programming/observability/otel-elasticsearch/","summary":"Introduction Hello, In this post, I\u0026rsquo;ll guide you through the process of monitoring your Elasticsearch cluster using OpenTelemetry. The best part is that we won\u0026rsquo;t need to modify any Elasticsearch configurations or install additional exporters, as the Otel contrib version already contains an Elasticsearch receiver that we can use. We\u0026rsquo;ll also gather logs and system metrics, building upon what I\u0026rsquo;ve previously shown you in other articles.\nOTEL Version Make sure you have OTEL 0.","title":"Optimizing Elasticsearch Monitoring with OpenTelemetry: A Comprehensive Guide"},{"content":"Hello!\nIn this post, I want to show you how you can monitor Kibana with OpenTelemetry Collector. We will gather two types of data: logs and metrics. Both of them require modifications in the Kibana configuration. For logs, it simply involves changes in the Kibana config file, while for metrics, we would need to install the Prometheus exporter.\nKibana Logs Configuration Let\u0026rsquo;s start with logs. First, we need to update the Kibana config to output logs in JSON format. Here is the config I am using, where logs are written to the /var/log/kibana/ folder in JSON format. I have also implemented a policy that creates a new log file if the current one exceeds 50MB. By default, Kibana stores the last 7 log files, deleting the older ones.\nlogging: appenders: file: type: rolling-file fileName: /var/log/kibana/kibana.log layout: type: json policy: type: size-limit size: 50mb root: appenders: [file] Metrics Configuration To expose Prometheus metrics, we just need to install one plugin. You can install it with a single command. Make sure to replace package_version with the correct Kibana version (e.g., 8.8.1 in my case).\n/usr/share/kibana/bin/kibana-plugin install https://github.com/pjhampton/kibana-prometheus-exporter/releases/download/{{ package_version }}/kibanaPrometheusExporter-{{ package_version }}.zip Kibana is now ready. Let\u0026rsquo;s move on to the OTEL configuration.\nOTEL I am using OTEL 0.79.0 because it allows the use of dynamic indexes in Elasticsearch.\nFilelog Receiver For logs, we are going to use the filelog receiver. In the config, we need to specify which files to read and where to get the timestamp. I am also going to parse the log messages into separate fields in Elasticsearch.\nreceivers: filelog/kibana: include: - /var/log/kibana/*.log include_file_name: false include_file_path: true operators: - type: json_parser timestamp: layout: \u0026#39;2006-01-02T15:04:05.999-07:00\u0026#39; parse_from: attributes[\u0026#34;@timestamp\u0026#34;] layout_type: gotime - field: attributes[\u0026#34;elasticsearch.index.suffix\u0026#34;] type: add value: kibana - parse_from: body parse_to: attributes type: json_parser In this config, I first specify which files to read. Then, I use operators. The first operator is used to extract the correct timestamp from the log entry by specifying the layout and the JSON field representing the timestamp. Next, I add an additional field to the log entry with the name \u0026quot;elasticsearch.index.suffix\u0026quot; and the value kibana. The Elasticsearch exporter will read this field and use the value kibana in the index name. Finally, the last operator reads the JSON message and creates a field for every field in the JSON.\nMetrics Receiver Logs are ready, so let\u0026rsquo;s move to metrics. We are going to use the Prometheus receiver to scrape the Kibana endpoint.\nprometheus/kibana: config: scrape_configs: - job_name: \u0026#39;kibana\u0026#39; scrape_interval: \u0026#39;15s\u0026#39; metrics_path: \u0026#39;_prometheus/metrics\u0026#39; static_configs: - targets: [\u0026#39;localhost:5601\u0026#39;] basic_auth: username: USERNAME password: PASSWORD The config for this receiver is very simple. As the target, I am using localhost because Kibana is running on the same host as OTEL. The metrics path is taken from the plugin documentation. If you have authentication, add your username and password.\nAfter configuring the receivers, we need to configure the exporters. In my case, I use Prometheus and Elasticsearch.\nHostmetrics Receiver I am also using the hostmetrics receiver to get instance-level metrics such as CPU and memory. Here is the config I have, which lists the desired metrics.\nhostmetrics: collection_interval: 15s scrapers: cpu: disk: filesystem: load: memory: network: paging: process: mute_process_exe_error: true mute_process_io_error: true Exporters Here are my exporters. Replace the endpoints and credentials with your own. As for logs_index, I am using the value logs-. I have the hyphen because logs_dynamic_index is enabled, and OTEL will take the value from the elasticsearch.index.suffix log entry and add it to the data stream name. So my final data stream will be logs-kibana.\nexporters: elasticsearch: endpoints: - ELASTIC_ENDPOINT logs_dynamic_index: enabled: true logs_index: logs- password: ELASTIC_PASSWORD user: ELASTIC_USER prometheusremotewrite: auth: authenticator: sigv4auth endpoint: PROMETHEUS_ENDPOINT resource_to_telemetry_conversion: enabled: true timeout: 30s Pipeline Everything is ready, and we can create the OTEL pipeline. The pipeline describes where to take the values, adds some processing, and sends them to the exporter.\nservice: pipelines: metrics: receivers: - prometheus/kibana - hostmetrics processors: - batch exporters: - prometheusremotewrite logs/kibana: exporters: - elasticsearch processors: - batch receivers: - filelog/kibana Complete OTEL Config You can find the complete OTEL config here\nreceivers: filelog/kibana: include: - /var/log/kibana/*.log include_file_name: false include_file_path: true operators: - type: json_parser timestamp: layout: \u0026#34;2006-01-02T15:04:05.999-07:00\u0026#34; parse_from: attributes[\u0026#34;@timestamp\u0026#34;] layout_type: gotime - field: attributes[\u0026#34;elasticsearch.index.suffix\u0026#34;] type: add value: kibana - parse_from: body parse_to: attributes type: json_parser prometheus/kibana: config: scrape_configs: - job_name: \u0026#34;kibana\u0026#34; scrape_interval: \u0026#34;10s\u0026#34; metrics_path: \u0026#34;_prometheus/metrics\u0026#34; static_configs: - targets: [\u0026#34;localhost:5601\u0026#34;] basic_auth: username: anonymous password: anonymous hostmetrics: collection_interval: 10s scrapers: cpu: disk: filesystem: load: memory: network: paging: process: mute_process_exe_error: true mute_process_io_error: true processors: batch: exporters: elasticsearch: endpoints: - ELASTIC_ENDPOINT logs_dynamic_index: enabled: true logs_index: logs- password: ELASTIC_PASSWORD user: ELASTIC_USER prometheusremotewrite: auth: authenticator: sigv4auth endpoint: PROMETHEUS_ENDPOINT resource_to_telemetry_conversion: enabled: true timeout: 30s service: pipelines: metrics: receivers: - prometheus/kibana - hostmetrics processors: - batch exporters: - prometheusremotewrite logs/kibana: exporters: - elasticsearch processors: - batch receivers: - filelog/kibana ","permalink":"https://mpostument.com/posts/programming/observability/otel-kibana/","summary":"Hello!\nIn this post, I want to show you how you can monitor Kibana with OpenTelemetry Collector. We will gather two types of data: logs and metrics. Both of them require modifications in the Kibana configuration. For logs, it simply involves changes in the Kibana config file, while for metrics, we would need to install the Prometheus exporter.\nKibana Logs Configuration Let\u0026rsquo;s start with logs. First, we need to update the Kibana config to output logs in JSON format.","title":"Monitoring Kibana with OpenTelemetry Collector: Gathering Logs and Metrics"},{"content":"Hello!\nRecently, I worked on a task that involved receiving StatsD metrics and sending them to Prometheus. Since I was already using the Otel collector, I decided to utilize it for this task.\nOtelcontrib provides a StatsD receiver (you can find it here: StatsD Receiver). Configuring the receiver itself is a relatively easy task. However, sending the received metrics to Prometheus proved to be more challenging. The reason for this is that the Prometheus Remote Write exporter drops StatsD metrics. Fortunately, I managed to solve this issue and would like to share my findings.\nI attempted to set up Otel with StatsD in Kubernetes using the official Helm chart, as well as on an EC2 instance using RPM. The configuration itself remained the same in both cases. Below, I will share my example of setting it up on an EC2 instance.\nTo make it work, you first need a functioning Otel installation. It\u0026rsquo;s important to note that the StatsD receiver does not support scaling, so you can only have one instance of Otel with StatsD to receive data from a particular service. That\u0026rsquo;s why I added a separate instance of Otel with only the StatsD receiver, away from my other instances of Otel that have scaling enabled.\nThe configuration for the StatsD receiver is straightforward:\nreceivers: statsd: endpoint: 0.0.0.0:8125 is_monotonic_counter: true Next, I wanted to add an exporter such as the Prometheus Remote Write exporter:\nprometheusremotewrite: auth: authenticator: sigv4auth endpoint: https://PROMETHEUS/api/v1/remote_write resource_to_telemetry_conversion: enabled: true timeout: 30s Here is my pipeline configuration:\nmetrics: exporters: - logging - prometheusremotewrite processors: - memory_limiter - batch - cumulativetodelta receivers: - otlp - statsd However, when I checked for data in Prometheus, I found nothing. This is because the Prometheus Remote Write exporter drops all Otel data. As mentioned in the Otel documentation:\n⚠️ Non-cumulative monotonic, histogram, and summary OTLP metrics are dropped by this exporter. I needed a way to make the metrics compatible, and another Otel exporter helped me achieve this. I introduced the Prometheus Exporter to my pipeline. This exporter exports data in the Prometheus format, allowing it to be scraped by a Prometheus server.\nHere is the configuration for the Prometheus exporter:\nprometheus: enable_open_metrics: true endpoint: 0.0.0.0:9999 metric_expiration: 3m namespace: statsd resource_to_telemetry_conversion: enabled: true send_timestamps: true Now that I had the Prometheus exporter, I needed to write data to it by modifying the pipeline. In reality, I needed two pipelines, so I created the first one named \u0026ldquo;statsd\u0026rdquo;:\nmetrics/statsd: exporters: - logging - prometheus processors: - memory_limiter - batch - cumulativetodelta receivers: - statsd This pipeline receives data from StatsD and exports it as Prometheus metrics. I can now access my metrics using \u0026ldquo;MY_IP:9999/metrics\u0026rdquo;. My metrics are now in the Prometheus format. The next step is to scrape them and send them to the Prometheus backend. This can be achieved using the Prometheus receiver and scrape configuration:\nprometheus: config: scrape_configs: - job_name: statsd metric_relabel_configs: - replacement: dev target_label: env scrape_interval: 60s static_configs: - targets: - localhost:9999 Finally, I can build my pipelines, which will perform the following steps: data -\u0026gt; StatsD -\u0026gt; Prometheus endpoint -\u0026gt; scrape job -\u0026gt; Prometheus backend.\nHere is the pipeline configuration in code:\nmetrics/prometheus: exporters: - logging - prometheusremotewrite processors: - memory_limiter - batch receivers: - otlp - prometheus metrics/statsd: exporters: - logging - prometheus processors: - memory_limiter - batch - cumulativetodelta receivers: - statsd The cumulativetodelta processor is required in the StatsD pipeline because it converts monotonic, cumulative sum, and histogram metrics to monotonic delta metrics. Non-monotonic sums and exponential histograms are excluded.\nProcessors also need to be defined in the configuration:\nprocessors: batch: {} cumulativetodelta: null memory_limiter: check_interval: 5s limit_percentage: 80 spike_limit_percentage: 25 The final configuration will look like this:\nexporters: logging: {} prometheusremotewrite: auth: authenticator: sigv4auth endpoint: https://PROMETHEUS/api/v1/remote_write resource_to_telemetry_conversion: enabled: true timeout: 30s prometheus: enable_open_metrics: true endpoint: 0.0.0.0:9999 metric_expiration: 3m namespace: statsd resource_to_telemetry_conversion: enabled: true send_timestamps: true extensions: health_check: {} processors: batch: {} cumulativetodelta: null memory_limiter: check_interval: 5s limit_percentage: 80 spike_limit_percentage: 25 receivers: prometheus: config: scrape_configs: - job_name: opentelemetry-collector metric_relabel_configs: - replacement: statsd target_label: type scrape_interval: 10s static_configs: - targets: - localhost:8888 - job_name: statsd metric_relabel_configs: - replacement: dev target_label: env scrape_interval: 60s static_configs: - targets: - localhost:9999 statsd: endpoint: 0.0.0.0:8125 is_monotonic_counter: true service: extensions: - health_check pipelines: metrics/prometheus: exporters: - logging - prometheusremotewrite processors: - memory_limiter - batch receivers: - otlp - prometheus metrics/statsd: exporters: - logging - prometheus processors: - memory_limiter - batch - cumulativetodelta receivers: - statsd As you can see in this configuration, there is an additional scrape job on port 8888. This is not required for StatsD, but this endpoint exposes Otel metrics. If you want to have the ability to monitor the status of your Otel installation, I suggest adding this scrape job.\n","permalink":"https://mpostument.com/posts/programming/observability/otel-statsd/","summary":"Hello!\nRecently, I worked on a task that involved receiving StatsD metrics and sending them to Prometheus. Since I was already using the Otel collector, I decided to utilize it for this task.\nOtelcontrib provides a StatsD receiver (you can find it here: StatsD Receiver). Configuring the receiver itself is a relatively easy task. However, sending the received metrics to Prometheus proved to be more challenging. The reason for this is that the Prometheus Remote Write exporter drops StatsD metrics.","title":"Integrating StatsD Metrics with Prometheus Using OpenTelemetry"},{"content":"Hello!\nThe topic of today\u0026rsquo;s post is goroutines. We will learn what they are and how to use them.\nGoroutine A Goroutine allows you to run code concurrently. For example, we can call several functions at the same time without waiting for each other. To create a goroutine before calling the function, add the go keyword.\npackage main import \u0026#34;fmt\u0026#34; func main() { go hello() } func hello() { fmt.Println(\u0026#34;Hello!\u0026#34;) } But if run this code, nothing will happen. Because the main function is not going to wait for function hello to complete its work, and it will just close the program. We can add sleep, to wait for the hello function. But this is not the best approach. Because we don\u0026rsquo;t know for sure how long it will take to complete the function. There is another way how the go can wait for goroutine, using WaitGroup.\nWaitGroup WaitGroup allows you to wait for the execution of a goroutine. To do this, we create the variable wg, which has the value sync.WaitGroup{}. And later in the code before calling the goroutine, we call wg.Add(1) to add one goroutine to the waiting list. After the goroutine call, we need to add wg.Wait() to wait for execution. And in the goroutine itself, you need to add wg.Done() to remove the previously added goroutine from the waiting list. And now when we call the code we will get the result.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) var wg = sync.WaitGroup{} func main() { wg.Add(1) go hello() wg.Wait() } func hello() { fmt.Println(\u0026#34;Hello!\u0026#34;) wg.Done() } Mutex Let\u0026rsquo;s try to run two goroutines that will simultaneously write/read from the same variable. In this example, goroutines are executed in a loop. One of the functions increments the value of the counter by 1, and the other print the value.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) var wg = sync.WaitGroup{} var counter = 0 func main() { for i := 0; i \u0026lt; 10; i++ { wg.Add(2) go readCounter() go increment() } wg.Wait() } func increment() { counter++ wg.Done() } func readCounter() { fmt.Println(counter) wg.Done() } After running this code, we will get the following result\n$ go run main.go 0 2 3 5 4 6 6 7 7 9 Not exactly what we expected, and if you restart the code, the result will be different. This is because a race condition occurs and all goroutines try to change one object. If you run the code with the -race key, you will see the following error\n$ go run -race main.go 0 3 1 1 1 1 1 ================== WARNING: DATA RACE Write at 0x000104f55a88 by goroutine 10: main.increment() /Users/maksym/go/src/github.com/mpostument/hello/main.go:22 +0x3c Previous read at 0x000104f55a88 by goroutine 7: main.readCounter() /Users/maksym/go/src/github.com/mpostument/hello/main.go:27 +0x2c Goroutine 10 (running) created at: main.main() /Users/maksym/go/src/github.com/mpostument/hello/main.go:15 +0x50 Goroutine 7 (finished) created at: main.main() /Users/maksym/go/src/github.com/mpostument/hello/main.go:14 +0x44 ================== 3 3 5 Found 1 data race(s) exit status 66 How can this be solved? Using Mutex. We can allow access to our object for only one goroutine. In the example, we will use RWMutex, which allows any number of goroutines to read data and write for only one. Before the goroutine that reads data, add RLock, and before the one that writes, add Lock. In the function itself, after the writing/reading has taken place, remove the lock. And now if we run the code, we will get the expected result.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) var wg = sync.WaitGroup{} var mutex = sync.RWMutex{} var counter = 0 func main() { for i := 0; i \u0026lt; 10; i++ { wg.Add(2) mutex.RLock() go readCounter() mutex.Lock() go increment() } wg.Wait() } func increment() { counter++ mutex.Unlock() wg.Done() } func readCounter() { fmt.Println(counter) mutex.RUnlock() wg.Done() } $ go run -race main.go 0 1 2 3 4 5 6 7 8 9 Video ","permalink":"https://mpostument.com/posts/programming/golang/basics/go-routines/","summary":"Hello!\nThe topic of today\u0026rsquo;s post is goroutines. We will learn what they are and how to use them.\nGoroutine A Goroutine allows you to run code concurrently. For example, we can call several functions at the same time without waiting for each other. To create a goroutine before calling the function, add the go keyword.\npackage main import \u0026#34;fmt\u0026#34; func main() { go hello() } func hello() { fmt.Println(\u0026#34;Hello!\u0026#34;) } But if run this code, nothing will happen.","title":"Goroutines in go"},{"content":"Hello!\nToday\u0026rsquo;s topic is interfaces. Interfaces are a type that defines a collection of a method\u0026rsquo;s signature.\nInterface type Printer interface { Print() string } A type will implement the interface when a Print method is created for it.\ntype Book struct { Name string Author string } type Money int func (b Book) Print() string { return fmt.Sprintf(\u0026#34;%s - %s\u0026#34;, b.Author, b.Name) } func (m Money) Print() string { return fmt.Sprintf(\u0026#34;You have %d dollars\u0026#34;, m) } In this code, we have two types Money and Book that implement the Printer interface because they have a Print method.\nNow how can we use all this? We can create a function that will accept the Printer type as an argument. And since our Money and Books types implement the Printer interface, we can pass it as an argument to this function\nfunc WriteLog(p Printer) { log.Println(p.Print()) } And now we will call all this the main method\nfunc main() { a := Book{ Name: \u0026#34;He Who Fights With Monsters\u0026#34;, Author: \u0026#34;Shirtaloon\u0026#34;, } m := Money(99999999999) WriteLog(a) WriteLog(m) } And as a result, we will get the following\n$ go run main.go 2023/01/15 13:17:01 Shirtaloon - He Who Fights With Monsters 2023/01/15 13:17:01 You have 99999999999 dollars The complete code will look like this\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; ) type Printer interface { Print() string } type Book struct { Name string Author string } type Money int func (b Book) Print() string { return fmt.Sprintf(\u0026#34;%s - %s\u0026#34;, b.Author, b.Name) } func (m Money) Print() string { return fmt.Sprintf(\u0026#34;You have %d dollars\u0026#34;, m) } func main() { a := Book{ Name: \u0026#34;He who fight monsters\u0026#34;, Author: \u0026#34;Shirtaloon\u0026#34;, } m := Money(99999999999) WriteLog(a) WriteLog(m) } func WriteLog(p Printer) { log.Println(p.Print()) } Interfaces can reduce code duplication. They also help with tests, where you can replace the function that connects to the database with special test-only functions that don\u0026rsquo;t perform the actual connection, but return a result as if the connection had occurred.\nVideo ","permalink":"https://mpostument.com/posts/programming/golang/basics/go-interfaces/","summary":"Hello!\nToday\u0026rsquo;s topic is interfaces. Interfaces are a type that defines a collection of a method\u0026rsquo;s signature.\nInterface type Printer interface { Print() string } A type will implement the interface when a Print method is created for it.\ntype Book struct { Name string Author string } type Money int func (b Book) Print() string { return fmt.Sprintf(\u0026#34;%s - %s\u0026#34;, b.Author, b.Name) } func (m Money) Print() string { return fmt.","title":"Interfaces in Go"},{"content":"Hello!\nThe topic of today\u0026rsquo;s post functions in golang.\nFunction In all previous posts, we have used functions multiple times. For example fmt.Println() is a function. Let\u0026rsquo;s create the simplest function. In golang, there is always a main function, which is the entry point to the program. And accordingly, if we want to call our function, we will add it to the main function\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { printText() } func printText() { fmt.Println(\u0026#34;Hello\u0026#34;) } Thus we have two functions main and printText. The printText function is called in the main function. The main function is called when the program starts. Now our function does not accept arguments, let\u0026rsquo;s try to add a string argument to it\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { printText(\u0026#34;Hello!\u0026#34;) } func printText(msg string) { fmt.Println(msg) } If the function accepts an argument, it must be specified when calling the function. Otherwise, there will be an error\n$ go run main.go # command-line-arguments ./main.go:8:2: not enough arguments in call to printText have() want (string) More than one argument can be passed, and if those arguments are of the same type, then the type can be specified only once.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { printText(\u0026#34;Message1\u0026#34;, \u0026#34;Message2\u0026#34;) } func printText(msg, second_message string) { fmt.Println(msg, second_message) } $ go run main.go Message1 Message2 If we have a parameter in the main method and we pass it to a function and then change its value, the original value is not changed because golang makes a copy of it.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { message := \u0026#34;Hello\u0026#34; printText(message) fmt.Println(message) } func printText(message string) { message = \u0026#34;Good Bye\u0026#34; fmt.Println(message) } $ go run main.go Good Bye Hello But if we need to change the original value, we can pass a pointer.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { message := \u0026#34;Hello\u0026#34; printText(\u0026amp;message) fmt.Println(message) } func printText(message *string) { *message = \u0026#34;Bye!\u0026#34; fmt.Println(*message) } $ go run main.go Bye! Bye! If you need to pass many parameters of the same type, you can use ... syntax.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { printText(\u0026#34;Hello\u0026#34;, \u0026#34;Bye\u0026#34;) } func printText(messages ...string) { for _, msg := range messages { fmt.Println(msg) } } $ go run main.go Hello Bye In this case, all the parameters passed to the function will be in the form of a slice.\nReturn value In all previous cases, we printed the value of variables directly in the function. If we need to pass the value to another function, we can use the return keyword. And in the function itself, you need to specify what type will be returned.\npackage main import \u0026#34;fmt\u0026#34; func main() { result := printText(6, 3) fmt.Println(result) } func printText(a, b int) int { return a / b } $ go run main.go 2 Also, if necessary, you can return a pointer.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { r := returnPointer() fmt.Println(*r) } func returnPointer() *int { result := 4 / 2 return \u0026amp;result } $ go run main.go 2 Return multiple values Several values can be returned from the function.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { first, second := returnMultiple() fmt.Println(first, second) } func returnMultiple() (int, int) { result := 4 / 2 secondResult := 6 / 2 return result, secondResult } $ go run main.go 2 3 Return error Often, the function returns an error in addition to the value. And after that, the calling function is checked to see if there was an error.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; ) func main() { r, err := returnMultiple(1, 0) if err != nil { log.Fatalln(err) } fmt.Println(r) } func returnMultiple(a, b int) (int, error) { if b == 0 { return 0, fmt.Errorf(\u0026#34;Cannot divide be zero\u0026#34;) } result := a / b return result, nil } Anonymous functions An anonymous function is a function that does not have a name. It is created in the same way as a normal function, only the only thing is that it does not have a name.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { func() { fmt.Println(\u0026#34;Hello\u0026#34;) }() } $ go run main.go Hello For the function to be called, you need to add another pair of () at the end. Otherwise, we will get an error\n$ go run main.go # command-line-arguments ./main.go:8:2: func() {…} (value of type func()) is not used If the function does not need to be called immediately, it can be assigned to some parameter and called when needed\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { a := func() { fmt.Println(\u0026#34;Hello\u0026#34;) } a() } $ go run main.go Hello Methods A method is also a function, but which belongs to a certain type. For example, we have a structure or our type and we can create methods for them. In this example, we have a Hello structure with two fields. Next, we create a function, but additionally before the name of the function we indicate to which type it belongs, in our case, it is a structure named Hello and with the help of the variable h we can refer to its fields. After we have created an instance of this structure in the main method, we can call our method using VARIABLE.METHOD_NAME()\npackage main import ( \u0026#34;fmt\u0026#34; ) type Hello struct { Name string Message string } func (h Hello) hello() { fmt.Println(h.Message, h.Name) } func main() { a := Hello{ Name: \u0026#34;Maksym\u0026#34;, Message: \u0026#34;Hello\u0026#34;, } a.hello() } $ go run main.go Hello Maksym Video ","permalink":"https://mpostument.com/posts/programming/golang/basics/go-functions/","summary":"Hello!\nThe topic of today\u0026rsquo;s post functions in golang.\nFunction In all previous posts, we have used functions multiple times. For example fmt.Println() is a function. Let\u0026rsquo;s create the simplest function. In golang, there is always a main function, which is the entry point to the program. And accordingly, if we want to call our function, we will add it to the main function\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { printText() } func printText() { fmt.","title":"Go Functions"},{"content":"Hi there!\nIn this programming, we will learn what is pointers in golang. We have already used them several times in previous posts. But let\u0026rsquo;s check them in more detail. Pointer is a parameter that stores the memory address and not the value itself.\nCreate Pointer If we create one parameter and assign it to another, then golang makes a copy of it. And if we change the value of one parameter, the other will remain unchanged\npackage main import \u0026#34;fmt\u0026#34; func main() { var a int = 30 var b int = a fmt.Println(a, b) b = 99 fmt.Println(a, b) } $ go run main.go 30 30 30 99 But we can create a parameter that will be a pointer and will indicate the memory location in which the value of the first parameter is located, and then changing the value of any of them, both will be changed. Pointer is created as follows var VAR_NAME *TYPE\npackage main import \u0026#34;fmt\u0026#34; func main() { var a int = 30 var b *int = \u0026amp;a fmt.Println(a, b) *b = 99 fmt.Println(a, b) } When we specify a value for b, we specify not just the parameter a, but \u0026amp;a, which means that we specify a location in memory and not the value of the parameter. And when we are changing value of b we cannot simply specify b = VALUE, because b contain memory address and not value. To change value we need to add * before parameter name\n$ go run main.go 30 0xc0000180c0 99 0xc0000180c0 As we can see from the output, changing the value of b also changed the value of a. But instead of b, we got not the value, but only the address in memory. To get the value of parameter b you need to use dereferencing.\nDereferencing To get the value, just add * before the parameter name.\npackage main import \u0026#34;fmt\u0026#34; func main() { var a int = 30 var b *int = \u0026amp;a fmt.Println(a, *b) *b = 99 fmt.Println(a, *b) } $ go run main.go 30 30 99 99 Pointer default value When we create a parameter that is a pointer but do not set its initial value, it will be nil\npackage main import \u0026#34;fmt\u0026#34; func main() { var b *int fmt.Println(b) } $ go run main.go \u0026lt;nil\u0026gt; Video ","permalink":"https://mpostument.com/posts/programming/golang/basics/go-pointers/","summary":"Hi there!\nIn this programming, we will learn what is pointers in golang. We have already used them several times in previous posts. But let\u0026rsquo;s check them in more detail. Pointer is a parameter that stores the memory address and not the value itself.\nCreate Pointer If we create one parameter and assign it to another, then golang makes a copy of it. And if we change the value of one parameter, the other will remain unchanged","title":"Pointers"},{"content":"Hi there!\nIn this programming, we will learn what is defer, panic and how to recover from panic\nDefer Defer allows you to postpone the execution of the code and execute it at the moment when there is an exit from the function happening\npackage main import \u0026#34;fmt\u0026#34; func main() { defer fmt.Println(\u0026#34;Hello!\u0026#34;) fmt.Println(\u0026#34;Bye!\u0026#34;) } $ go run main.go Bye! Hello! Although the code was supposed to display ``Hello!first,Bye!was displayed first, and only thenHello!`. This is because we have delayed the execution of this code until the main function exits.\nMost often, defer is used to clean up resources after use. For example, close a database connection or file after use. Of course, this can be done without defer, but in this case, you need to remember to close the connection after finishing work, and the closing will be far from the opening code\npackage main import ( \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; ) func main() { file, err := os.Create(\u0026#34;file.txt\u0026#34;) if err != nil { log.Fatalln(err) } _, err = io.WriteString(file, \u0026#34;Writing text to file\u0026#34;) if err != nil { log.Fatalln(err) } file.Close() } Here we close the file using file.Close() at the end of the code that works with the file. If we add defer, the closing can be added immediately after opening. But it will be called in the same way as in the first option\npackage main import ( \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; ) func main() { file, err := os.Create(\u0026#34;file.txt\u0026#34;) defer file.Close() if err != nil { log.Fatalln(err) } _, err = io.WriteString(file, \u0026#34;Writing text to file\u0026#34;) if err != nil { log.Fatalln(err) } } If we have several defers, they will be called in reverse order\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { defer fmt.Println(1) defer fmt.Println(2) defer fmt.Println(3) } $ go run main.go 3 2 1 Panic panic is used in case an error has occurred and you need to stop the program execution\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { fmt.Println(1) panic(\u0026#34;Critical Error\u0026#34;) fmt.Println(2) } $ go run main.go 1 panic: Critical Error goroutine 1 [running]: main.main() /Users/maksym.postument/go/src/github.com/mpostument/hello/main.go:9 +0x6c exit status 2 Recover recover allows you to recover from panic and not terminate the program. For this, we will make a separate function called handlePanic, in which recover() will be called. And we will call it using defer in the method in which panic can happen.\npackage main import ( \u0026#34;fmt\u0026#34; ) func handlePanic() { a := recover() if a != nil { fmt.Println(\u0026#34;RECOVER\u0026#34;, a) } } func main() { fmt.Println(\u0026#34;Start execution\u0026#34;) runPanic() fmt.Println(\u0026#34;End of execution\u0026#34;) } func runPanic() { defer handlePanic() panic(\u0026#34;Critical Error\u0026#34;) } $ go run main.go Start execution RECOVER Critical Error End of execution After panic happened. The execution of the program continued smoothly despite the panic\nVideo ","permalink":"https://mpostument.com/posts/programming/golang/basics/go-defer-panic/","summary":"Hi there!\nIn this programming, we will learn what is defer, panic and how to recover from panic\nDefer Defer allows you to postpone the execution of the code and execute it at the moment when there is an exit from the function happening\npackage main import \u0026#34;fmt\u0026#34; func main() { defer fmt.Println(\u0026#34;Hello!\u0026#34;) fmt.Println(\u0026#34;Bye!\u0026#34;) } $ go run main.go Bye! Hello! Although the code was supposed to display ``Hello!first,Bye!was displayed first, and only thenHello!","title":"Defer, Panic, and Recovery in Golang"},{"content":"Hi there!\nIn this programming, we will learn how to use loops in golang.\nFor Loop The for loop allows you to perform a certain action as many times as long as the condition returns true.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { for i := 0; i \u0026lt; 6; i++ { fmt.Println(i) } } The loop starts with the word for keyword, then the parameter i is initialized, then the condition when the loop should stop. In this example, the loop will work until i is less than 6. And at the end, we specify that we want to increase i by 1 after each iteration. Well, in the body of the loop, I simply display the value of i.\n$ go run main.go 0 1 2 3 4 5 The i parameter is available only in the body of the loop. If I try to use it outside I get an error\n$ go run main.go # command-line-arguments ./main.go:11:14: undefined: i For i to be used outside the loop, its initialization must be done outside the loop.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { i := 0 for ; i \u0026lt; 6; i++ { fmt.Println(i) } fmt.Printf(\u0026#34;Final I is %v\\n\u0026#34;, i) } $ go run main.go 0 1 2 3 4 5 Final I is 6 ; must remain, otherwise, the next error will occur\n$ go run main.go # command-line-arguments ./main.go:9:17: syntax error: unexpected {, expecting semicolon or newline ./main.go:10:17: syntax error: unexpected newline, expecting { after for clause The part where we increment i using i++ can also be moved to the body of the loop. In this case ; can be removed.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { i := 0 for i \u0026lt; 6 { i++ fmt.Println(i) } fmt.Printf(\u0026#34;Final I is %v\\n\u0026#34;, i) } $ go run main.go 1 2 3 4 5 6 Final I is 6 Also, the increment of i does not necessarily have to be in the form of i++. You can perform other operations. For example, multiplication.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { for i := 0; i \u0026lt; 10; i = i + 1*2 { fmt.Println(i) } } $ go run main.go 0 2 4 6 8 Break and Continue Continue Continue can be used if in the loop we want to skip some iteration under a certain condition. In the following example, we check whether i is an even number, if so, we do not continue this iteration and go to the next one\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { for i := 0; i \u0026lt; 10; i++ { if i%2 == 0 { continue } fmt.Println(i) } } $ go run main.go 1 3 5 7 9 Break The break is used if you need to end the loop under a certain condition. In this example, the loop works as long as i is less than 10. But in the body of the loop, there is a check that if i is equal to 5, then the loop ends immediately\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { for i := 0; i \u0026lt; 10; i++ { if i == 5 { break } fmt.Println(i) } } go run main.go 0 1 2 3 4 Range Loops are also used to iterate over elements in data types such as an array, slice, map, and others. This is done using the range keyword. And has the form for INDEX, VALUE := range SLICE {CODE}.\nRange in array/slice package main import ( \u0026#34;fmt\u0026#34; ) func main() { a := []int{1, 5, 7, 9, 10} for i, v := range a { fmt.Println(i, v) } } $ go run main.go 0 1 1 5 2 7 3 9 4 10 In this example, the index of the element in the slice and the element itself are displayed. If we don\u0026rsquo;t need an index, it should be replaced with _\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { a := []int{1, 5, 7, 9, 10} for _, v := range a { fmt.Println(v) } } $ go run main.go 1 5 7 9 10 If we leave the index parameter in the loop but do not use it in the body of the loop, we will get an error\n$ go run main.go # command-line-arguments ./main.go:10:6: i declared but not used That is why it needs to be replaced with _.\nRange in maps The range loop for the map looks the same as for the slice, with the only difference that there will be a key value instead of an index\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { a := map[string]int{\u0026#34;A\u0026#34;: 1, \u0026#34;B\u0026#34;: 5, \u0026#34;C\u0026#34;: 7, \u0026#34;D\u0026#34;: 9, \u0026#34;E\u0026#34;: 10} for k, v := range a { fmt.Println(k, v) } } $ go run main.go D 9 E 10 A 1 B 5 C 7 Video ","permalink":"https://mpostument.com/posts/programming/golang/basics/go-loops/","summary":"Hi there!\nIn this programming, we will learn how to use loops in golang.\nFor Loop The for loop allows you to perform a certain action as many times as long as the condition returns true.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { for i := 0; i \u0026lt; 6; i++ { fmt.Println(i) } } The loop starts with the word for keyword, then the parameter i is initialized, then the condition when the loop should stop.","title":"Loops in golang"},{"content":"Hello!\nThe topic of today\u0026rsquo;s post is If and Switch statements in go.\nIf The if block is executed if the condition returns true.\npackage main import \u0026#34;fmt\u0026#34; func main() { if true { fmt.Println(\u0026#34;Condition is true\u0026#34;) } if false { fmt.Println(\u0026#34;Condition is false\u0026#34;) } } $ go run main.go Condition is true In this case, only Conditions is true was printed because the condition in the second if a block was false. Let\u0026rsquo;s check an example that is closer to real life.\npackage main import \u0026#34;fmt\u0026#34; func main() { firstNumber := 10 secondNumber := 20 if firstNumber \u0026gt; secondNumber { fmt.Println(\u0026#34;First number is bigger\u0026#34;) } if firstNumber \u0026lt; secondNumber { fmt.Println(\u0026#34;First number is bigger\u0026#34;) } if firstNumber == secondNumber { fmt.Println(\u0026#34;First number is bigger\u0026#34;) } } $ go run main.go First number is bigger Here, again, only the first line was displayed. Because all others returned false because the first number is greater than the second. If you change the numbers, a different line will be printed.\nIn addition to the above \u0026lt; less than, \u0026gt; greater than and == equal to, golang has the following operators != not equal, \u0026gt;= greater or equal, and \u0026lt;= less or equal.\nAlso, in one if block there can be several conditions that can be combined with \u0026amp;\u0026amp; - and or || - or. \u0026amp;\u0026amp; will be true only if all conditions return true. At || will return true if one of the conditions returns true.\npackage main import \u0026#34;fmt\u0026#34; func main() { firstNumber := 10 secondNumber := 20 if firstNumber \u0026gt;= 20 \u0026amp;\u0026amp; secondNumber \u0026gt;= 20 { fmt.Println(\u0026#34;Both number higher then 20\u0026#34;) } if firstNumber \u0026gt;= 20 || secondNumber \u0026gt;= 20 { fmt.Println(\u0026#34;One or both numbers higher then 20\u0026#34;) } if firstNumber != secondNumber { fmt.Println(\u0026#34;Numbers are not equal\u0026#34;) } } $ go run main.go One or both numbers higher then 20 Numbers are not equal In this example, the second message is displayed because one of the numbers is greater than 20. Also, the last message is displayed because the numbers are not equal. In order for not all conditions to be evaluated, but for example, to stop after the first one that returned true, you can use if/else. Let\u0026rsquo;s rewrite the same code with the addition of else.\npackage main import \u0026#34;fmt\u0026#34; func main() { firstNumber := 10 secondNumber := 20 if firstNumber \u0026gt;= 20 \u0026amp;\u0026amp; secondNumber \u0026gt;= 20 { fmt.Println(\u0026#34;Both number higher then 20\u0026#34;) } else if firstNumber \u0026gt;= 20 || secondNumber \u0026gt;= 20 { fmt.Println(\u0026#34;One or both numbers higher then 20\u0026#34;) } else if firstNumber != secondNumber { fmt.Println(\u0026#34;Numbers are not equal\u0026#34;) } else { fmt.Println(\u0026#34;Final case\u0026#34;) } } $ go run main.go One or both numbers higher then 20 In this case, when the first condition returned true, the next ones are no longer evaluated, so we see only one output.\nSwitch A switch is similar to the if condition but is used when we have many similar checks.\npackage main import \u0026#34;fmt\u0026#34; func main() { number := 30 switch number { case 10: fmt.Println(\u0026#34;Number is 10\u0026#34;) case 20: fmt.Println(\u0026#34;Number is 20\u0026#34;) case 30: fmt.Println(\u0026#34;Number is 30\u0026#34;) default: fmt.Println(\u0026#34;Default cast\u0026#34;) } } Several cases can be combined into one\npackage main import \u0026#34;fmt\u0026#34; func main() { number := 30 switch number { case 10, 20, 30: fmt.Println(\u0026#34;Number is 10, 20 or 30\u0026#34;) case 40, 50, 60: fmt.Println(\u0026#34;Number is 40, 50 or 60\u0026#34;) default: fmt.Println(\u0026#34;Default cast\u0026#34;) } } $ go run main.go Number is 10, 20 or 30 As in if, you can use comparison operators. The switch looks the same, only the name of the parameter we are checking is not indicated next to it.\npackage main import \u0026#34;fmt\u0026#34; func main() { number := 30 switch { case number \u0026lt;= 30: fmt.Println(\u0026#34;Number is less then 30\u0026#34;) case number \u0026lt;= 60: fmt.Println(\u0026#34;Number is less then 60\u0026#34;) default: fmt.Println(\u0026#34;Default cast\u0026#34;) } } $ go run main.go Number is less than 30 In this case, the next case where number \u0026lt;= 60 is not executed, although it also returns true, for it to be called, you need to add a keyword fallthrough, but then the next case will be executed regardless of the condition in it\npackage main import \u0026#34;fmt\u0026#34; func main() { number := 30 switch { case number \u0026lt;= 30: fmt.Println(\u0026#34;Number is less then 30\u0026#34;) fallthrough case number \u0026lt;= 10: fmt.Println(\u0026#34;Number is less then 10\u0026#34;) default: fmt.Println(\u0026#34;Default cast\u0026#34;) } } $ go run main.go Number is less than 30 Number is less than 10 Even though number \u0026lt;= 10 returned false, Number is less than 10 was still printed.\nAnother option for using cases is type-checking.\npackage main import \u0026#34;fmt\u0026#34; func main() { var n any = 1 switch n.(type) { case int: fmt.Println(\u0026#34;N is int\u0026#34;) case string: fmt.Println(\u0026#34;N is string\u0026#34;) default: fmt.Println(\u0026#34;Undefined type\u0026#34;) } } $ go run main.go N is int If you replace 1 with \u0026ldquo;1\u0026rdquo;, N is string will be displayed.\nVideo ","permalink":"https://mpostument.com/posts/programming/golang/basics/go-if-switch/","summary":"Hello!\nThe topic of today\u0026rsquo;s post is If and Switch statements in go.\nIf The if block is executed if the condition returns true.\npackage main import \u0026#34;fmt\u0026#34; func main() { if true { fmt.Println(\u0026#34;Condition is true\u0026#34;) } if false { fmt.Println(\u0026#34;Condition is false\u0026#34;) } } $ go run main.go Condition is true In this case, only Conditions is true was printed because the condition in the second if a block was false.","title":"If and Switch in golang"},{"content":"Hello!\nThe topic of today\u0026rsquo;s post is about maps and structs. Lets start start with maps\nMaps Create map Maps is a data structure in which data is specified in pairs, keys, and values. In golang it is created as follows. map[KEY_TYPE]VALUE_TYPE.\nimport \u0026#34;fmt\u0026#34; func main() { studentsGrades := map[string]int{\u0026#34;John\u0026#34;: 50, \u0026#34;Alex\u0026#34;: 70} fmt.Println(studentsGrades) } $ go run main.go map[Alex:70 John:50] Another option is to create a map using the make keyword\nimport \u0026#34;fmt\u0026#34; func main() { studentsGrades := make(map[string]int) fmt.Println(studentsGrades) } Add to map A new element is added to the map as follows MAP_NAME[KEY] = VALUE\npackage main import \u0026#34;fmt\u0026#34; func main() { studentsGrades := map[string]int{} studentsGrades[\u0026#34;Susan\u0026#34;] = 99 fmt.Println(studentsGrades) } $ go run main.go map[Susan:99] If the map is created using the make keyword, you can add elements to it in the same way.\nDelete from a map The element can be deleted using the delete method, delete(MAP_NAME, KEY)\npackage main import \u0026#34;fmt\u0026#34; func main() { studentsGrades := map[string]int{\u0026#34;John\u0026#34;: 56, \u0026#34;Alex\u0026#34;: 67} studentsGrades[\u0026#34;Susan\u0026#34;] = 99 fmt.Println(studentsGrades) delete(studentsGrades, \u0026#34;Alex\u0026#34;) fmt.Println(studentsGrades) } $ go run main.go map[Alex:67 John:56 Susan:99] map[John:56 Susan:99] Get element from the map You can get the value of the key in this way MAP_NAME[KEY].\npackage main import \u0026#34;fmt\u0026#34; func main() { studentsGrades := map[string]int{\u0026#34;John\u0026#34;: 56, \u0026#34;Alex\u0026#34;: 67} fmt.Println(studentsGrades[\u0026#34;John\u0026#34;]) } $ go run main.go 56 But if we try to specify a key that does not exist, we will not get an error but will get a null value\npackage main import \u0026#34;fmt\u0026#34; func main() { studentsGrades := map[string]int{\u0026#34;John\u0026#34;: 56, \u0026#34;Alex\u0026#34;: 67} fmt.Println(studentsGrades[\u0026#34;Susan\u0026#34;]) } $ go run main.go 0 To check whether the key exists, you can use the following construction VARIABLE, ok := MAP_NAME[KEY]. The ok variable will be true/false depending on whether the key exists.\npackage main import \u0026#34;fmt\u0026#34; func main() { studentsGrades := map[string]int{\u0026#34;John\u0026#34;: 56, \u0026#34;Alex\u0026#34;: 67} susanGrade, ok := studentsGrades[\u0026#34;Susan\u0026#34;] fmt.Printf(\u0026#34;Susan, %v %v\\n\u0026#34;, susanGrade, ok) johnGrade, ok := studentsGrades[\u0026#34;John\u0026#34;] fmt.Printf(\u0026#34;John, %v %v\\n\u0026#34;, johnGrade, ok) } $ go run main.go Susan, 0 false John, 56 true Struct A struct is a collection of fields that can be of different types.\nCreate struct Struct is created like this type NAME struct {FIELDS}.\npackage main import \u0026#34;fmt\u0026#34; type Student struct { Name string LastName string Grade int Subjects []string } func main() { studentJohn := Student{ Name: \u0026#34;John\u0026#34;, LastName: \u0026#34;Snow\u0026#34;, Grade: 90, Subjects: []string{\u0026#34;Algebra\u0026#34;, \u0026#34;Geometry \u0026#34;}, } fmt.Println(studentJohn) } go run main.go {John Snow 90 [Algebra Geometry ]} You can also refer to a specific field of struct separately STRUCT_NAME.FILED_NAME\nfmt.Println(studentJohn.Name) $ go run main.go John Embedding Go does not have an inheritance as in traditional OOP languages. But go uses composition. Let\u0026rsquo;s review how it can be used\npackage main import \u0026#34;fmt\u0026#34; type Animal struct { Name string } type Bird struct { Animal Speed int } func main() { kiwi := Bird{} kiwi.Name = \u0026#34;Kiwi\u0026#34; kiwi.Speed = 30 fmt.Println(kiwi.Name) } $ go run main.go Kiwi But unlike inheritance here Bird is not an Animal, but Bird has an Animal.\nIf you need to create an instance of the structure as in the previous example, when we initialize the fields at the time of creation, it will look like this\npackage main import \u0026#34;fmt\u0026#34; type Animal struct { Name string } type Bird struct { Animal Speed int } func main() { kiwi := Bird{ Speed: 80, Animal: Animal{ Name: \u0026#34;Kiwi\u0026#34; }} fmt.Println(kiwi.Name) } Anonymous struct You can create anonymous structures without first creating a structure type with a field definition\npackage main import \u0026#34;fmt\u0026#34; func main() { kiwi := struct { name string speed int }{ name: \u0026#34;Kiwi\u0026#34;, speed: 80, } fmt.Println(kiwi.name) } $ go run main.go Kiwi Such structures are convenient to use to organize data that does not long live in the program.\nCopy struct If you take a structure and assign it to another variable, a copy of the structure will be created\npackage main import \u0026#34;fmt\u0026#34; func main() { kiwi := struct { name string speed int }{ name: \u0026#34;Kiwi\u0026#34;, speed: 80, } b := kiwi b.name = \u0026#34;Emu\u0026#34; fmt.Println(kiwi) fmt.Println(b) } $ go run main.go {Kiwi 80} {Emu 80} To pass the same structure, you need to add the symbol \u0026amp;\npackage main import \u0026#34;fmt\u0026#34; func main() { kiwi := struct { name string speed int }{ name: \u0026#34;Kiwi\u0026#34;, speed: 80, } b := \u0026amp;kiwi b.name = \u0026#34;Emu\u0026#34; fmt.Println(kiwi) fmt.Println(b) } $ go run main.go {Emu 80} \u0026amp;{Emu 80} Video ","permalink":"https://mpostument.com/posts/programming/golang/basics/go-maps-structs/","summary":"Hello!\nThe topic of today\u0026rsquo;s post is about maps and structs. Lets start start with maps\nMaps Create map Maps is a data structure in which data is specified in pairs, keys, and values. In golang it is created as follows. map[KEY_TYPE]VALUE_TYPE.\nimport \u0026#34;fmt\u0026#34; func main() { studentsGrades := map[string]int{\u0026#34;John\u0026#34;: 50, \u0026#34;Alex\u0026#34;: 70} fmt.Println(studentsGrades) } $ go run main.go map[Alex:70 John:50] Another option is to create a map using the make keyword","title":"Maps and Structs in golang"},{"content":"Hello!\nIn this post we are going to explore what is arrays та slices в golang.\nArrays Creation An array in golang is created as follows [NUMBER_OF_ELEMENTS_IN_ARRAY]TYPE{ELEMENTS}. And allows you to store many objects of the same type\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { students := [3]string{\u0026#34;John\u0026#34;, \u0026#34;Lucy\u0026#34;, \u0026#34;Alex\u0026#34;} fmt.Println(students) } $ go run main.go [John Lucy Alex] To not specify a number that will mean the number of elements in the array, you can specify .... In this case, go will automatically determine how many elements there are and create an array of this length.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { students := [...]string{\u0026#34;John\u0026#34;, \u0026#34;Lucy\u0026#34;, \u0026#34;Alex\u0026#34;} fmt.Println(students) } When we create an array, it is not necessary to specify the elements of array during creation. You can create an empty one and add elements later. But in this case, you need to specify how many elements there should be. Because if we specify ... then an array with a length of 0 will be created and we will not be able to add any elements to it.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { students := [...]string{} students[0] = \u0026#34;John\u0026#34; fmt.Println(students) } If we specify the number of elements but do not specify them during creation, then null values of this type will be used. For example, our array will be of type int, it will be filled with zeros.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { studentsGrades := [3]int{} fmt.Println(studentsGrades) } $ go run main.go [0 0 0] Change values Now that we have an empty array, we can add values to it. This is done as follows: arrayName[INDEX] = VALUE. Indexes start from zero, if we need to add the first element, then its index will be equal to 0, the index of the second element will be equal to 1, and so on\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { studentsGrades := [3]int{} studentsGrades[0] = 99 studentsGrades[2] = 22 fmt.Println(studentsGrades) } $ go run main.go [99 0 22] In this example, we specified the value for the first element and the third, and the second element remained unchanged.\nArray length In order to see the length of the array, you can use the len() method\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { studentsGrades := [3]int{} fmt.Println(len(studentsGrades)) } $ go run main.go 3 Copy array An array can be copied by simply assigning it to another variable. Example\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { studentsGrades := [3]int{1, 2, 3} b := studentsGrades fmt.Printf(\u0026#34;Students grades: %v\\n\u0026#34;, studentsGrades) fmt.Printf(\u0026#34;Array b: %v\\n\u0026#34;, b) } Let\u0026rsquo;s now try to change the value in the array b and print the value\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { studentsGrades := [3]int{1, 2, 3} b := studentsGrades b[0] = 99 fmt.Printf(\u0026#34;Students grades: %v\\n\u0026#34;, studentsGrades) fmt.Printf(\u0026#34;Array b: %v\\n\u0026#34;, b) } $ go run main.go Students grades: [1 2 3] Array b: [99 2 3] As you can see, only the second array has changed, the first has remained unchanged. And that\u0026rsquo;s because we made a copy of the array. In order to pass the same array to another variable, and not its copy, you need to use \u0026amp;. In future posts, we will take a closer look at what it is, and now we will just try to use it\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { studentsGrades := [3]int{1, 2, 3} b := \u0026amp;studentsGrades b[0] = 99 fmt.Printf(\u0026#34;Students grades: %v\\n\u0026#34;, studentsGrades) fmt.Printf(\u0026#34;Array b: %v\\n\u0026#34;, b) } go run main.go Students grades: [99 2 3] Array b: \u0026amp;[99 2 3] Slices Creating Slices are very similar to arrays, they are even created very similarly. The only thing is that in slices, we do not specify in advance how many elements will be in the slice.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { sliceOne := []int{1, 2, 3} fmt.Printf(\u0026#34;Slice One: %v\\n\u0026#34;, sliceOne) } $ go run main.go Slice One: [1 2 3] You can also see the length of a slice using the len() method. But we also add the cap method to the output.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { sliceOne := []int{1, 2, 3} fmt.Printf(\u0026#34;Len: %v, Capacity: %v\\n\u0026#34;, len(sliceOne), cap(sliceOne)) } $ go run main.go Len: 3, Capacity: 3 Append to slice What does this mean? Slices are using arrays to store data. And capacity indicates the size of the array currently used by the slice.\nThe main difference is that we can expand slices (which we can\u0026rsquo;t do with arrays). For example, I create a slice of three elements, in this case len and cap will be equal to three. Let\u0026rsquo;s try to add the fourth element to the slice. For this, you need to use the append method.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { sliceOne := []int{1, 2, 3} fmt.Printf(\u0026#34;Len: %v, Capacity: %v\\n\u0026#34;, len(sliceOne), cap(sliceOne)) sliceOne = append(sliceOne, 22) fmt.Printf(\u0026#34;Len: %v, Capacity: %v\\n\u0026#34;, len(sliceOne), cap(sliceOne)) } $ go run main.go Len: 3, Capacity: 3 Len: 4, Capacity: 6 Initially, our len and cap were equal to three. After adding one element, len increased by 1 and became 4, but cap doubled and became 6. Because it is not efficient for go to create a new array every time and copy data there, therefore go itself determines which size array to create in order not to take up a lot of memory, but also do not create a new array every time we add a new element to a slice\nWith append you can add many elements to a slice SLICE_NAME = append(SLICE_NAME, ELEM1, ELEM2, ELEM3). You can also add a slice to a slice. But it will not work if we specify a slice instead of an element in the append method.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { sliceOne := []int{1, 2, 3, 5, 10} sliceTwo := []int{11, 233, 34} sliceOne = append(sliceOne, sliceTwo) fmt.Printf(\u0026#34;Slice one: %v\\n\u0026#34;, sliceOne) fmt.Printf(\u0026#34;Slice two: %v\\n\u0026#34;, sliceTwo) } $ go run main.go # command-line-arguments ./main.go:10:30: cannot use sliceTwo (variable of type []int) as type int in argument to append In order to be able to add a slice, you need to unpack it, this can be done using ...\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { sliceOne := []int{1, 2, 3, 5, 10} sliceTwo := []int{11, 233, 34} sliceOne = append(sliceOne, sliceTwo...) fmt.Printf(\u0026#34;Slice one: %v\\n\u0026#34;, sliceOne) fmt.Printf(\u0026#34;Slice two: %v\\n\u0026#34;, sliceTwo) } $ go run main.go Slice one: [1 2 3 5 10 11 233 34] Slice two: [11 233 34] Also, unlike arrays, when we assign it to another variable, we do not create a copy, but transfer the slice itself\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { sliceOne := []int{1, 2, 3} b := sliceOne b[1] = 999 fmt.Printf(\u0026#34;Slice one: %v\\n\u0026#34;, sliceOne) fmt.Printf(\u0026#34;Slice b: %v\\n\u0026#34;, b) } $ go run main.go Slice one: [1 999 3] Slice b: [1 999 3] Slice from slice We can make a slice from an existing slice using indexes like ARRAY_NAME[first_index:last_index]. For example, we have a slice of 5 elements and we need to create a slice that will have only the last three elements, then this can be done as follows\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { sliceOne := []int{1, 2, 3, 5, 10} sliceTwo := sliceOne[2:] fmt.Printf(\u0026#34;Slice one: %v\\n\u0026#34;, sliceOne) fmt.Printf(\u0026#34;Slice two: %v\\n\u0026#34;, sliceTwo) } $ go run main.go Slice one: [1 2 3 5 10] Slice two: [3 5 10] sliceTwo := sliceOne[2:] we create a new slice that includes all elements starting from the second index to the last. Therefore, you do not need to specify anything after :. If necessary, you can specify the last element that will be included.\nIf we try to replace an element in the second slice, it will be replaced in the first, this is because both slices use the same array with data\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { sliceOne := []int{1, 2, 3, 5, 10} sliceTwo := sliceOne[1:3] sliceTwo[0] = 222 fmt.Printf(\u0026#34;Slice one: %v\\n\u0026#34;, sliceOne) fmt.Printf(\u0026#34;Slice two: %v\\n\u0026#34;, sliceTwo) } $ go run main.go Slice one: [1 222 3 5 10] Slice two: [222 3] Create slice with make The make(TYPE, LEN, CAP) method can be used to create slices. Capacity is an optional argument. If we use make without capacity, it will correspond to the same thing as we create an empty slice []int{}.\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { sliceOne := make([]int, 5) fmt.Printf(\u0026#34;Slice one len %v, capacity %v\\n\u0026#34;, len(sliceOne), cap(sliceOne)) } $ go run main.go Slice one len 5, capacity 5 If we know the required capacity, it can be specified immediately, and then go will not have to create new arrays and transfer data to them as objects are added\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { sliceOne := make([]int, 5, 100) fmt.Printf(\u0026#34;Slice one len %v, capacity %v\\n\u0026#34;, len(sliceOne), cap(sliceOne)) } $ go run main.go Slice one len 5, capacity 100 Video ","permalink":"https://mpostument.com/posts/programming/golang/basics/go-arrays-slices/","summary":"Hello!\nIn this post we are going to explore what is arrays та slices в golang.\nArrays Creation An array in golang is created as follows [NUMBER_OF_ELEMENTS_IN_ARRAY]TYPE{ELEMENTS}. And allows you to store many objects of the same type\npackage main import ( \u0026#34;fmt\u0026#34; ) func main() { students := [3]string{\u0026#34;John\u0026#34;, \u0026#34;Lucy\u0026#34;, \u0026#34;Alex\u0026#34;} fmt.Println(students) } $ go run main.go [John Lucy Alex] To not specify a number that will mean the number of elements in the array, you can specify .","title":"Arrays and Slices in golang"},{"content":"Hello!\nToday we will see what a timeline is and how you can use it to create a series of events that will be launched one after another. We will also use the path that we already reviewed in the post, to create objects and send them along a given path, like in TD games.\nTimeline Timeline is created like any other entity in Game Maker. By right-clicking on the Timelines folder and then Create -\u0026gt; Timeline. I will name it tml_spawn.\n.\nThis is what the Timeline window looks like\nYou can add/remove elements using the Add and Remove buttons. The first element added will have a Moment of 0. Next, I\u0026rsquo;ll add another Moment but change its value to 60. Game Maker defaults to 60 frames per second. And accordingly, specifying 60 means that the second event will take place in 1 second after the previous one. If specified 120, it will mean 2 seconds, and so on. Now I will open Moment which has 0 and add the following code\ninstance_create_depth(x,y,1,obj_enemy) This code will create an object based on the coordinates where the timeline will be placed. I will call Timeline in a circle to create objects, and I will leave the code in Moment with the value 60 empty to pause for 1 second between creating objects. In this case, I will create only one type of object, if necessary, you can create different types of objects with a different intervals. For example, a warrior appears in the first second, an archer in the second, a mage in the third, and so on in a loop.\nNow we need to create an object that will run the timeline. To do this, I will create an obj_spawn object in the objects folder and add the Create event with the following code\ntimeline_index=tml_spawn timeline_loop=true timeline_running=true First, we indicate which timeline we will run, then we make the timeline run in a loop and turn on the timeline. After that, the timeline needs to be added to the room at the point with which our objects will go.\npath_start(pth_enemy, 5, path_action_stop, false) Now let\u0026rsquo;s start the game and see how objects are created at one point and move along a given path\nVideo ","permalink":"https://mpostument.com/posts/programming/game_maker/timeline/","summary":"Hello!\nToday we will see what a timeline is and how you can use it to create a series of events that will be launched one after another. We will also use the path that we already reviewed in the post, to create objects and send them along a given path, like in TD games.\nTimeline Timeline is created like any other entity in Game Maker. By right-clicking on the Timelines folder and then Create -\u0026gt; Timeline.","title":"Game Maker - Timelines"},{"content":"Hello!\nLet\u0026rsquo;s learn how paths work in GameMaker. With their help, you can set the trajectory along which the object will move.\nPath A path is created just like any other entity in GameMaker. In the Asset Browser, right-click and select Create-\u0026gt;Path.\nI will call it pth_enemy and in the window that appears, you can draw a path. It is preferable to start from the point 0, 0. And then by pressing the left mouse button, we indicate the points along which the object will move\nThere are also several important options in this window. The first one is Closed, if it is pressed, the last point will be connected to the first\nAnother option is Smooth Curved. If you specify it, the path lines will be rounded\nObject movement Now that we have a path, we can tell the object to move along it. To do this, you need to create an object and add a sprite to it. For the object, add the Create event and start a movement in it using the path_start method, which accepts 4 arguments. The first is the name of the path along which to move, the second is the speed, and the third is what to do when the path ends (start again, go back in the opposite direction, continue or stop). And the last one is where to start the movement, from the coordinate point relative to the room (the coordinate from which we started drawing the path will be used) or relative to the object.\npath_start(pth_enemy, 5, path_action_continue, 0) I place the object in the room and start the game and my object moves. The movement starts from the place where we placed the object because I specified 0 as the last parameter. If I set it as 1, then the movement will start from the zero point in the room.\nVideo ","permalink":"https://mpostument.com/posts/programming/game_maker/path/","summary":"Hello!\nLet\u0026rsquo;s learn how paths work in GameMaker. With their help, you can set the trajectory along which the object will move.\nPath A path is created just like any other entity in GameMaker. In the Asset Browser, right-click and select Create-\u0026gt;Path.\nI will call it pth_enemy and in the window that appears, you can draw a path. It is preferable to start from the point 0, 0. And then by pressing the left mouse button, we indicate the points along which the object will move","title":"Game Maker - Path"},{"content":"Hello!\nThe topic of today\u0026rsquo;s post loops. The game maker has loops such as while, until, and repeat. We explored the for loop in the topic about arrays, you can read about in post about game maker arrays.\nWhile loop The while loop works as long as the condition returns true. The while keyword is followed by a condition in parentheses, and as long as this condition returns true, the loop will continue. As soon as it becomes false, the loop ends immediately\nwhile (\u0026lt;expression\u0026gt;) { \u0026lt;statement\u0026gt;; \u0026lt;statement\u0026gt;; ... } First, I create the parameter i, which has a value of 0, and then I start a loop in which I specify that it should work as long as i is less than 5. In the loop, using draw_text, I display the value of i on the screen and increase i by one. If this is not done, i will always have the value 0 and our loop will never end and the game will not start. I added this code to the Draw Event.\ni = 0 while (i \u0026lt; 5){ draw_text(10+70*i, 10, i) i++ } Do until loop The next loop is named do until. Unlike while, it works until the condition returns false, as soon as it becomes true, the loop ends immediately\ndo { \u0026lt;statement\u0026gt;; \u0026lt;statement\u0026gt;; ... } until (\u0026lt;expression\u0026gt;); The result of this loop will be the same as the previous one within a while. But the condition here is different. We again have i, which has a value of 0. Then we start a loop in which we display the value of i on the screen and increase the value of i, and then we check whether i is greater than 5. If so, then we end the loop, and if not, then we continue.\ni = 0 do { draw_text(10+70*i, 10, i) i++ } until (i \u0026gt; 5) The peculiarity of this loop is that even if i initially has a value greater than 5. For example, 6, the loop will work once, because checking whether i is greater than 5 occurs at the end of the loop, and thus the loop has time to complete one iteration.\nRepeat The last type of loop is a repeat. Unlike the previous options, there are no conditions here, and we simply indicate how many times to go through the loop\nrepeat (\u0026lt;expression\u0026gt;) { \u0026lt;statement\u0026gt;; \u0026lt;statement\u0026gt;; ... } Let\u0026rsquo;s try to implement the same thing that we implemented with the while and do loops. After the key repeat, we indicate in parentheses how many times the loop should be repeated, and in the body itself we execute the same code as before, output the value of i, and increase it by one\ni = 0 repeat(5) { draw_text(10+70*i, 10, i) i++ } Video ","permalink":"https://mpostument.com/posts/programming/game_maker/loops/","summary":"Hello!\nThe topic of today\u0026rsquo;s post loops. The game maker has loops such as while, until, and repeat. We explored the for loop in the topic about arrays, you can read about in post about game maker arrays.\nWhile loop The while loop works as long as the condition returns true. The while keyword is followed by a condition in parentheses, and as long as this condition returns true, the loop will continue.","title":"Game Maker - Loops"},{"content":"Hello!\nLet\u0026rsquo;s learn what constants are, how they should be named in golang, and also what iota is.\nNaming If you are familiar with constants in other programming languages, you know that their names usually include only capital letters, for example, PORT_NUMBER. In golang, we cannot do this because by making the first letter uppercase, we will immediately make the constant available to other packages. Therefore, in golang, we name constants in the same way as variables portNumber or PortNumber if we want the constant to be available in other packages.\npackage main import \u0026#34;fmt\u0026#34; func main() { const portNumber int = 80 fmt.Printf(\u0026#34;%v: %T\u0026#34;, portNumber, portNumber) } $ go run main.go 80: int Just as with the variables, the type may be omitted\nconst portNumber = 80 The peculiarity of the constant is that it must have a value at the time of compilation, for example, if in the constant I try to take the power of the constant value, I will get an error, because it is calculated at the time of starting the program, and not at the compilation stage\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) func main() { const portNumber float64 = math.Pow(2.0, 2.0) fmt.Println(portNumber) } $ go run main.go # command-line-arguments ./main.go:9:29: math.Pow(2.0, 2.0) (value of type float64) is not constant Shadowing Just like with variables, shadowing also works with constants.\npackage main import ( \u0026#34;fmt\u0026#34; ) const portNumber int32 = 80 func main() { const portNumber int = 8080 fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, portNumber, portNumber) printConstant() } func printConstant() { fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, portNumber, portNumber) } $ go run main.go 8080: int 80: int32 Math Operations Mathematical operations work in the same way as variables\npackage main import \u0026#34;fmt\u0026#34; func main() { const a int = 8080 const b int = 10 fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, a+b, a+b) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, a-b, a-b) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, a*b, a*b) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, a/b, a/b) } $ go run main.go 8090: int 8070: int 80800: int 808: int If we try to add constants of different types, we will get an error\npackage main import \u0026#34;fmt\u0026#34; func main() { const a int32 = 8080 const b int = 10 fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, a+b, a+b) } $ go run main.go # command-line-arguments ./main.go:9:25: invalid operation: a + b (mismatched types int32 and int) But there is one peculiarity here if we do not specify the type in one of the constants, but simply the value 8080, then the compiler will automatically define it as an int. Then we will create another constant of type int16 and try to add them, then we will get a successful execution. This is because the compiler at the time of compilation replaces the constant with its value in all places where it is used.\npackage main import \u0026#34;fmt\u0026#34; func main() { const a = 8080 const b int16 = 10 fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, a, a) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, b, b) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, a+b, a+b) } To the compiler, this code will look like this\npackage main import \u0026#34;fmt\u0026#34; func main() { const a = 8080 const b int16 = 10 fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, 8080, 8080) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, 10, 10) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, 8080+10, 8080+10) } And as a result, we will get int16\n$ go run main.go 8080: int 10: int16 8090: int16 Iota When creating constants, you can specify the special word iota\npackage main import \u0026#34;fmt\u0026#34; const ( a = iota ) func main() { fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, a, a) } $ go run main.go 0: int iota allows you to create enum constants.\npackage main import \u0026#34;fmt\u0026#34; const ( a = iota b = iota c = iota ) func main() { fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, a, a) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, b, b) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, c, c) } $ go run main.go 0: int 1: int 2: int The word iota for each constant is optional if these constants are in the same block\npackage main import \u0026#34;fmt\u0026#34; const ( a = iota b c ) func main() { fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, a, a) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, b, b) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, c, c) } $ go run main.go 0: int 1: int 2: int Sometimes it is necessary to omit the value in the constant\npackage main import \u0026#34;fmt\u0026#34; const ( _ = iota a b c ) func main() { fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, a, a) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, b, b) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, c, c) } $ go run main.go 1: int 2: int 3: int Also, iota can start not from zero if you add a number to it\npackage main import \u0026#34;fmt\u0026#34; const ( a = iota + 5 b c ) func main() { fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, a, a) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, b, b) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, c, c) } $ go run main.go 5: int 6: int 7: int Video ","permalink":"https://mpostument.com/posts/programming/golang/basics/go-constant/","summary":"Hello!\nLet\u0026rsquo;s learn what constants are, how they should be named in golang, and also what iota is.\nNaming If you are familiar with constants in other programming languages, you know that their names usually include only capital letters, for example, PORT_NUMBER. In golang, we cannot do this because by making the first letter uppercase, we will immediately make the constant available to other packages. Therefore, in golang, we name constants in the same way as variables portNumber or PortNumber if we want the constant to be available in other packages.","title":"Go Constants"},{"content":"Hello!\nIn this programming, we will learn how to work with arrays in Game Maker.\nArrays can contain many elements and then can be one-dimensional or multidimensional. Unlike other programming languages, an array in Game Maker does not need to be initialized. You can add an element to the array without array initialization. The name of the array comes first and its index is indicated in square brackets. Numbering starts from zero\ninventory[0] = \u0026#34;bow\u0026#34; inventory[1] = \u0026#34;arrow\u0026#34; As a result, it will look like an array of two elements [\u0026ldquo;bow\u0026rdquo;, \u0026ldquo;arrow\u0026rdquo;]. Two-dimensional arrays are also often used. To do this, you need to add one more square brackets which will indicate the row\ninventory[0][0] = \u0026#34;bow\u0026#34; inventory[0][1] = \u0026#34;arrow\u0026#34; inventory[1][0] = \u0026#34;sword\u0026#34; inventory[1][0] = \u0026#34;shield\u0026#34; In this case, it will look like a table with two rows and two columns.\nFor loop A for loop is often used to fill an array with data or read from it. This significantly reduces the amount of code that needs to be written. For example, if you need to add 100 elements to the array, you can copy the assignment line of the element to the array 100 times, but it is not very convenient to work with it. Let\u0026rsquo;s create an empty array of 100 elements.\ninventory[0] = \u0026#34;empty\u0026#34; inventory[1] = \u0026#34;empty\u0026#34; inventory[98] = \u0026#34;empty\u0026#34; ... inventory[99] = \u0026#34;empty\u0026#34; If you use for loop, it will be much more compact. The for loop looks like this\nfor (i = 0; i \u0026lt; 99, i++) { # CODE } i indicates where we start the countdown. Next comes the condition that i must be less than 99, as soon as it becomes 99, the cycle will end immediately. And at the end, we have i++, which means that I will increase by 1. Thus, when the loop starts, we have i = 0. After the first pass of the loop, i increased by 1 and becomes equal to i = 1. And so on until i becomes 99. Then the cycle ends.\nLet\u0026rsquo;s try to create an empty one-dimensional array using a loop.\nfor (i = 0; i \u0026lt; 99, i++) { inventory[i] = \u0026#34;empty\u0026#34; } Thus, at each iteration of the loop, i will increase and we will create an array of 100 elements with the value \u0026ldquo;empty\u0026rdquo;. This can be added to the Draw event to display the array. But before that, it must be created. Its creation is defined in the Create event. In draw_text, I will add 70*i to the x coordinate, this is to create an indentation between the elements of the array. When i = 0, 70*i will also be 0. Then it will be 70, 140, and so on.\nfor (i = 0; i \u0026lt; 99, i++) { draw_text(10+70*i, 10, inventory[i]) } A two-dimensional array can be created in the same way, only we will need two loops\nfor (i = 0; i \u0026lt; 99, i++) { for (j = 0; j \u0026lt; 99, i++) { inventory[i][j] = \u0026#34;empty\u0026#34; } } What will happen in this case? First, we start the first loop where i = 0, then we immediately get into the second loop where j = 0. And we make the first entry in the array at index [0][0]. Further, i remains 0, and j increases by 1. Therefore, we make an entry in the array at index [0][1] and so on until j becomes 99. Then the nested loop ends and we return to the loop with i. Here i will increase by 1 and again go into a nested loop where j is equal to 0. And we will write to the index [1][0] and again until j becomes 99 and i will increase by 1 until will reach 99 and then the loop will end.\nTo display a two-dimensional array, two cycles also be required. And everything else is the same as with one-dimensional\nfor (i = 0; i \u0026lt; 99, i++) { for (j = 0; j \u0026lt; 99, i++) { draw_text(10+70*i, 10+70*j, inventory[i][j]) } } Video ","permalink":"https://mpostument.com/posts/programming/game_maker/arrays/","summary":"Hello!\nIn this programming, we will learn how to work with arrays in Game Maker.\nArrays can contain many elements and then can be one-dimensional or multidimensional. Unlike other programming languages, an array in Game Maker does not need to be initialized. You can add an element to the array without array initialization. The name of the array comes first and its index is indicated in square brackets. Numbering starts from zero","title":"Game Maker - Arrays and For Loop"},{"content":"Hello!\nIn this programming, we will look at the basic data types in golang.\nBool The first type we will consider is bool. In this type, only two values are possible: true and false. You can create a bool variable in one of the following ways:\nvar isActive bool var isEnabled = false isLoaded := true In the second and third options, we immediately set the value that the variable will have. In the first, we do not specify anything, so a zero value will be used. For the bool type, it will be false. I will display the type and value of each variable\npackage main import \u0026#34;fmt\u0026#34; func main() { var isActive bool var isEnabled bool = false isLoaded := true fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, isActive, isActive) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, isEnabled, isEnabled) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, isLoaded, isLoaded) } $ go run main.go false: bool false: bool true: bool Int The next one is int. There are several int types in golang, and there is also unsigned int. Signed int can include numbers with a minus sign, unsigned only positive numbers. Accordingly, all int types in golang are uint8, uint16, uint32, uint64, int8, int16, int32, and int64.. They differ in what range of numbers we can use. For example, for int8 this range will be -128 to 127 and for uint, it will be 0 to 255 and with each subsequent type, this range will increase. variables are created in the same way as the bool type\nvar a int64 var b int var l uint = 20 var c = 35 d := 86 Now I will output each value and type\npackage main import \u0026#34;fmt\u0026#34; func main() { var a int64 var b int var l uint = 20 var c = 35 d := 86 fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, a, a) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, b, b) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, l, l) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, c, c) fmt.Printf(\u0026#34;%v: %T\\n\u0026#34;, d, d) } go run main.go 0: int64 0: int 20: uint 35: int 86: int For int, the null value will be 0.\nMathematical operations can be performed on the int type. But it must be the same int type. If we have int64 and int32 we will not be able to perform mathematical operations. In this case, you will first need to bring them to the same type.\nThe following mathematical operations are available: addition(+), subtraction(-), multiplication(*), division(/), remainder(%). Most operations are self-explanatory. Only division and remainder require additional attention. Since there are no decimal places in int. After dividing 10 by 3, we will get the whole number 3. and the remainder remains 1. To get the remainder exactly, the % operation is used\npackage main import \u0026#34;fmt\u0026#34; func main() { var a int = 10 var b int = 3 fmt.Println(a + b) fmt.Println(a - b) fmt.Println(a * b) fmt.Println(a / b) fmt.Println(a % b) } $ go run main.go 13 7 30 3 1 Float Comma numbers are created similarly to ints and have the same math operations except for calculating the remainder(%).\npackage main import \u0026#34;fmt\u0026#34; func main() { var a float32 = 10 var b float32 = 3 c := 10.4 var d = 10.2 fmt.Printf(\u0026#34;%f: %T\\n\u0026#34;, a, a) fmt.Printf(\u0026#34;%f: %T\\n\u0026#34;, b, b) fmt.Printf(\u0026#34;%f: %T\\n\u0026#34;, c, c) fmt.Printf(\u0026#34;%f: %T\\n\u0026#34;, d, d) } $ go run main.go 10.000000: float32 3.000000: float32 10.400000: float64 10.200000: float64 And mathematical operations\npackage main import \u0026#34;fmt\u0026#34; func main() { var a float32 = 10.2 var b float32 = 3.4 fmt.Println(a + b) fmt.Println(a - b) fmt.Println(a * b) fmt.Println(a / b) } $ go run main.go 13.6 6.7999997 34.68 2.9999998 String The last type for today is a string.\npackage main import \u0026#34;fmt\u0026#34; func main() { var a string = \u0026#34;Hello World\u0026#34; var c = \u0026#34;Hello\u0026#34; b := \u0026#34;World\u0026#34; fmt.Printf(\u0026#34;%s: %T\\n\u0026#34;, a, a) fmt.Printf(\u0026#34;%s: %T\\n\u0026#34;, b, b) fmt.Printf(\u0026#34;%s: %T\\n\u0026#34;, c, c) } $ go run main.go Hello World: string World: string Hello: string There is one mathematical operation for string, this is +. It allows you to add two string variables.\npackage main import \u0026#34;fmt\u0026#34; func main() { var c = \u0026#34;Hello \u0026#34; b := \u0026#34;World!\u0026#34; fmt.Printf(c + b) } $ go run main.go Hello World! You can also access the string type using indexes. You can imagine that the variable of the type string is a list, in golang, the numbering in the list starts from zero. In this way, you can get only part of the string from it. Example\npackage main import \u0026#34;fmt\u0026#34; func main() { var c = \u0026#34;Hello \u0026#34; fmt.Printf(\u0026#34;%v\u0026#34;, string(c[2])) } $ go run main.go l This way I get the third character from the variable named c. But besides that, you still need to convert the received value to the string type. Otherwise, I would get the number 108, which in UTF stands for l.\nUnlike lists, I cannot replace a character in a string. Because the string is an immutable data type. But I can convert a string to a list of bytes\nimport \u0026#34;fmt\u0026#34; func main() { var c = \u0026#34;Hello \u0026#34; b := []byte(c) fmt.Println(b) } $ go run main.go [72 101 108 108 111 32] And then I can replace any value in the string\npackage main import \u0026#34;fmt\u0026#34; func main() { var c = \u0026#34;Hello \u0026#34; b := []byte(c) b[0] = byte(122) fmt.Println(string(b)) } $ go run main.go zello But initial string c will stay the same. Only array b will be changed\nVideo ","permalink":"https://mpostument.com/posts/programming/golang/basics/go-basic-types/","summary":"Hello!\nIn this programming, we will look at the basic data types in golang.\nBool The first type we will consider is bool. In this type, only two values are possible: true and false. You can create a bool variable in one of the following ways:\nvar isActive bool var isEnabled = false isLoaded := true In the second and third options, we immediately set the value that the variable will have.","title":"Basic Types in Go"},{"content":"Hello!\nLet\u0026rsquo;s explore conditions in Game Maker. Conditions can be created using if/else or switch statements. Let\u0026rsquo;s try both options.\nIf/Else In Game Maker if/else conditions are the same as in other programming languages. The block itself looks like this:\nif (\u0026lt;expression\u0026gt;) { \u0026lt;statement\u0026gt;; } else { \u0026lt;statement\u0026gt;; } For example, we can check the health of our object. If hp \u0026lt; 0, we destroy the object. In order to do this, we will use a new type of event called Step. This event is executed every frame. Usually, the main game logic happens in this event.\nFirst, we set the initial amount of hp in the Create event\nhp = 100 Then add event Step with the following code in it. In parentheses next to if is our condition. Where we check if hp is less than 100. In addition to \u0026lt;= less or equal, there is also \u0026lt; less than, \u0026gt; greater than, = equal to, not equal to, !=, \u0026gt;= greater or equal to. You can also use more than one condition at once, for this there is \u0026amp;\u0026amp; which means and and || which means or. You can write both symbols or letters in the code. And it can look like this if (hp \u0026lt;= and armor \u0026lt;= 0)\nIf our condition returns False because the number of hp is greater than 0, we go to else. In else, we simply move our object diagonally and subtract 1 hp. If condition is checked every frame and after a few seconds the number of hp will be less than 0 and the object will be destroyed.\nif (hp \u0026lt;= 0) { instance_destroy() } else { x = x + 1 y = y + 1 hp = hp - 1 } If we need to use several conditions, we can use else if. For example, if there is less than 50 hp left, add another object\nif (hp \u0026lt;= 0) { instance_destroy() } else if (hp \u0026lt;= 50) { instance_create_layer(x, y, \u0026#34;Instances\u0026#34;, obj_vars) } else { x = x + 1 y = y + 1 hp = hp - 1 } If there are many conditions, then it is better to use switch.\nSwitch This is what a switch looks like\nswitch (\u0026lt;expression\u0026gt;) { case \u0026lt;constant1\u0026gt;: \u0026lt;code\u0026gt; break; case \u0026lt;constant2\u0026gt;: \u0026lt;code\u0026gt; break; // more cases (with breaks) default: \u0026lt;code\u0026gt;; } For example, we can perform some action depending on which key is pressed. In the parentheses next to the switch, we specify keyboard_key, which is a built-in parameter that takes a value depending on which key is pressed. And then we describe each case. If the up key is pressed, we move the hero up, if down, then down, and so on\nswitch (keyboard_key) { case vk_left: x -= 4; break; case vk_right: x += 4; break; case vk_up: y -= 4; break; case vk_down: y += 4; break; } There may also be a default action in the case, it is performed if all other cases returned false.\nVideo ","permalink":"https://mpostument.com/posts/programming/game_maker/conditions/","summary":"Hello!\nLet\u0026rsquo;s explore conditions in Game Maker. Conditions can be created using if/else or switch statements. Let\u0026rsquo;s try both options.\nIf/Else In Game Maker if/else conditions are the same as in other programming languages. The block itself looks like this:\nif (\u0026lt;expression\u0026gt;) { \u0026lt;statement\u0026gt;; } else { \u0026lt;statement\u0026gt;; } For example, we can check the health of our object. If hp \u0026lt; 0, we destroy the object. In order to do this, we will use a new type of event called Step.","title":"Game Maker - Conditions"},{"content":"Hello!\nLet\u0026rsquo;s consider how coordinates work in Game Maker.\nRoom coordinates I will open the room (Room1) and if you move the mouse to the upper left corner, the coordinates x = 0, y = 0 will be there. If you move to the right/left, the x coordinate will change, and if you move up/down, the y coordinate will change.\nThis is the basic information you need to know by coordinates. Now let\u0026rsquo;s try to add an object and move it\nObject coordinates The object can be added to the room by any available coordinates. I will create an object. But for the object to be visible in the room, it needs a sprite. You can simply paint the object with some color. To do this, on the object page, you can click New Sprite or Ctrl+Shift+N. A sprite window will open immediately where you can click Edit Image, then a window will open in which you can draw your sprite\nBut a sprite will be created with the name Sprite1 and in the objects folder. It should be renamed and moved to the Sprites folder. I will rename it to spr_vars so that it is clear that the sprite refers to my object called obj_vars.\nNow you can open the room and drag the object there\nWe also modify the object so that it prints the coordinates in which it is located.\nI will add the event Create to the object in which I will specify the initial coordinates\nx = 0 y = 0 I will also add the Draw event in which I will display the coordinates. I will use the draw_text which is a method already known from previous posts. Before displaying the text, you need to add draw_self() to draw the object itself. And then two execution of the draw_text method for the X and Y coordinates. As the first argument, I pass x, which is equal to the x coordinate of the object. The same with y, but I shift the text by the y coordinate so that the text does not overlap with each other. And I print the following text \u0026ldquo;X: 0\u0026rdquo;. To do this, I add the string \u0026ldquo;X:\u0026rdquo; to the x coordinate. But since the x coordinate is an int, I first need to convert it to the string type.\ndraw_self() draw_text(x, y+5, \u0026#34;X:\u0026#34; + string(x)) draw_text(x, y+20, \u0026#34;Y:\u0026#34; + string(y)) Press F5 to start the game\nNow let\u0026rsquo;s add the ability to move the object and see the coordinates of the object. I\u0026rsquo;ll do it with the Global Left Down type event. why do you need global? To be able to click anywhere in the room. If you use Left Down, the click will be counted only if it is made on the object. And I will add the following code\nx = mouse_x y = mouse_y This code changes the coordinates of the object to the coordinates of the mouse cursor\nOrigin Every time we click, the upper left corner of the object is placed at this point. This is because the origin is set there. Origin is specified at the sprite level and can be changed. To do this, you need to open the sprite and you will immediately see that the origin is set to the upper left corner. With the help of the mouse, you can move it to any point or choose one of the existing options such as Top Left, Top Center, and others.\nIf I move the origin to another point and start the game and move the object, then exactly the place specified as the origin will fit in this point\nBut now, as you can see, the text that we display has shifted. Because it displays according to origin. Therefore, to redraw the text inside the object, the coordinates in the draw_text method need to be changed\nFor example, this is the case if the origin is in the center\ndraw_self() draw_text(x-20, y-15, \u0026#34;X:\u0026#34; + string(x)) draw_text(x-20, y, \u0026#34;Y:\u0026#34; + string(y)) Video ","permalink":"https://mpostument.com/posts/programming/game_maker/coordinates/","summary":"Hello!\nLet\u0026rsquo;s consider how coordinates work in Game Maker.\nRoom coordinates I will open the room (Room1) and if you move the mouse to the upper left corner, the coordinates x = 0, y = 0 will be there. If you move to the right/left, the x coordinate will change, and if you move up/down, the y coordinate will change.\nThis is the basic information you need to know by coordinates.","title":"Game Maker - Coordinates"},{"content":"Hello!\nToday we will talk about variables in golang. There are three ways to create variables, let\u0026rsquo;s try each of them.\nVar keyword The first way is to create a variable using the var keyword. This is how you can create a variable that will have no value (will have an initial value depending on variable type). It looks like this var PARAM_NAME PARAM_TYPE for example var level int\npackage main import \u0026#34;fmt\u0026#34; func main() { var level int fmt.Println(level) } $ go run main.go 0 Even if we did not specify a value for the level variable, it will still have a value. Now after a variable is created, it can be changed.\npackage main import \u0026#34;fmt\u0026#34; func main() { var level int level = 10 fmt.Println(level) } $ go run main.go 10 Also, this variable can be moved outside of the function and the code will still work.\npackage main import \u0026#34;fmt\u0026#34; var level int func main() { level = 10 fmt.Println(level) } If you need to create several variables at the same time, you can do it this way in order not to repeat the word var\nvar ( level int exp int life int ) Var keyword with a value Also, when creating a variable, an initial value can be set for it. var PARAM_NAME PARAM_TYPE = PARAM_VALUE, for example, var level int = 10.\npackage main import \u0026#34;fmt\u0026#34; func main() { var level int = 10 fmt.Println(level) } $ go run main.go 10 Since we specify the value of the variable immediately, the type can be omitted as var level = 10 and the program will work without changes.\npackage main import \u0026#34;fmt\u0026#34; func main() { var level = 10 fmt.Println(level) } $ go run main.go 10 This variable, like the previous one, can be moved outside the function and the code will work. If you need to create several variables at the same time, you can do it similarly as before\nvar ( level = 10 exp = 300 life = 4 ) Variables without var The third way for creating a variable is level := 10. The word var is no longer used here, but such variables can only be created inside functions\npackage main import \u0026#34;fmt\u0026#34; func main() { level := 10 fmt.Println(level) } $ go run main.go 10 And if I try to take the variable outside the function, I get the following\ngo run main.go # command-line-arguments ./main.go:5:1: syntax error: non-declaration statement outside the function body Variables Naming Usually in golang variables are named as follows. If the variable does not start with a capital letter, it means that it is available only within this package.\nvar level = 1 If the variable starts with a capital letter, then this variable becomes available for use in other packages\nvar Level = 1 If the variable consists of more than one word, then its name also depends on whether it should be available only inside the package or from others. The second word in the variable name must always begin with an uppercase letter\nvar heroLevel = 1 var HeroLevel = 1 Variables conversion If we have a variable of one type, it can be converted to another type. For example, we have an int and we need it to be a float. This can be done as follows\npackage main import \u0026#34;fmt\u0026#34; func main() { var level int = 10 levelFloat := float64(level) fmt.Printf(\u0026#34;%v: %T\u0026#34;, levelFloat, levelFloat) } go run main.go 10: float64 Now let\u0026rsquo;s try to do the same but with the string type\npackage main import \u0026#34;fmt\u0026#34; func main() { var level int = 123 levelFloat := string(level) fmt.Printf(\u0026#34;%v: %T\u0026#34;, levelFloat, levelFloat) } go run main.go {: string The result is not exactly what we expected. When converting, golang takes the Unicode value under this number. And under the number 123 there is the value {.\nSo how to convert int to string? For this, go has special methods in the strconv package, for example strconv.Itoa to convert int to string\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;strconv\u0026#34; ) func main() { var level int = 123 levelFloat := strconv.Itoa(level) fmt.Printf(\u0026#34;%v: %T\u0026#34;, levelFloat, levelFloat) } $ go run main.go 123: string Shadowing If we have a global variable outside the function and we create a variable with the same name in the function, this is called shadowing. Let\u0026rsquo;s check an example\npackage main import ( \u0026#34;fmt\u0026#34; ) var level int = 1 func main() { fmt.Println(level) var level = 20 fmt.Println(level) printLevel() } func printLevel() { fmt.Println(level) } Here we have a package-level variable with a value of 1, we print this variable and then create another variable with the same name inside the function. And we print its value. We also have another method printLevel method that also prints the value of the level variable.\n$ go run main.go 1 20 1 As can be seen from the result, first the output of the package level variable took place, then the output of the local variable from the function, and at the end the package variable again\nVideo ","permalink":"https://mpostument.com/posts/programming/golang/basics/go-variables/","summary":"Hello!\nToday we will talk about variables in golang. There are three ways to create variables, let\u0026rsquo;s try each of them.\nVar keyword The first way is to create a variable using the var keyword. This is how you can create a variable that will have no value (will have an initial value depending on variable type). It looks like this var PARAM_NAME PARAM_TYPE for example var level int\npackage main import \u0026#34;fmt\u0026#34; func main() { var level int fmt.","title":"Go Variables"},{"content":"Hello!\nLet\u0026rsquo;s dive into the world of game development with Game Maker 2. And today we\u0026rsquo;ll start with options in Game Maker. Let\u0026rsquo;s create a new project by selecting New Blank and naming its variables.\nObject Parameters can be added to objects. The object can be almost anything, starting with the hero and enemies, and ending with objects that control some logic in the game. Right-click the Objects folder in the Assets Browser to create an object and select Create -\u0026gt; Object.\nAn object named Object1 will be created. It needs to be renamed immediately. It is recommended to add a prefix to the name because if we have an object with the name hero and we want to add a sprite that will also be called hero, we will get an error that a file with that name already exists. Therefore, each object must have the obj_ prefix. Thus, the object that I will create will be called obj_vars.\nAfter creation, the object will open immediately. Here you can specify a sprite for the object, specify whether it will be visible, and a few more options. Now we are interested in the events window. This is where we will write the code that will control our object.\nClick the Add Event button and select Create. Event type Create is called when the object is created and it is convenient to set some initial characteristics here. Double-click the left mouse button to open the code editor.\nLocal Variables In the editor, we will create heroExp parameters, which will be responsible for the experience of our object. These parameters are of type integer. We will also add the parameter heroName, which will be of type string, and the parameter heroAlive, which will be of type bool\nheroExp = 50 heroName = \u0026#34;Maksym\u0026#34; heroAlive = true Now let\u0026rsquo;s add another type of event called a draw. It is responsible for displaying the object. To do this, click Add Event again and select Draw -\u0026gt; Draw in the list. I will open the editor for this type of event and write the following in the code\ndraw_text(20, 20, heroExp) draw_text(20, 40, heroName) draw_text(20, 60, heroAlive) The draw_text function is used to display text, the first two parameters are the x and y coordinates, and the third parameter is the text to be displayed. Let\u0026rsquo;s now add our object to the room and try it and see what happens.\nAdd the object to a room In the Assets Browser you need to open the Rooms folder and double-click on Room1. The room window will open. In the top left, Instances must be selected to be able to add objects to the room. Now our object can be simply dragged into the room. It is also worth changing the background color so that the text displaying the object is visible. To do this, in the room options, you need to select ``Background\u0026rsquo;\u0026rsquo; and choose a color (Color option). I chose black because the text will be white.\nNow that everything is ready, press F5 to start the game.\nDynamically change variables value Now let\u0026rsquo;s try to add an event that will change the value of our parameters. Let\u0026rsquo;s add the Key Press - Up event, which will be called every time the Up button is pressed on the keyboard, and the Key Press - Space event. The up button will add experience to our object and the space button will kill our object.\nAdd the following to the Key Press - Up code\nheroExp++ And next code to Key Press - Space\nheroAlive = false And let\u0026rsquo;s start the game. If you press Up or Space, the parameters will be changed.\nGlobal Vars Local parameters are available only within one object. If you want to add a parameter to which all objects will have access, you need to add global. before the name of the parameter. Example\nglobal.heroAlive = true And now this parameter can be changed or used in other objects\nVideo ","permalink":"https://mpostument.com/posts/programming/game_maker/variables/","summary":"Hello!\nLet\u0026rsquo;s dive into the world of game development with Game Maker 2. And today we\u0026rsquo;ll start with options in Game Maker. Let\u0026rsquo;s create a new project by selecting New Blank and naming its variables.\nObject Parameters can be added to objects. The object can be almost anything, starting with the hero and enemies, and ending with objects that control some logic in the game. Right-click the Objects folder in the Assets Browser to create an object and select Create -\u0026gt; Object.","title":"Game Maker - Variables"},{"content":"Hello!\nThis post is the start of a series of posts about golang. In this one, we will install golang and configure vscode.\nInstall go The current version of golang can be downloaded from official site. I\u0026rsquo;m using Linux so I\u0026rsquo;ll download the Linux version\n$ wget https://go.dev/dl/go1.19.2.linux-amd64.tar.gz --2022-10-28 22:35:40-- https://go.dev/dl/go1.19.2.linux-amd64.tar.gz Resolving go.dev (go.dev)... 216.239.36.21, 216.239.34.21, 216.239.38.21, ... Connecting to go.dev (go.dev)|216.239.36.21|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://dl.google.com/go/go1.19.2.linux-amd64.tar.gz [following] --2022-10-28 22:35:40-- https://dl.google.com/go/go1.19.2.linux-amd64.tar.gz Resolving dl.google.com (dl.google.com)... 216.58.215.78, 2a00:1450:401b:810::200e Connecting to dl.google.com (dl.google.com)|216.58.215.78|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 148883574 (142M) [application/x-gzip] Saving to: ‘go1.19.2.linux-amd64.tar.gz’ go1.19.2.linux-amd64.tar.gz 100%[================================================================\u0026gt;] 141,99M 20,8MB/s in 13s 2022-10-28 16:36:03 (10,6 MB/s) - ‘go1.19.2.linux-amd64.tar.gz’ saved [148883574/148883574] Now you need to unpack the archive to /usr/local folder\nsudo tar -C /usr/local -xzf go1.19.2.linux-amd64.tar.gz For the go command to become available, it must be added to the PATH. I use zsh, so I will add the path to go to the ~/.zshrc file\nexport PATH=$PATH:/usr/local/go/bin After restarting the terminal, the go command is available\n$ go version go version go1.19.2 linux/amd64 Vscode install To make it convenient to write go code, I use the Visual Studio Code text, editor. After the editor is installed, you need to install the Go application. To install the application, press Ctrl+Shift+X or click on . Enter go in the search, select Go, and click Install.\nFor the go application to work, you need to install additional go libraries. To do this, you need to press Ctrl+Shift+P and write go: install/update tools. Select all libraries from the list and click OK. Vscode output will be produced\nTools environment: GOPATH=/home/maksym/go Installing 7 tools at /home/maksym/go/bin in module mode. gotests gomodifytags impl goplay dlv golangci-lint gopls Installing github.com/cweill/gotests/gotests@latest (/home/maksym/go/bin/gotests) SUCCEEDED Installing github.com/fatih/gomodifytags@latest (/home/maksym/go/bin/gomodifytags) SUCCEEDED Installing github.com/josharian/impl@latest (/home/maksym/go/bin/impl) SUCCEEDED Installing github.com/haya14busa/goplay/cmd/goplay@latest (/home/maksym/go/bin/goplay) SUCCEEDED Installing github.com/go-delve/delve/cmd/dlv@latest (/home/maksym/go/bin/dlv) SUCCEEDED Installing github.com/golangci/golangci-lint/cmd/golangci-lint@latest (/home/maksym/go/bin/golangci-lint) SUCCEEDED Installing golang.org/x/tools/gopls@latest (/home/maksym/go/bin/gopls) SUCCEEDED All tools successfully installed. You are ready to Go. :) Hello World Now let\u0026rsquo;s create the first program on go. To begin with, I will create a directory in which the code will be stored\nmkdir -p ~/go/src/github.com/mpostument/hello_world In the hello_world folder, you first need to initialize the go module.\n$ go mod init go: creating new go.mod: module github.com/mpostument/hello_world In the hello_world folder, you first need to initialize the go module with go mod init command. If the command ends successfully, the file go.mod will appear, which will contain information about all 3rd party libraries used in module. But now this file only contains information about the version of go and the name of the module.\nmodule github.com/mpostument/hello_world go 1.19 Now you can start writing code. Let\u0026rsquo;s create a file main.go and add the following code to it\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;Hello World!\u0026#34;) } Since we are not writing a library, but creating executable, the package name should be main. Next, is the libraries import, we need fmt to print text to console. The main function is called as we execute binary. In main function, we print text to the terminal using the Println method from the fmt library.\nThe code can be run in two ways. Compile the binary or run with go run.\n$ go run main.go Hello World! $ go build $ ls go.mod hello_world main.go $ ./hello_world Hello World! Video ","permalink":"https://mpostument.com/posts/programming/golang/basics/setup-go/","summary":"Hello!\nThis post is the start of a series of posts about golang. In this one, we will install golang and configure vscode.\nInstall go The current version of golang can be downloaded from official site. I\u0026rsquo;m using Linux so I\u0026rsquo;ll download the Linux version\n$ wget https://go.dev/dl/go1.19.2.linux-amd64.tar.gz --2022-10-28 22:35:40-- https://go.dev/dl/go1.19.2.linux-amd64.tar.gz Resolving go.dev (go.dev)... 216.239.36.21, 216.239.34.21, 216.239.38.21, ... Connecting to go.dev (go.dev)|216.239.36.21|:443... connected. HTTP request sent, awaiting response... 302 Found Location: https://dl.","title":"Golang - Setting Up Development Environment"},{"content":"Hello!\nWhat if we need to create a user in the database and store their password securely in code? This can be done using ansible vault. Ansible vault allows you to encrypt both the entire file and individual variables.\nEncrypted file Let\u0026rsquo;s create a file that will contain our variables.\ndb_password: secure_password! admin_password: super_securee_password! We cannot simply leave this file because it stores passwords. But it can be encrypted using the ansible-vault encrypt VAR_FILE command. You will need to enter a password that will be used for decryption.\n$ ansible-vault encrypt 1.yml New Vault password: Confirm New Vault password: Encryption successful And the file itself will look like this and can be added to source control.\n$ANSIBLE_VAULT;1.1;AES256 32393936363535383238323361373334376664613530346332336333653066366437336232313761 3638396339376462636630336161376161396637366338360a653962326431616266656463613966 32366130613763343937366363623864646263616664323231633662626333353331613537613733 3132393337346431660a613037333166626563306634346461303538383237376263373734626666 64623136336535323230303631313736376162363633626264346538646163363339643438396633 62653234373230323538616563306130353334333931616338363561346530633335346562363837 62656166613939303139653233346466393636666637353438376230393935336638363664303037 64313162363661396161 This file is a normal file with variables, but when running ansible-playbook you need to pass a password for decryption. There are several ways to do it. First, with `\u0026ndash;ask-vault-pass\u0026rsquo; key, in this case, Ansible will ask for a password at startup. Accordingly, this option is not suitable for automatically starting Ansible, because the password must be entered every time.\nLet\u0026rsquo;s use the following code for the test, where vars are our encrypted variables\n--- - name: Run locally hosts: localhost connection: local vars_files: - vars.yml tasks: - name: Print var debug: msg: \u0026#34;{{ db_password }}\u0026#34; $ ansible-playbook -i \u0026#34;localhost,\u0026#34; main.yml --ask-vault-pass Vault password: PLAY [Run locally] ***************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************ ok: [localhost] TASK [Print var] ****************************************************************************************************************** ok: [localhost] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;secure_password\u0026#34; } PLAY RECAP ************************************************************************************************************************ localhost : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 The next option is using a file with a password. For this, a file is created in which the password will be stored and ansible is launched with the key --vault-password-file PATH_TO_FILE\n$ ansible-playbook -i \u0026#34;localhost,\u0026#34; main.yml --vault-password-file .vault_pass PLAY [Run localy] ***************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************ ok: [localhost] TASK [Print var] ****************************************************************************************************************** ok: [localhost] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;secure_password\u0026#34; } PLAY RECAP ************************************************************************************************************************ localhost : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 If you run it without these variables, you will get\nansible-playbook -i \u0026#34;localhost,\u0026#34; main.yml ERROR! Attempting to decrypt but no vault secrets found Decrypt file To edit the encrypted file, you can use the ansible-vault edit ENCRYPTED_FILE command, enter the password and edit the file.\nChange vault password The password can be changed using the ansible-vault rekey VAULT_FILE command\nansible-vault rekey vars.yml Vault password: New Vault password: Confirm New Vault password: Rekey successful Encrypted variable If you do not need to encrypt an entire file, but only one parameter, this can also be done using the `ansible-vault encrypt_string \u0026lsquo;TEXT_TO_ENCRYPT\u0026rsquo; \u0026ndash;name \u0026lsquo;VARIABLE_NAME\u0026rsquo;\u0026rsquo; command.\nansible-vault encrypt_string \u0026#39;secure\u0026#39; --name db_password New Vault password: Confirm New Vault password: Encryption successful db_password: !vault | $ANSIBLE_VAULT;1.1;AES256 32613335323933306136303139616131303039396139646332656337343532383034336439363739 3032376165376462653533633731626366613337356234340a353662343931366436643635383666 32313163373765333666623065373930616431333365656331376362316464363038636136346138 3033376563636437630a663533383336353434336566633935396137373737303662636432326433 3630 As a result, we get a parameter that can be added to other unencrypted variables. But as with an encrypted file, when you run ansible, you need to pass the password for decryption. If we encrypt several variables, they must have one password for decryption\napp_port: 8080 db_user: postgres db_password: !vault | $ANSIBLE_VAULT;1.1;AES256 32613335323933306136303139616131303039396139646332656337343532383034336439363739 3032376165376462653533633731626366613337356234340a353662343931366436643635383666 32313163373765333666623065373930616431333365656331376362316464363038636136346138 3033376563636437630a663533383336353434336566633935396137373737303662636432326433 3630 To not specify the password in the terminal when encrypting, you can also use a file with a password using the --vault-password-file key.\nansible-vault encrypt_string --vault-password-file pass \u0026#39;secure\u0026#39; --name db_password Encryption successful db_password: !vault | $ANSIBLE_VAULT;1.1;AES256 31623232663866376238643739353231323835646530326531333766623831363637396130363330 6139303362643962343432353765643639623339396134370a653236336433613064303530623933 34633765636536333935363833323537356134653333326562363435653437366639393136316336 6462363437633130330a366439653933353766326565623464353364626234623862666265643539 3338 For the test, we will use the following code:\n--- - name: Run locally hosts: localhost connection: local vars: admin_password: !vault | $ANSIBLE_VAULT;1.1;AES256 31386261643538643135333865383233656435366161333865393232643035393265393963646331 6130636537343866653733623938363030333739313031300a393663356165343038656164323365 32303230376536306633633936303430396234343961393862323434663435626136313062663032 3661653564353539650a323733376635306562383263613738383934313931353062633266323762 34343231386139303737613538346430633336383165333065303532373738636661 tasks: - name: Print var debug: msg: \u0026#34;{{ admin_password }}\u0026#34; And I will get the result\n$ ansible-playbook -i \u0026#34;localhost,\u0026#34; main.yml --vault-password-file .vault_pass PLAY [Run localy] ***************************************************************************************************************** TASK [Gathering Facts] ************************************************************************************************************ ok: [localhost] TASK [Print var] ****************************************************************************************************************** ok: [localhost] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;secure_admin_password\u0026#34; } PLAY RECAP ************************************************************************************************************************ localhost : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Video ","permalink":"https://mpostument.com/posts/programming/ansible/ansible-vault/","summary":"Hello!\nWhat if we need to create a user in the database and store their password securely in code? This can be done using ansible vault. Ansible vault allows you to encrypt both the entire file and individual variables.\nEncrypted file Let\u0026rsquo;s create a file that will contain our variables.\ndb_password: secure_password! admin_password: super_securee_password! We cannot simply leave this file because it stores passwords. But it can be encrypted using the ansible-vault encrypt VAR_FILE command.","title":"Ansible Vault"},{"content":"Hello!\nSo what is a role in Ansible? Roles are a logical organization of Ansible code. For example, you can create a role that will install a MySQL database, put it in a separate repository, and make it publicly available or shared with other teams, and everyone should be able to use it\nCreate a Role So how to create a role? You can create all the necessary files and folders manually or use the command ansible-galaxy role init ROLE_NAME.\nansible-galaxy role init roles/httpd - Role roles/httpd was created successfully I specify roles/httpd for ansible to create the roles folder. If roles/ is not specified, the role will be created in the folder where the command was executed.\n$ tree roles roles └── httpd ├── README.md ├── defaults │ └── main.yml ├── files ├── handlers │ └── main.yml ├── meta │ └── main.yml ├── tasks │ └── main.yml ├── templates ├── tests │ ├── inventory │ └── test.yml └── vars └── main.yml 9 directories, 8 files What is inside the roles/httpd folder?\ndefaults - here are the parameters that have the lowest priority, that is, the default values ​​of the parameters that can then be overwritten. files - static files required for the role handlers - handlers, this is code that is always executed at the very end, under certain conditions. For example, restart the service if the configuration file has changed. meta - meta-information about the role. Title, version, author, etc tasks - here are the tasks that will be performed by Ansible templates - a place for files that will be dynamically generated using jinja2 tests - tests vars - parameters that have a higher priority than the default The main folder in which we will work tasks. Our role will install httpd, so in main.yml we will add the code that will install httpd depending on the OS.\n- name: Install apache on Redhat ansible.builtin.yum: name: httpd state: latest become: yes when: ansible_facts[\u0026#39;os_family\u0026#39;] == \u0026#34;RedHat\u0026#34; - name: Install apache on Debian ansible.builtin.apt: name: apache2 state: latest become: yes when: ansible_facts[\u0026#39;os_family\u0026#39;] == \u0026#34;Debian\u0026#34; After running the playbook, the httpd package will be installed but not started. To start it and enable automatic start after OS restart, we will use the service module\n- name: Start service httpd, if not started ansible.builtin.service: name: apache2 state: started enabled: yes become: yes when: ansible_facts[\u0026#39;os_family\u0026#39;] == \u0026#34;Debian\u0026#34; - name: Start service httpd, if not started ansible.builtin.service: name: httpd state: started enabled: yes become: yes when: ansible_facts[\u0026#39;os_family\u0026#39;] == \u0026#34;RedHat\u0026#34; Now ansible will start the httpd service on the server. Let\u0026rsquo;s add the index.html page using a template. To do this, I will create an index.html.j2 file with the following content in the templates folder\n\u0026lt;b\u0026gt;Hello {{ user_name }}!\u0026lt;/b\u0026gt; Where {{ user_name }} is an ansible parameter that will be inserted into the generated file. But for this, you need to create a parameter. I will create it in defaults/main.yml.\nuser_name: Maksym And finally, there is a task that will generate the file. I will add it to tasks/main.yml.\n- name: Generate index.html template: src: index.html.j2 dest: /var/www/html/index.html become: true We can also use handlers. For example, if you want httpd to restart every time index.html is updated. To do this, I will add the following to handlers/main.yml\n- name: Restart service httpd Debian ansible.builtin.service: name: apache2 state: restarted become: yes when: ansible_facts[\u0026#39;os_family\u0026#39;] == \u0026#34;Debian\u0026#34; - name: Restart service httpd Redhat ansible.builtin.service: name: httpd state: restarted become: yes when: ansible_facts[\u0026#39;os_family\u0026#39;] == \u0026#34;RedHat\u0026#34; This is similar to the code that start httpd in the tasks folder. But this code will be called only when someone notifies handlers to run, and in our case, it will be called by creating index.html. In the code where we create index.html, we need to add a handler call.\n- name: Generate index.html template: src: index.html.j2 dest: /var/www/html/index.html become: true notify: - Restart service httpd Debian - Restart service httpd Redhat Here I am calling two handlers, but only one will actually be executed depending on the OS.\nNow the role can be used. To do this, in our main main.yml we can specify the role which we are going to use. But without roles/ in the name.\n--- - name: Configure webserver hosts: web gather_facts: true roles: - httpd main.yml located outside of roles folders. This is what the final file structure looks like\n$ tree ansible ansible ├── inventory ├── main.yml └── roles └── httpd ├── README.md ├── defaults │ └── main.yml ├── files ├── handlers │ └── main.yml ├── meta │ └── main.yml ├── tasks │ └── main.yml ├── templates │ └── index.html.j2 ├── tests │ ├── inventory │ └── test.yml └── vars └── main.yml 10 directories, 11 files And the result\n$ ansible-playbook -i inventory main.yml PLAY [Configure webserver] ************************************************************************************************* TASK [Gathering Facts] ***************************************************************************************************** ok: [44.201.242.83] TASK [httpd : Install apache on redhat] ************************************************************************************ changed: [44.201.242.83] TASK [httpd : Start httpd] ************************************************************************************************* changed: [44.201.242.83] TASK [httpd : Generate index.html] ***************************************************************************************** changed: [44.201.242.83] RUNNING HANDLER [httpd : Restart httpd] ************************************************************************************ changed: [44.201.242.83] PLAY RECAP ***************************************************************************************************************** 44.201.242.83 : ok=5 changed=4 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Ansible galaxy The Ansible community has many great roles created. You can find them at ansible galaxy. For example, after entering httpd in the search, I found a role that installs httpd. Here you can read the role documentation and instructions on how to download it. This would normally be ansible-galaxy install ROLE_NAME\n$ ansible-galaxy install buluma.httpd Starting galaxy role install process - downloading role \u0026#39;httpd\u0026#39;, owned by buluma - downloading role from https://github.com/buluma/ansible-role-httpd/archive/1.0.9.tar.gz - extracting buluma.httpd to /home/maksym/.ansible/roles/buluma.httpd - buluma.httpd (1.0.9) was installed successfully Now I can specify this role in main.yml\n--- - name: Configure webserver hosts: web gather_facts: true become: true roles: - buluma.httpd ansible-playbook -i inventory main.yml PLAY [Configure webserver] ************************************************************************************************* TASK [Gathering Facts] ***************************************************************************************************** ok: [44.201.242.83] TASK [buluma.httpd : Test if httpd_servername is set correctly] ************************************************************ ok: [44.201.242.83 -\u0026gt; localhost] TASK [buluma.httpd : Test if httpd_port is set correctly] ****************************************************************** ok: [44.201.242.83 -\u0026gt; localhost] TASK [buluma.httpd : Test if https_ssl_enable is set correctly] ************************************************************ ok: [44.201.242.83 -\u0026gt; localhost] TASK [buluma.httpd : Test if httpd_ssl_servername is set correctly] ******************************************************** ok: [44.201.242.83 -\u0026gt; localhost] TASK [buluma.httpd : Test if httpd_ssl_port is set correctly] ************************************************************** ok: [44.201.242.83 -\u0026gt; localhost] TASK [buluma.httpd : Test if httpd_locations is set correctly] ************************************************************* skipping: [44.201.242.83] TASK [buluma.httpd : Test if item in httpd_locations is set correctly] ***************************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Test if httpd_vhosts is set correctly] **************************************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Test if item in httpd_vhosts is set correctly] ******************************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Test if item.create_docroot in httpd_vhosts is set correctly] ***************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Test if item.serveralias in httpd_vhosts is set correctly] ******************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Test if httpd_directories is set correctly] *********************************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Test if item.name in httpd_directories is set correctly] ********************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Test if item.options in httpd_directories is set correctly] ******************************************* skipping: [44.201.242.83] TASK [buluma.httpd : Test if item.allow_override in httpd_directories is set correctly] ************************************ skipping: [44.201.242.83] TASK [buluma.httpd : Test if httpd_remove_example is set correctly] ******************************************************** ok: [44.201.242.83 -\u0026gt; localhost] TASK [buluma.httpd : Install apache httpd] ********************************************************************************* ok: [44.201.242.83] TASK [buluma.httpd : Modify selinux settings] ****************************************************************************** skipping: [44.201.242.83] =\u0026gt; (item=httpd_can_network_connect) TASK [buluma.httpd : Allow connections to custom port] ********************************************************************* skipping: [44.201.242.83] =\u0026gt; (item=80) skipping: [44.201.242.83] =\u0026gt; (item=443) TASK [buluma.httpd : Configure httpd] ************************************************************************************** changed: [44.201.242.83] TASK [buluma.httpd : Install ssl packages] ********************************************************************************* skipping: [44.201.242.83] TASK [buluma.httpd : Place configuration] ********************************************************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Configure redirect from http to https] **************************************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Enable modules] *************************************************************************************** skipping: [44.201.242.83] =\u0026gt; (item=proxy) skipping: [44.201.242.83] =\u0026gt; (item=proxy_http) TASK [buluma.httpd : Configure locations] ********************************************************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Create docroot] *************************************************************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Configure vhosts] ************************************************************************************* skipping: [44.201.242.83] TASK [buluma.httpd : Configure ports] ************************************************************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Create directories] *********************************************************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Configure directories] ******************************************************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Flush handlers] *************************************************************************************** RUNNING HANDLER [buluma.httpd : Test httpd configuration validity] ********************************************************* ok: [44.201.242.83] RUNNING HANDLER [buluma.httpd : Restart httpd] ***************************************************************************** changed: [44.201.242.83] TASK [buluma.httpd : Removing default html] ******************************************************************************** skipping: [44.201.242.83] TASK [buluma.httpd : Start and enable httpd] ******************************************************************************* ok: [44.201.242.83] PLAY RECAP ***************************************************************************************************************** 44.201.242.83 : ok=12 changed=2 unreachable=0 failed=0 skipped=23 rescued=0 ignored=0 Video ","permalink":"https://mpostument.com/posts/programming/ansible/ansible-role/","summary":"Hello!\nSo what is a role in Ansible? Roles are a logical organization of Ansible code. For example, you can create a role that will install a MySQL database, put it in a separate repository, and make it publicly available or shared with other teams, and everyone should be able to use it\nCreate a Role So how to create a role? You can create all the necessary files and folders manually or use the command ansible-galaxy role init ROLE_NAME.","title":"Ansible Role"},{"content":"Hello!\nToday I will show what ansible-playbook is and how to create it.\nPlaybook Ansible Playbooks is an iterative, reusable, configuration management and deployment framework that is well-suited for deploying complex applications. If you need to run a task with Ansible more than once, you can do it with a playbook.\nLet\u0026rsquo;s create the simplest playbook. In the playbook, you need to specify its name, the group of servers on which it will be executed, and the tasks themselves.\n--- - name: Update web servers hosts: webapp tasks: - name: Ensure apache is at the latest version ansible.builtin.yum: name: httpd state: latest become: true In this example, the playbook is executed on the webapp server group. On all servers from this group, ansible installs the latest version of the httpd package using the yum module. Before executing the task, ansible checks whether the package is installed, if it is installed, nothing will happen.\nThe playbook is started using the ansible-playbook command. Let\u0026rsquo;s save our playbook under the name main.yml and run it\n$ ansible-playbook -i inventory main.yml PLAY [Update web servers] ****************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************* ok: [3.92.61.18] TASK [Ensure apache is at the latest version] ********************************************************************************** changed: [3.92.61.18] PLAY RECAP ********************************************************************************************************************* 3.92.61.18 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Ansible host variables Before the playbook starts executing, ansible collects information about the server. This option can be disabled by specifying gather_facts: false in the playbook. Ansible collects a lot of information about the operating system, network settings, and more in this way\n{ \u0026#34;ansible_all_ipv4_addresses\u0026#34;: [ \u0026#34;REDACTED IP ADDRESS\u0026#34; ], \u0026#34;ansible_all_ipv6_addresses\u0026#34;: [ \u0026#34;REDACTED IPV6 ADDRESS\u0026#34; ], \u0026#34;ansible_apparmor\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;disabled\u0026#34; }, \u0026#34;ansible_architecture\u0026#34;: \u0026#34;x86_64\u0026#34;, \u0026#34;ansible_bios_date\u0026#34;: \u0026#34;10/02/2022\u0026#34;, \u0026#34;ansible_bios_version\u0026#34;: \u0026#34;4.1.5\u0026#34;, To see all information, you can call the run command ansible command setup, and all parameters will be printed.\nansible -i inventory all -m ansible.builtin.setup Debug Sometimes you need to debug the execution of the playbook or look at the values of some parameters, all this can be done using the debug module.\nFor example, I need to see the value of the ansible_bios_version parameter, obtained using gather_facts in the playbook startup process. This can be done like this:\n- name: Print bios version ansible.builtin.debug: msg: Bios version is {{ ansible_bios_version }} $ ansible-playbook -i inventory main.yml ✘ INT  ansible-210 3.9.13 17:29:55 PLAY [Update web servers] ****************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************* ok: [3.92.61.18] TASK [Print bios version] ****************************************************************************************************** ok: [3.92.61.18] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;Bios version is 4.11.amazon\u0026#34; } PLAY RECAP ********************************************************************************************************************* 3.92.61.18 : ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Register and Set Facts Also, in Ansible, you can create parameters based on the result of the task. For example, I want to get how long the system has been running. This can be done by calling the Linux uptime command. But if I do it like this:\n- name: Get uptime information ansible.builtin.shell: /usr/bin/uptime $ ansible-playbook -i inventory main.yml PLAY [Update web servers] ****************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************* ok: [3.92.61.18] TASK [Get uptime information] ************************************************************************************************** changed: [3.92.61.18] PLAY RECAP ********************************************************************************************************************* 3.92.61.18 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 As a result, I get nothing. To get a response from the uptime command, I need to save the result in a variable. This is done using register: VAR_NAME and then the value of this variable can be output to the terminal.\n- name: Get uptime information ansible.builtin.shell: /usr/bin/uptime register: sys_uptime - name: Print system uptime ansible.builtin.debug: msg: \u0026#34;{{ sys_uptime }}\u0026#34; $ ansible-playbook -i inventory main.yml PLAY [Update web servers] ****************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************* ok: [3.92.61.18] TASK [Get uptime information] ************************************************************************************************** changed: [3.92.61.18] TASK [Print system uptime] ***************************************************************************************************** ok: [3.92.61.18] =\u0026gt; { \u0026#34;msg\u0026#34;: { \u0026#34;changed\u0026#34;: true, \u0026#34;cmd\u0026#34;: \u0026#34;/usr/bin/uptime\u0026#34;, \u0026#34;delta\u0026#34;: \u0026#34;0:00:00.005060\u0026#34;, \u0026#34;end\u0026#34;: \u0026#34;2022-10-07 14:31:50.846284\u0026#34;, \u0026#34;failed\u0026#34;: false, \u0026#34;msg\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;rc\u0026#34;: 0, \u0026#34;start\u0026#34;: \u0026#34;2022-10-07 14:31:50.841224\u0026#34;, \u0026#34;stderr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stderr_lines\u0026#34;: [], \u0026#34;stdout\u0026#34;: \u0026#34; 14:31:50 up 15 min, 1 user, load average: 0,01, 0,05, 0,02\u0026#34;, \u0026#34;stdout_lines\u0026#34;: [ \u0026#34; 14:31:50 up 15 min, 1 user, load average: 0,01, 0,05, 0,02\u0026#34; ] } } PLAY RECAP ********************************************************************************************************************* 3.92.61.18 : ok=3 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 If during execution you need to create a parameter, this can be done using the [set_fact] (https://docs.ansible.com/ansible/latest/collections/ansible/builtin/set_fact_module.html) module\n- name: Setting host facts using complex arguments ansible.builtin.set_fact: important_var: important_value - name: Print important value ansible.builtin.debug: msg: \u0026#34;{{ important_var }}\u0026#34; $ ansible-playbook -i inventory main.yml PLAY [Update web servers] ****************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************* ok: [3.92.61.18] TASK [Setting host facts using complex arguments] ****************************************************************************** ok: [3.92.61.18] TASK [Print important value] *************************************************************************************************** ok: [3.92.61.18] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;important_value\u0026#34; } PLAY RECAP ********************************************************************************************************************* 3.92.61.18 : ok=3 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Condition It is good when the playbook is universal and can work on different operating systems. But what if we have specific tasks that should be performed in Ubuntu but not in RedHat. This can be done using conditions. Let\u0026rsquo;s try to create a universal playbook that will install apache http regardless of the OS version. For RedHat we need to use the yum module and for Ubuntu apt. Let\u0026rsquo;s use the condition when\n- name: Install apache on Redhat ansible.builtin.yum: name: httpd state: latest when: ansible_facts[\u0026#39;os_family\u0026#39;] == \u0026#34;RedHat\u0026#34; - name: Install apache on Debian ansible.builtin.apt: name: apache2 state: latest when: ansible_facts[\u0026#39;os_family\u0026#39;] == \u0026#34;Debian\u0026#34; $ ansible-playbook -i inventory main.yml PLAY [Update web servers] ****************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************* ok: [3.92.61.18] TASK [Install apache on Redhat] ************************************************************************************************ ok: [3.92.61.18] TASK [Install apache on Debian] ************************************************************************************************ skipping: [3.92.61.18] PLAY RECAP ********************************************************************************************************************* 3.92.61.18 : ok=2 changed=0 unreachable=0 failed=0 skipped=1 rescued=0 ignored=0 Thus, the task is executed only if the condition returns `True\u0026rsquo;, otherwise not.\nBlock Block is used to logically combine tasks in Ansible. For example, you can organize all tasks for Ubuntu and apply a condition at the block level. For example, sometimes not.\n- name: Debian block block: - name: Install apache on Ubuntu ansible.builtin.apt: name: apache2 state: latest - name: Start service apache2, if not started ansible.builtin.service: name: apache2 state: started when: ansible_facts[\u0026#39;os_family\u0026#39;] == \u0026#34;Debian\u0026#34; $ ansible-playbook -i inventory main.yml PLAY [Update web servers] ****************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************* ok: [3.92.61.18] TASK [Install apache on Ubuntu] ************************************************************************************************ skipping: [3.92.61.18] TASK [Start service apache2, if not started] *********************************************************************************** skipping: [3.92.61.18] PLAY RECAP ********************************************************************************************************************* 3.92.61.18 : ok=1 changed=0 unreachable=0 failed=0 skipped=2 rescued=0 ignored=0 You can also handle errors using rescue and always. The rescue block is executed only if an error occurs in the main block. Always is always launched, regardless of what happened in the previous block.\n- name: Error block block: - name: Print a message ansible.builtin.debug: msg: \u0026#39;I execute normally\u0026#39; - name: Force a failure ansible.builtin.command: /bin/false - name: Never print this ansible.builtin.debug: msg: \u0026#39;I never execute, due to the above task failing, :-(\u0026#39; rescue: - name: Print when errors ansible.builtin.debug: msg: \u0026#39;I caught an error\u0026#39; - name: Force a failure in middle of recovery! \u0026gt;:-) ansible.builtin.command: /bin/false - name: Never print this ansible.builtin.debug: msg: \u0026#39;I also never execute :-(\u0026#39; always: - name: Always do this ansible.builtin.debug: msg: \u0026#34;This always executes\u0026#34; $ ansible-playbook -i inventory main.yml PLAY [Update web servers] ****************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************* ok: [3.92.61.18] TASK [Print a message] ********************************************************************************************************* ok: [3.92.61.18] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;I execute normally\u0026#34; } TASK [Force a failure] ********************************************************************************************************* fatal: [3.92.61.18]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: true, \u0026#34;cmd\u0026#34;: [\u0026#34;/bin/false\u0026#34;], \u0026#34;delta\u0026#34;: \u0026#34;0:00:00.003974\u0026#34;, \u0026#34;end\u0026#34;: \u0026#34;2022-10-07 14:37:27.522485\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;non-zero return code\u0026#34;, \u0026#34;rc\u0026#34;: 1, \u0026#34;start\u0026#34;: \u0026#34;2022-10-07 14:37:27.518511\u0026#34;, \u0026#34;stderr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stderr_lines\u0026#34;: [], \u0026#34;stdout\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stdout_lines\u0026#34;: []} TASK [Print when errors] ******************************************************************************************************* ok: [3.92.61.18] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;I caught an error\u0026#34; } TASK [Force a failure in middle of recovery! \u0026gt;:-)] ***************************************************************************** fatal: [3.92.61.18]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: true, \u0026#34;cmd\u0026#34;: [\u0026#34;/bin/false\u0026#34;], \u0026#34;delta\u0026#34;: \u0026#34;0:00:00.002841\u0026#34;, \u0026#34;end\u0026#34;: \u0026#34;2022-10-07 14:37:30.324774\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;non-zero return code\u0026#34;, \u0026#34;rc\u0026#34;: 1, \u0026#34;start\u0026#34;: \u0026#34;2022-10-07 14:37:30.321933\u0026#34;, \u0026#34;stderr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stderr_lines\u0026#34;: [], \u0026#34;stdout\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stdout_lines\u0026#34;: []} TASK [Always do this] ********************************************************************************************************** ok: [3.92.61.18] =\u0026gt; { \u0026#34;msg\u0026#34;: \u0026#34;This always executes\u0026#34; } PLAY RECAP ********************************************************************************************************************* 3.92.61.18 : ok=4 changed=0 unreachable=0 failed=1 skipped=0 rescued=1 ignored=0 Loop Ansible has several types of loops. Let\u0026rsquo;s start with the simplest loop through the list. In this programming, we will check only the simplest one\nSimple list In this case, several users (testuser1 and testuser2) are created, which are specified in the loop block, and {{ item }} is specified instead of the user name, which will be replaced by the user name when executed\n- name: Add several users ansible.builtin.user: name: \u0026#34;{{ item }}\u0026#34; state: present groups: \u0026#34;wheel\u0026#34; loop: - testuser1 - testuser2 become: true $ ansible-playbook -i inventory main.yml PLAY [Update web servers] ****************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************* ok: [3.92.61.18] TASK [Add several users] ******************************************************************************************************* changed: [3.92.61.18] =\u0026gt; (item=testuser1) changed: [3.92.61.18] =\u0026gt; (item=testuser2) PLAY RECAP ********************************************************************************************************************* 3.92.61.18 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 List of hashes This is how the cycle looks like through the list of hash maps. The cycle looks the same as the previous one but with one difference in the task block the name of the key from the map is added to {{ item }} and then the value of the name field will look like \u0026quot;{{ item.name }}\u0026quot; and the group \u0026quot;{ { item.groups }}\u0026quot;\n- name: Add several users ansible.builtin.user: name: \u0026#34;{{ item.name }}\u0026#34; state: present groups: \u0026#34;{{ item.groups }}\u0026#34; loop: - { name: \u0026#39;testuser3\u0026#39;, groups: \u0026#39;wheel\u0026#39; } - { name: \u0026#39;testuser4\u0026#39;, groups: \u0026#39;root\u0026#39; } become: true $ ansible-playbook -i inventory main.yml PLAY [Update web servers] ****************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************* ok: [3.92.61.18] TASK [Add several users] ******************************************************************************************************* changed: [3.92.61.18] =\u0026gt; (item={\u0026#39;name\u0026#39;: \u0026#39;testuser3\u0026#39;, \u0026#39;groups\u0026#39;: \u0026#39;wheel\u0026#39;}) changed: [3.92.61.18] =\u0026gt; (item={\u0026#39;name\u0026#39;: \u0026#39;testuser4\u0026#39;, \u0026#39;groups\u0026#39;: \u0026#39;root\u0026#39;}) PLAY RECAP ********************************************************************************************************************* 3.92.61.18 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Register var in loop If we use a loop and want to use register to save the output of the task, then the result in this case will be saved in a list.\n- name: Register loop output as a variable ansible.builtin.shell: \u0026#34;echo {{ item }}\u0026#34;2022-10-02 loop: - \u0026#34;one\u0026#34; - \u0026#34;two\u0026#34; register: echo - name: Print variable ansible.builtin.debug: msg: \u0026#34;{{ echo }}\u0026#34; { \u0026#34;changed\u0026#34;: true, \u0026#34;msg\u0026#34;: \u0026#34;All items completed\u0026#34;, \u0026#34;results\u0026#34;: [ { \u0026#34;changed\u0026#34;: true, \u0026#34;cmd\u0026#34;: \u0026#34;echo \\\u0026#34;one\\\u0026#34; \u0026#34;, \u0026#34;delta\u0026#34;: \u0026#34;0:00:00.003110\u0026#34;, \u0026#34;end\u0026#34;: \u0026#34;2022-10-02 12:00:05.187153\u0026#34;, \u0026#34;invocation\u0026#34;: { \u0026#34;module_args\u0026#34;: \u0026#34;echo \\\u0026#34;one\\\u0026#34;\u0026#34;, \u0026#34;module_name\u0026#34;: \u0026#34;shell\u0026#34; }, \u0026#34;item\u0026#34;: \u0026#34;one\u0026#34;, \u0026#34;rc\u0026#34;: 0, \u0026#34;start\u0026#34;: \u0026#34;2022-10-02 12:00:05.184043\u0026#34;, \u0026#34;stderr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stdout\u0026#34;: \u0026#34;one\u0026#34; }, { \u0026#34;changed\u0026#34;: true, \u0026#34;cmd\u0026#34;: \u0026#34;echo \\\u0026#34;two\\\u0026#34; \u0026#34;, \u0026#34;delta\u0026#34;: \u0026#34;0:00:00.002920\u0026#34;, \u0026#34;end\u0026#34;: \u0026#34;2022-10-02 12:00:05.245502\u0026#34;, \u0026#34;invocation\u0026#34;: { \u0026#34;module_args\u0026#34;: \u0026#34;echo \\\u0026#34;two\\\u0026#34;\u0026#34;, \u0026#34;module_name\u0026#34;: \u0026#34;shell\u0026#34; }, \u0026#34;item\u0026#34;: \u0026#34;two\u0026#34;, \u0026#34;rc\u0026#34;: 0, \u0026#34;start\u0026#34;: \u0026#34;2022-10-02 12:00:05.242582\u0026#34;, \u0026#34;stderr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stdout\u0026#34;: \u0026#34;two\u0026#34; } ] } Delegate To delegate_to is used if you need to transfer the execution of tasks to another server or locally. For example, the server is configured, and when finished, it must be added to the load balancer. But the server itself may not have such rights, so the execution of this task can be transferred to a local server that has such rights.\n- name: Add server to load balancer ansible.builtin.command: echo \u0026#34;Adding to loadbalancer\u0026#34; delegate_to: 127.0.0.1 $ ansible-playbook -i inventory main.yml PLAY [Update web servers] ****************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************* ok: [3.92.61.18] TASK [Add server to load balancer] ********************************************************************************************* changed: [3.92.61.18 -\u0026gt; 127.0.0.1] PLAY RECAP ********************************************************************************************************************* 3.92.61.18 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Failed when If ansible executes a shell script, it cannot always correctly determine whether the task was completed successfully or not, in such cases, you can use the parameter failed_when, which will describe in which cases the task is considered not completed successfully\n- name: Making sure the Physical Memory more than 2gb ansible.builtin.shell: \u0026#34;cat /proc/meminfo|grep -i memtotal|awk \u0026#39;{print $2/1024/1024}\u0026#39;\u0026#34; register: memory failed_when: \u0026#34;memory.stdout|float \u0026lt; 2\u0026#34; $ ansible-playbook -i inventory main.yml PLAY [Update web servers] ****************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************* ok: [3.92.61.18] TASK [Making sure the Physical Memory more than 2gb] *************************************************************************** fatal: [3.92.61.18]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: true, \u0026#34;cmd\u0026#34;: \u0026#34;cat /proc/meminfo|grep -i memtotal|awk \u0026#39;{print $2/1024/1024}\u0026#39;\u0026#34;, \u0026#34;delta\u0026#34;: \u0026#34;0:00:00.006063\u0026#34;, \u0026#34;end\u0026#34;: \u0026#34;2022-10-07 14:46:37.761174\u0026#34;, \u0026#34;failed_when_result\u0026#34;: true, \u0026#34;msg\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;rc\u0026#34;: 0, \u0026#34;start\u0026#34;: \u0026#34;2022-10-07 14:46:37.755111\u0026#34;, \u0026#34;stderr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stderr_lines\u0026#34;: [], \u0026#34;stdout\u0026#34;: \u0026#34;0.943119\u0026#34;, \u0026#34;stdout_lines\u0026#34;: [\u0026#34;0.943119\u0026#34;]} PLAY RECAP ********************************************************************************************************************* 3.92.61.18 : ok=1 changed=0 unreachable=0 failed=1 skipped=0 rescued=0 ignored=0 Ignore Errors If we have tasks in which we expect that there may be an error, but we still want to continue the execution, then we can add the parameter ignore_errors: yes and the execution of the playbook will continue.\n- name: Do not count this as a failure ansible.builtin.command: /bin/false ignore_errors: yes $ ansible-playbook -i inventory main.yml PLAY [Update web servers] ****************************************************************************************************** TASK [Gathering Facts] ********************************************************************************************************* ok: [3.92.61.18] TASK [Do not count this as a failure] ****************************************************************************************** fatal: [3.92.61.18]: FAILED! =\u0026gt; {\u0026#34;changed\u0026#34;: true, \u0026#34;cmd\u0026#34;: [\u0026#34;/bin/false\u0026#34;], \u0026#34;delta\u0026#34;: \u0026#34;0:00:00.002920\u0026#34;, \u0026#34;end\u0026#34;: \u0026#34;2022-10-07 14:43:48.380284\u0026#34;, \u0026#34;msg\u0026#34;: \u0026#34;non-zero return code\u0026#34;, \u0026#34;rc\u0026#34;: 1, \u0026#34;start\u0026#34;: \u0026#34;2022-10-07 14:43:48.377364\u0026#34;, \u0026#34;stderr\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stderr_lines\u0026#34;: [], \u0026#34;stdout\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;stdout_lines\u0026#34;: []} ...ignoring PLAY RECAP ********************************************************************************************************************* 3.92.61.18 : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=1 Video ","permalink":"https://mpostument.com/posts/programming/ansible/ansible-playbook/","summary":"Hello!\nToday I will show what ansible-playbook is and how to create it.\nPlaybook Ansible Playbooks is an iterative, reusable, configuration management and deployment framework that is well-suited for deploying complex applications. If you need to run a task with Ansible more than once, you can do it with a playbook.\nLet\u0026rsquo;s create the simplest playbook. In the playbook, you need to specify its name, the group of servers on which it will be executed, and the tasks themselves.","title":"Ansible Playbook"},{"content":"Hello!\nToday I will show you what inventory in Ansible and how to use them.\nAnsible has two types of inventory - static and dynamic. Static is used when we know in advance the list of servers on which Ansible should run. And dynamic when we do not have this data in advance and we want to receive it dynamically.\nStatic inventory To use static inventory, it is enough to simply create a file, for example, inventory, and inside you can specify IP addresses, for example:\n192.168.1.1 This can be used immediately by specifying this file in the ansible command using -i inventory all. all means to run ansible on all servers from the inventory file.\n$ ansible -m ping -i inventory all --private-key SSH_KEY --user SSH_USER 192.168.1.1 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/bin/python\u0026#34; }, \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } Servers can also be combined into groups.\n[web] 192.168.1.1 [database] 192.168.1.1 And now you can run ansible only for some group. To do this, you need to specify the name of the group instead of all. If you need to run for all servers, you can specify all again.\n$ ansible -m ping -i inventory web --private-key SSH_KEY --user SSH_USER 192.168.1.1 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/bin/python\u0026#34; }, \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } You can also specify a set of parameters for each server or group in the inventory. For example, the value that we passed using the \u0026ndash;user key can be specified in the inventory.\n[web] 192.168.1.1 ansible_user=centos Now the \u0026ndash;user key can be omitted\n$ ansible -m ping -i inventory web --private-key SSH_KEY 192.168.1.1 | SUCCESS =\u0026gt; { \u0026#34;ansible_facts\u0026#34;: { \u0026#34;discovered_interpreter_python\u0026#34;: \u0026#34;/usr/bin/python\u0026#34; }, \u0026#34;changed\u0026#34;: false, \u0026#34;ping\u0026#34;: \u0026#34;pong\u0026#34; } Also, parameters can be set at the group level, for this, you need to add another group with a name like[GROUP_NAME:vars]\n[web] 192.168.1.1 [web:vars] ansible_user=centos The result will be the same as in the previous version, only now these parameters will be applied to this group.\nAny values that will be used in playbooks in the future can be passed as parameters.\nAlso, several groups can be added to one group.\n[web] 192.168.1.1 [db] 192.168.1.2 [webdb:children] web db And if I specify -i inventory webdb when running ansible, ansible will run for all servers from those two groups.\nGroup Vars Inventory is not the best place to store parameters. It is better to create a folder called group_vars and organize parameters for each group in it. The group can be organized as a folder or a file. In this example, I created a folder with a name that corresponds to the name of the group and inside a file that can have any name.\ntree group_vars group_vars └── web └── web In file web now i can add variables which will be used for group web\nansible_user: centos And if I run ansible, I get the same result as last time.\nDynamic Inventory Dynamic inventory is needed if we do not know the IP addresses in advance. For example, with AWS, where servers can often change and have different IP addresses. Dynamic inventory is a script that is passed to the -i parameter, and when this script is run, I will get all the necessary information about the servers. Some dynamic inventories have become part of ansible and do not need to be downloaded additionally, it is enough to simply indicate that it needs to be used.\nTo use AWS dynamic inventory, you need to install an additional python library called boto3. It can be installed using pip install boto3.\nSince AWS dynamic inventory is part of ansible, no scripts need to be downloaded. But you need to create a configuration file aws_ec2.yml. The simplest one will look like this. And specify this file with -i key\nplugin: aws_ec2 regions: - REGION boto_profile: AWS_PROFILE In this case, ansible will be launched on all servers that are in this AWS account in the specified region. You can use filters to filter only the servers you want. For example, only servers that have the type = database tag.\nplugin: aws_ec2 regions: - REGION boto_profile: AWS_PROFILE filters: tag:type: database Filters can be combined. All possible options can be found in the documentation of a specific dynamic plugin(AWS docs).\nVideo ","permalink":"https://mpostument.com/posts/programming/ansible/ansible-inventory/","summary":"Hello!\nToday I will show you what inventory in Ansible and how to use them.\nAnsible has two types of inventory - static and dynamic. Static is used when we know in advance the list of servers on which Ansible should run. And dynamic when we do not have this data in advance and we want to receive it dynamically.\nStatic inventory To use static inventory, it is enough to simply create a file, for example, inventory, and inside you can specify IP addresses, for example:","title":"Ansible Inventory"},{"content":"Hello!\nWith this programming, I will start a series about AWS, in which I will show how to use various AWS services, and automate them using terraform and the AWS SDK. And let\u0026rsquo;s start with the EC2 service.\nEC2 is the core compute component of AWS. In practice, EC2 makes life easier for developers by providing scalable, secure computing power on AWS. It makes scaling up or down much easier, can be integrated with several other services, and comes with a plan where you only pay for what you use.\nUI Overview This is what the EC2 UI looks like. It has several categories:\nInstance Images EBS Network \u0026amp; Security Load Balancing Auto Scaling Instance In the instance section, you can create instances, view existing ones, and create templates of instance configurations for further reuse. To create an instance, you need to click Launch Instance.\nHere we need to specify the attributes of our future instance. AMI is mandatory. AMI is an image of the operating system that will be used for EC2. It can be a clean OS without additional settings, or an image with certain settings, for example, with LAMP Stack or WordPress.\nThe next mandatory parameters are Instance Type and Key Pair. The type of instance determines how much RAM and processor cores it will have. Depending on the type of instance, the price will be determined.\nDifferent types of instances have different purposes:\nGeneral purpose - balance computing, network resources, and memory. They can be used for many different workloads. These instances are ideal for applications that use such resources in equal amounts, such as web instances and code repositories. Types T and M Computationally optimized - ideal for applications with computational speed limitations that can use high-performance processors. instances in this family are well-suited for batch workloads, multimedia transcoding, high-performance web instances, high-performance computing (HPC), scientific modeling, dedicated game and ad instances, machine learning inference, and other applications that require large computing resources. Type C Optimized for memory - provide high performance of workloads related to the processing of large data sets in memory. Type R Accelerated computing - use hardware accelerators (coprocessors) for operations such as floating-point calculations, graphics processing, and mapping of data samples, and as a result, perform them more efficiently than is possible when the software runs only on the CPU. Types P, G, F Storage-Optimized - Storage-optimized, suitable for workloads that require sequential access for multiple read and write operations to large data packets on local storage. They are optimized to deliver tens of thousands of random I/O operations per second (IOPS) to applications with low latency. Types I, D Key Type indicates which ssh key will be used to access the instance, or not to use a key at all. In this case, there will be no ssh access to the instance.\nThis section specifies which subnet will be used for the instance and which security group to use. Security Group is a firewall that controls which ports will be opened on the instance\nIn this section, you configure the size and type of disk for the instance, and you can mount the additional file system like EFS or FSx.\nTags are also an important category, you should always add at least a few so that in the future you can find out whose instance this is, what environment it belongs to, its purpose, etc. Tags are key-value pairs, for example, Name = Webinstance.\nThe last section is Advanced details. Where you can specify many other parameters, the most important of which are the IAM instance profile (determines access to which AWS resources will be from this instance), and User Data (the script that is executed when the instance is created).\nAfter all, parameters are specified, click Launch Instance and the instance will be created.\nInstance Type In this section, you can view existing types of instances and their prices.\nLaunch Template The launch template has almost all the same fields as when creating an instance. But creating a Launch Template does not create an instance, but only creates a template that can be used to create instances with the same configuration. In addition to the standard EC2 parameters, you need to specify the name of the template. Also, a template can be created from an existing instance\nAnd as soon as the template is created, you can create an instance from it, and you can also use it in an auto-scaling group.\nSpot Requests Spot instances are essentially the sale of available resources at a great discount. At the same time, the instance can be turned off and taken back at any time. When ordering, you make a bid - indicate the maximum price you are willing to pay for use. It is the balance of rates and free resources that forms the final cost, which at the same time differs in different regions.\nSavings Plan Allows you to get a discount on ec2/lambda/fargate for committing to use a certain amount of resources for 1 or 3 years.\nReserved instances It is similar to the Savings Plan with the only difference that a reserved instance is purchased for a specific type of instance for 1 or 3 years\nImages Images have an AMI Catalog where you can find all publicly available ami. The AMIs section contains all of our created AMIs. AMI can be created by right-clicking on the instance, in the menu that appears, and selecting Image and Templates -\u0026gt; Create Image.\nOnce all the information is entered, click Create Image and you can found in the AMIs section\nNow you can create new instances from this AMI\nElastic Block Store This section stores everything related to disks.\nVolumes Some disks are currently used by the instance or that are temporarily not used by anything. From here disk can be deleted, detached, resized, or mounted.\nSnapshots This section contains snapshots of the disk at a certain point in time. With their help, you can create a new volume if you need to restore data that was lost on the disk itself. Note that when we created the AMI, a disk image was automatically created. And it won\u0026rsquo;t be able to be removed as long as the AMI exists. First, you need to delete ami and only then you can delete the picture\nLifecycle Manager Allows you to create rules according to which disk snapshot/ami will be automatically created and deleted.\nOn the first page of the configuration, you need to specify the name of the policy and also select the IAM role that will be used to create/delete AMIs and snapshots. To select an instance that will be used in the policy you need to specify the tag of this instance. For example, if I have a tag on my servers as type = database, then I can specify that tag, and the policy will be applied only to this instance.\nOn the next page, you need to define the schedule on which snapshots/AMIs will be created and how many snapshots/AMIs to keep. For example, run every day at 12-00 UTC and save the last 20.\nNetwork \u0026amp; Security In this section, there are security groups that control the configuration of access to ports on the instance. SSH keys and static IP are also located here.\nSecurity Groups Security Groups allow you to open specific ports to access the instance. For example, you can open port 443 for HTTPS access, or port 22 for ssh access.\nThe are two types of rules: Inbound and Outbound, which control incoming and outgoing traffic. In this example, there will be one rule in the incoming traffic that allows access to port 22 from a certain IP address. All outgoing traffic is available. Without any restrictions.\nThe security group can be selected when creating the instance, or right-click on the existing instance and select Security \u0026gt; Change security groups. Select/delete the desired group and press Save.\nElastic IPs Allows you to reserve a static public IP address. If an instance in AWS is removed or stopped, its public IP address changes. If we need it to always be the same, it can be done in this section. Just click on Allocate Elastic IP Address\nNow you need to tell the instance to use this address. To do this, you need to select the IP address and click Actions -\u0026gt; Associate Elastic IP address in the menu. Select the instance and its private IP and click Associate.\nIf the IP is no longer needed, it can be deleted by clicking Actions -\u0026gt; Release Elastic IP address and confirming the deletion.\nPlacement groups When you start a new instance, AWS tries to place it so that all instances are distributed on the same hardware to minimize correlated failures\nKey Pairs Public parts of SSH keys are stored in Key Pairs. They can be selected when creating an instance. A new public key can also be uploaded here\nNetwork interfaces Here are the network interfaces of all AWS services.\nLoad Balancer та Autoscaling будуть розглянуті в наступному пості How to create ec2 with terraform Using terraform, it is very easy to create an instance. For this, terraform has a resource called aws_instance.\nThe simplest code example will look like this\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; subnet_id = \u0026#34;subnet-db73f0ac\u0026#34; vpc_security_group_ids = [\u0026#34;sg-b04b8cd4\u0026#34;] tags = { Env = \u0026#34;Dev222\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } } In this example, the terraform execute a resource named aws_instance, to which we pass a set of parameters, the same ones we passed when creating the instance through the UI.\nGolang SDK Another option for creating an instance is the AWS SDK, more specifically the Golang AWS SDK. To use it golang must be installed together with the AWS SDK\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/config\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/ec2\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/ec2/types\u0026#34; ) func main() { // Load the Shared AWS Configuration (~/.aws/config) cfg, err := config.LoadDefaultConfig(context.TODO(), config.WithSharedConfigProfile(\u0026#34;default\u0026#34;)) if err != nil { log.Fatal(err) } client := ec2.NewFromConfig(cfg) input := \u0026amp;ec2.RunInstancesInput{ ImageId: aws.String(\u0026#34;ami-0cff7528ff583bf9a\u0026#34;), InstanceType: types.InstanceTypeT2Micro, SubnetId: aws.String(\u0026#34;subnet-db73f0ac\u0026#34;), SecurityGroupIds: aws.ToStringSlice([]*string{ aws.String(\u0026#34;sg-b04b8cd4\u0026#34;), }), MaxCount: aws.Int32(1), MinCount: aws.Int32(1), } _, err = client.RunInstances(context.Background(), input) if err != nil { log.Fatal(err) } } Several events occur in the code. First, we create an AWS client in which I specify that I want to use a profile named default. The profile configuration itself is in the ~/.aws/credentials folder. AWS Access Key and Secret Key should be stored there. You can generate Access and Secret key for your user in the AWS UI.\nIn RunInstancesInput we specify the parameters with which we will create the instance. Here everything is the same as when creating with UI as when using terraform. And we create an instance with these parameters. The advantage of terraform over this method is that everything created by terraform can be removed using terraform destroy. In this case, you would need to create your logic for how to delete or delete resources from the UI.\nVideo ","permalink":"https://mpostument.com/posts/programming/aws/aws-101-ec2-part-1/","summary":"Hello!\nWith this programming, I will start a series about AWS, in which I will show how to use various AWS services, and automate them using terraform and the AWS SDK. And let\u0026rsquo;s start with the EC2 service.\nEC2 is the core compute component of AWS. In practice, EC2 makes life easier for developers by providing scalable, secure computing power on AWS. It makes scaling up or down much easier, can be integrated with several other services, and comes with a plan where you only pay for what you use.","title":"AWS 101: EC2 Part 1"},{"content":"Hello!\nThis is the first post in the series about Ansible, in which I will show how you can manage versions of Ansible and Python using pyenv.\nPyenv pyenv allows you to install different versions of python and also create a virtual env in which ansible will be installed.\nTo install pyenv, you need to execute the command\ngit clone https://github.com/pyenv/pyenv.git ~/.pyenv and add pyenv to PATH\necho \u0026#39;export PYENV_ROOT=\u0026#34;$HOME/.pyenv\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;command -v pyenv \u0026gt;/dev/null || export PATH=\u0026#34;$PYENV_ROOT/bin:$PATH\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;eval \u0026#34;$(pyenv init -)\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.bashrc After the shell is restarted, the pyenv command will be available. But to create a virtualenv, you need an additional plugin that can be installed using the command:\ngit clone https://github.com/pyenv/pyenv-virtualenv.git $(pyenv root)/plugins/pyenv-virtualenv You also need to install certain packages for python to compile. For ubuntu it will be:\nsudo apt-get update; sudo apt-get install make build-essential libssl-dev zlib1g-dev \\ libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm \\ libncursesw5-dev xz-utils tk-dev libxml2-dev libxmlsec1-dev libffi-dev liblzma-dev Now that everything is ready, you can install the required version of python and ansible. Before ansible 2.5 there was support only for python 2, from a version higher than 2.5 you can use python 3. I will install python 3 and ansible version 2.10.\nInstall python To view available versions of python, you can execute the pyenv install --list command\n$ pyenv install --list Available versions: 2.1.3 2.2.3 2.3.7 2.4.0 2.4.1 2.4.2 2.4.3 2.4.4 2.4.5 2.4.6 2.5.0 2.5.1 2.5.2 2.5.3 2.5.4 2.5.5 2.5.6 ... After the desired python version is selected, it can be installed using pyenv install PYTHON_VERSION\n$ pyenv install 3.10.7 Downloading Python-3.10.7.tar.xz... -\u0026gt; https://www.python.org/ftp/python/3.10.7/Python-3.10.7.tar.xz Installing Python-3.10.7... Installed Python-3.10.7 to /home/maksym/.pyenv/versions/3.10.7 Ansible can be installed now, but if several versions are required, it will be difficult to do so. Since the versions will overwrite each other. It is better to use virtualenv. It can be created using pyenv virtualenv PYTHON_VERSION VIRTUALENV_NAME\npyenv virtualenv 3.10.7 ansible-2.10 When a virtualenv is created it needs to be activated, and this is how you can have multiple versions of ansible installed, with a separate virtualenv for each ansible version.\nYou can activate virtualenv using the pyenv activate VIRTUALENV_NAME command\npyenv activate ansible-2.10 Now you are ready to install ansible. This can be done using the pip install PACKAGE_NAME\n$ pip install ansible==2.10.7 Collecting ansible==2.10.7 Downloading ansible-2.10.7.tar.gz (29.9 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 29.9/29.9 MB 8.3 MB/s eta 0:00:00 Preparing metadata (setup.py) ... done Collecting ansible-base\u0026lt;2.11,\u0026gt;=2.10.5 Downloading ansible-base-2.10.17.tar.gz (6.1 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.1/6.1 MB 9.5 MB/s eta 0:00:00 Preparing metadata (setup.py) ... done Collecting jinja2 Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB) Collecting PyYAML Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 682.2/682.2 kB 8.6 MB/s eta 0:00:00 Collecting cryptography Downloading cryptography-38.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (4.2 MB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 9.9 MB/s eta 0:00:00 Collecting packaging Downloading packaging-21.3-py3-none-any.whl (40 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.8/40.8 kB 3.0 MB/s eta 0:00:00 Collecting cffi\u0026gt;=1.12 Downloading cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 441.8/441.8 kB 8.9 MB/s eta 0:00:00 Collecting MarkupSafe\u0026gt;=2.0 Downloading MarkupSafe-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB) Collecting pyparsing!=3.0.5,\u0026gt;=2.0.2 Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 6.2 MB/s eta 0:00:00 Collecting pycparser Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB) Using legacy \u0026#39;setup.py install\u0026#39; for ansible, since package \u0026#39;wheel\u0026#39; is not installed. Using legacy \u0026#39;setup.py install\u0026#39; for ansible-base, since package \u0026#39;wheel\u0026#39; is not installed. Installing collected packages: PyYAML, pyparsing, pycparser, MarkupSafe, packaging, jinja2, cffi, cryptography, ansible-base, ansible Running setup.py install for ansible-base ... done Running setup.py install for ansible ... done Successfully installed MarkupSafe-2.1.1 PyYAML-6.0 ansible-2.10.7 ansible-base-2.10.17 cffi-1.15.1 cryptography-38.0.1 jinja2-3.1.2 packaging-21.3 pycparser-2.21 pyparsing-3.0.9 Ansible is now ready to use\n$ ansible --version ansible 2.10.17 config file = None configured module search path = [\u0026#39;/home/maksym/.ansible/plugins/modules\u0026#39;, \u0026#39;/usr/share/ansible/plugins/modules\u0026#39;] ansible python module location = /home/maksym/.pyenv/versions/3.10.7/envs/ansible-2.10/lib/python3.10/site-packages/ansible executable location = /home/maksym/.pyenv/versions/ansible-2.10/bin/ansible python version = 3.10.7 (main, Sep 19 2022, 11:11:49) [GCC 9.4.0] If you need to install another version of ansible, you can deactivate virtualenv, create a new one and install ansible\n$ pyenv deactivate $ pyenv virtualenv 3.10.7 ansible-2.9 $ pip install ansible==2.9.27 Video ","permalink":"https://mpostument.com/posts/programming/ansible/manage-ansible-versions-with-pyenv/","summary":"Hello!\nThis is the first post in the series about Ansible, in which I will show how you can manage versions of Ansible and Python using pyenv.\nPyenv pyenv allows you to install different versions of python and also create a virtual env in which ansible will be installed.\nTo install pyenv, you need to execute the command\ngit clone https://github.com/pyenv/pyenv.git ~/.pyenv and add pyenv to PATH\necho \u0026#39;export PYENV_ROOT=\u0026#34;$HOME/.pyenv\u0026#34;\u0026#39; \u0026gt;\u0026gt; ~/.","title":"Manage Ansible Versions With pyenv"},{"content":"Hello!\nToday we will see how you can import existing resources into terraform and how to organize the code into a module for future reuse.\nImport There is a situation when the resources have already been created and you need to start managing them using terraform. Or the resource was removed from the terraform state and you need to add it there again. This is very easy to do with terraform import.\nThere is a code that creates an ec2 server, but the server already exists\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = data.aws_ami.amzn.id instance_type = var.instance_type subnet_id = \u0026#34;subnet-db73f0ac\u0026#34; vpc_security_group_ids = [\u0026#34;sg-b04b8cd4\u0026#34;] tags = { Env = \u0026#34;Dev\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } } If you run terraform, the server will be created one more time\n$ terraform plan data.aws_ami.amzn: Reading... data.aws_ami.amzn: Read complete after 0s [id=ami-05fa00d4c63e32376] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_instance.foo will be created + resource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { + ami = \u0026#34;ami-05fa00d4c63e32376\u0026#34; + arn = (known after apply) + associate_public_ip_address = (known after apply) + availability_zone = (known after apply) + cpu_core_count = (known after apply) + cpu_threads_per_core = (known after apply) + disable_api_stop = (known after apply) + disable_api_termination = (known after apply) + ebs_optimized = (known after apply) + get_password_data = false + host_id = (known after apply) + id = (known after apply) + instance_initiated_shutdown_behavior = (known after apply) + instance_state = (known after apply) + instance_type = \u0026#34;c5.large\u0026#34; + ipv6_address_count = (known after apply) + ipv6_addresses = (known after apply) + key_name = (known after apply) + monitoring = (known after apply) + outpost_arn = (known after apply) + password_data = (known after apply) + placement_group = (known after apply) + placement_partition_number = (known after apply) + primary_network_interface_id = (known after apply) + private_dns = (known after apply) + private_ip = (known after apply) + public_dns = (known after apply) + public_ip = (known after apply) + secondary_private_ips = (known after apply) + security_groups = (known after apply) + source_dest_check = true + subnet_id = \u0026#34;subnet-db73f0ac\u0026#34; + tags = { + \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } + tags_all = { + \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } + tenancy = (known after apply) + user_data = (known after apply) + user_data_base64 = (known after apply) + user_data_replace_on_change = false + volume_tags = { + \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } + vpc_security_group_ids = [ + \u0026#34;sg-b04b8cd4\u0026#34;, ] + capacity_reservation_specification { + capacity_reservation_preference = (known after apply) + capacity_reservation_target { + capacity_reservation_id = (known after apply) + capacity_reservation_resource_group_arn = (known after apply) } } + ebs_block_device { + delete_on_termination = (known after apply) + device_name = (known after apply) + encrypted = (known after apply) + iops = (known after apply) + kms_key_id = (known after apply) + snapshot_id = (known after apply) + tags = (known after apply) + throughput = (known after apply) + volume_id = (known after apply) + volume_size = (known after apply) + volume_type = (known after apply) } + enclave_options { + enabled = (known after apply) } + ephemeral_block_device { + device_name = (known after apply) + no_device = (known after apply) + virtual_name = (known after apply) } + maintenance_options { + auto_recovery = (known after apply) } + metadata_options { + http_endpoint = (known after apply) + http_put_response_hop_limit = (known after apply) + http_tokens = (known after apply) + instance_metadata_tags = (known after apply) } + network_interface { + delete_on_termination = (known after apply) + device_index = (known after apply) + network_card_index = (known after apply) + network_interface_id = (known after apply) } + private_dns_name_options { + enable_resource_name_dns_a_record = (known after apply) + enable_resource_name_dns_aaaa_record = (known after apply) + hostname_type = (known after apply) } + root_block_device { + delete_on_termination = (known after apply) + device_name = (known after apply) + encrypted = (known after apply) + iops = (known after apply) + kms_key_id = (known after apply) + tags = (known after apply) + throughput = (known after apply) + volume_id = (known after apply) + volume_size = (known after apply) + volume_type = (known after apply) } } Plan: 1 to add, 0 to change, 0 to destroy. To import the existing server into terraform, you need to run terraform import tf_resource.resource_name existing_resource_id. Since I want to import aws_instance named foo with id i-0ea46560d61c09724, the command will look like terraform import aws_instance.foo i-0ea46560d61c09724.\n$ terraform import aws_instance.foo i-0ea46560d61c09724 aws_instance.foo: Importing from ID \u0026#34;i-0ea46560d61c09724\u0026#34;... aws_instance.foo: Import prepared! Prepared aws_instance for import aws_instance.foo: Refreshing state... [id=i-0ea46560d61c09724] Import successful! The resources that were imported are shown above. These resources are now in your Terraform state and will henceforth be managed by Terraform. If you run terraform plan again, the result will be different.\n$ terraform plan data.aws_ami.amzn: Reading... data.aws_ami.amzn: Read complete after 0s [id=ami-05fa00d4c63e32376] aws_instance.foo: Refreshing state... [id=i-0ea46560d61c09724] No changes. Your infrastructure matches the configuration. Terraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed. Modules The module allows you to combine several terraform resources into one abstraction for convenient reuse. I will take the code from one of the previous posts that creates a server and security group. And let\u0026rsquo;s assume that you need to create 5 more similar servers and security groups. Of course, you can copy this code 5 times and change the attributes, or you can parameterize it and create a module.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = data.aws_ami.amzn.id instance_type = var.instance_type subnet_id = \u0026#34;subnet-db73f0ac\u0026#34; vpc_security_group_ids = [aws_security_group.test_sg.id] tags = { Env = \u0026#34;Dev\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;test_sg\u0026#34; { name = \u0026#34;test-sg\u0026#34; description = \u0026#34;allow traffic to web app\u0026#34; vpc_id = \u0026#34;vcp-123\u0026#34; dynamic \u0026#34;ingress\u0026#34; { for_each = [8080, 443, 22, 80, 3000, 8082] content { from_port = ingress.value to_port = ingress.value protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [ \u0026#34;0.0.0.0/0\u0026#34; ] } } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } Let\u0026rsquo;s add parameters. For security_group, I will create two parameters vpc and another with a list of ports that should be opened. There will be four parameters for the server: ami, subnet, instance_type, and security group. And one general parameter will be called prefix. It is needed to create different names for resources, or you can simply make a name parameter that will set the name for everything\nvariable \u0026#34;instance_type\u0026#34; { default = \u0026#34;t2.micro\u0026#34; description = \u0026#34;Instance type for test instance\u0026#34; type = string } variable \u0026#34;subnet_id\u0026#34; { description = \u0026#34;Subnet id where server will be created\u0026#34; type = string } variable \u0026#34;security_groups\u0026#34; { description = \u0026#34;List of sg for instance\u0026#34; type = list(string) } variable \u0026#34;ports_list\u0026#34; { description = \u0026#34;List of port which should be open\u0026#34; type = list(number) } variable \u0026#34;ami_id\u0026#34; { description = \u0026#34;Id of ami which should be used for instance\u0026#34; type = string } variable \u0026#34;vpc_id\u0026#34; { description = \u0026#34;Id of vpc where sg will be created\u0026#34; type = string } variable \u0026#34;prefix\u0026#34; { description = \u0026#34;Prefix to be added to all created resources\u0026#34; type = string } And in the code itself, we will specify to use of these parameters. Inside of vpc_security_group_ids i am using concat method to add security group id from new security group to list with security groups var.security_groups.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = var.ami_id instance_type = var.instance_type subnet_id = var.subnet_id vpc_security_group_ids = concat(var.security_groups, [aws_security_group.test_sg.id]) tags = { Name = \u0026#34;${var.prefix}-common-instance\u0026#34; } } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;test_sg\u0026#34; { name = \u0026#34;${var.prefix}-common-sg\u0026#34; description = \u0026#34;allow traffic to ${var.prefix}-common-instance\u0026#34; vpc_id = var.vpc_id dynamic \u0026#34;ingress\u0026#34; { for_each = var.ports_list content { from_port = ingress.value to_port = ingress.value protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [ \u0026#34;0.0.0.0/0\u0026#34; ] } } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } Now all that\u0026rsquo;s left is to move it to a separate folder and use it as a module. I will create a modules folder in which I will create another common_server into which I will move the created code.\n$ tree modules modules └── common_server ├── main.tf └── variables.tf 1 directory, 2 files Now I can create a new terraform file(main.tf) and specify to use the module. Where a source is module location in a file system.\nmodule \u0026#34;common_server\u0026#34; { source = \u0026#34;./modules/common_server\u0026#34; ami_id = data.aws_ami.amzn.id security_groups = [\u0026#34;sg-b04b8cd4\u0026#34;] vpc_id = \u0026#34;vpc-e5543380\u0026#34; subnet_id = \u0026#34;subnet-5f70bf74\u0026#34; ports_list = [80, 443] prefix = \u0026#34;maksym\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; } If I run terraform right away I get an error\n$ terraform plan ╷ │ Error: Module not installed │ │ on main.tf line 1: │ 1: module \u0026#34;common_server\u0026#34; { │ │ This module is not yet installed. Run \u0026#34;terraform init\u0026#34; to install all modules required by this configuration. First, you need to run terraform init to initialize the module.\n$ terraform init Initializing modules... - common_server in modules/common_server Initializing the backend... Initializing provider plugins... - Reusing previous version of hashicorp/aws from the dependency lock file - Using previously-installed hashicorp/aws v4.23.0 Terraform has made some changes to the provider dependency selections recorded in the .terraform.lock.hcl file. Review those changes and commit them to your version control system if they represent changes you intended to make. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \u0026#34;terraform plan\u0026#34; to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. After that, you can run terraform plan.\nterraform plan data.aws_ami.amzn: Reading... data.aws_ami.amzn: Read complete after 1s [id=ami-05fa00d4c63e32376] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # module.common_server.aws_instance.foo will be created + resource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { + ami = \u0026#34;ami-05fa00d4c63e32376\u0026#34; + arn = (known after apply) + associate_public_ip_address = (known after apply) + availability_zone = (known after apply) + cpu_core_count = (known after apply) + cpu_threads_per_core = (known after apply) + disable_api_stop = (known after apply) + disable_api_termination = (known after apply) + ebs_optimized = (known after apply) + get_password_data = false + host_id = (known after apply) + id = (known after apply) + instance_initiated_shutdown_behavior = (known after apply) + instance_state = (known after apply) + instance_type = \u0026#34;t2.micro\u0026#34; + ipv6_address_count = (known after apply) + ipv6_addresses = (known after apply) + key_name = (known after apply) + monitoring = (known after apply) + outpost_arn = (known after apply) + password_data = (known after apply) + placement_group = (known after apply) + placement_partition_number = (known after apply) + primary_network_interface_id = (known after apply) + private_dns = (known after apply) + private_ip = (known after apply) + public_dns = (known after apply) + public_ip = (known after apply) + secondary_private_ips = (known after apply) + security_groups = (known after apply) + source_dest_check = true + subnet_id = \u0026#34;subnet-5f70bf74\u0026#34; + tags = { + \u0026#34;Name\u0026#34; = \u0026#34;maksym-common-instance\u0026#34; } + tags_all = { + \u0026#34;Name\u0026#34; = \u0026#34;maksym-common-instance\u0026#34; } + tenancy = (known after apply) + user_data = (known after apply) + user_data_base64 = (known after apply) + user_data_replace_on_change = false + vpc_security_group_ids = (known after apply) + capacity_reservation_specification { + capacity_reservation_preference = (known after apply) + capacity_reservation_target { + capacity_reservation_id = (known after apply) + capacity_reservation_resource_group_arn = (known after apply) } } + ebs_block_device { + delete_on_termination = (known after apply) + device_name = (known after apply) + encrypted = (known after apply) + iops = (known after apply) + kms_key_id = (known after apply) + snapshot_id = (known after apply) + tags = (known after apply) + throughput = (known after apply) + volume_id = (known after apply) + volume_size = (known after apply) + volume_type = (known after apply) } + enclave_options { + enabled = (known after apply) } + ephemeral_block_device { + device_name = (known after apply) + no_device = (known after apply) + virtual_name = (known after apply) } + maintenance_options { + auto_recovery = (known after apply) } + metadata_options { + http_endpoint = (known after apply) + http_put_response_hop_limit = (known after apply) + http_tokens = (known after apply) + instance_metadata_tags = (known after apply) } + network_interface { + delete_on_termination = (known after apply) + device_index = (known after apply) + network_card_index = (known after apply) + network_interface_id = (known after apply) } + private_dns_name_options { + enable_resource_name_dns_a_record = (known after apply) + enable_resource_name_dns_aaaa_record = (known after apply) + hostname_type = (known after apply) } + root_block_device { + delete_on_termination = (known after apply) + device_name = (known after apply) + encrypted = (known after apply) + iops = (known after apply) + kms_key_id = (known after apply) + tags = (known after apply) + throughput = (known after apply) + volume_id = (known after apply) + volume_size = (known after apply) + volume_type = (known after apply) } } # module.common_server.aws_security_group.test_sg will be created + resource \u0026#34;aws_security_group\u0026#34; \u0026#34;test_sg\u0026#34; { + arn = (known after apply) + description = \u0026#34;allow traffic to maksym-common-instance\u0026#34; + egress = [ + { + cidr_blocks = [ + \u0026#34;0.0.0.0/0\u0026#34;, ] + description = \u0026#34;\u0026#34; + from_port = 0 + ipv6_cidr_blocks = [] + prefix_list_ids = [] + protocol = \u0026#34;-1\u0026#34; + security_groups = [] + self = false + to_port = 0 }, ] + id = (known after apply) + ingress = [ + { + cidr_blocks = [ + \u0026#34;0.0.0.0/0\u0026#34;, ] + description = \u0026#34;\u0026#34; + from_port = 443 + ipv6_cidr_blocks = [] + prefix_list_ids = [] + protocol = \u0026#34;tcp\u0026#34; + security_groups = [] + self = false + to_port = 443 }, + { + cidr_blocks = [ + \u0026#34;0.0.0.0/0\u0026#34;, ] + description = \u0026#34;\u0026#34; + from_port = 80 + ipv6_cidr_blocks = [] + prefix_list_ids = [] + protocol = \u0026#34;tcp\u0026#34; + security_groups = [] + self = false + to_port = 80 }, ] + name = \u0026#34;maksym-common-sg\u0026#34; + name_prefix = (known after apply) + owner_id = (known after apply) + revoke_rules_on_delete = false + tags_all = (known after apply) + vpc_id = \u0026#34;vpc-e5543380\u0026#34; } Plan: 2 to add, 0 to change, 0 to destroy. Now, to create several versions of servers, you just need to copy the module in main.tf and change the parameters.\nmodule \u0026#34;common_server\u0026#34; { source = \u0026#34;./modules/common_server\u0026#34; ami_id = data.aws_ami.amzn.id security_groups = [\u0026#34;sg-b04b8cd4\u0026#34;] vpc_id = \u0026#34;vpc-e5543380\u0026#34; subnet_id = \u0026#34;subnet-5f70bf74\u0026#34; ports_list = [80, 443] prefix = \u0026#34;maksym\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; } module \u0026#34;appserver\u0026#34; { source = \u0026#34;./modules/common_server\u0026#34; ami_id = data.aws_ami.amzn.id security_groups = [\u0026#34;sg-b04a9dc5\u0026#34;] vpc_id = \u0026#34;vpc-e5543380\u0026#34; subnet_id = \u0026#34;subnet-5f70bf74\u0026#34; ports_list = [80, 443, 9090, 8888] prefix = \u0026#34;app\u0026#34; instance_type = \u0026#34;m5.large\u0026#34; } The module does not necessarily have to be located on the file system to be used, as a source you can specify a link to source control such as GitHub or to terraform registry, and the module will be automatically downloaded.\nModules import Now, if you try to import the resource as we did at the beginning, it will not work because now the resources have different terraform names. For example, aws_instance before the module was displayed as aws_instance.foo, now the new name will be module.common_server.aws_instance.foo. Accordingly, to import this same server, the command will look like terraform import module.common_server.aws_instance.foo i-0ea46560d61c09724.\nVideo ","permalink":"https://mpostument.com/posts/programming/terraform/terraform-modules-and-import/","summary":"Hello!\nToday we will see how you can import existing resources into terraform and how to organize the code into a module for future reuse.\nImport There is a situation when the resources have already been created and you need to start managing them using terraform. Or the resource was removed from the terraform state and you need to add it there again. This is very easy to do with terraform import.","title":"Terraform Modules and Import"},{"content":"Hello!\nIn this programming, I will talk about conditions and lookups.\nConditions Conditions in terraform are used to select one value depending on another. For example, if it is a dev environment, then use the t3.micro server type, and if it is a production environment, then use m5.large. This is easy to do with a condition. Conditions are written like condition ? true_val : false_val. For server type conditions will look like var.env == \u0026quot;prod\u0026quot; ? \u0026quot;m5.large\u0026quot; : \u0026quot;t3.micro\u0026quot;. Here I check whether the parameter env has the value prod, and if so, I use m5.large, in all other cases it is t3.micro. The complete ec2 creation code will look like this.\nvariable \u0026#34;env\u0026#34; { default = \u0026#34;dev\u0026#34; description = \u0026#34;Environment name\u0026#34; type = string } resource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = data.aws_ami.amzn.id instance_type = var.env == \u0026#34;prod\u0026#34; ? \u0026#34;m5.large\u0026#34; : \u0026#34;t3.micro\u0026#34; subnet_id = \u0026#34;subnet-5f70bf74\u0026#34; vpc_security_group_ids = [\u0026#34;sg-b04b8cd4\u0026#34;] tags = { Env = \u0026#34;Dev\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } } First, I start terraform with the default value and the server type will be t3.micro\n$ terraform plan data.aws_ami.amzn: Reading... data.aws_ami.amzn: Read complete after 1s [id=ami-05fa00d4c63e32376] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_instance.foo will be created + resource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { + ami = \u0026#34;ami-05fa00d4c63e32376\u0026#34; + arn = (known after apply) + associate_public_ip_address = (known after apply) + availability_zone = (known after apply) + cpu_core_count = (known after apply) + cpu_threads_per_core = (known after apply) + disable_api_stop = (known after apply) + disable_api_termination = (known after apply) + ebs_optimized = (known after apply) + get_password_data = false + host_id = (known after apply) + id = (known after apply) + instance_initiated_shutdown_behavior = (known after apply) + instance_state = (known after apply) + instance_type = \u0026#34;t3.micro\u0026#34; Now I run it with value env set to prod\n$ terraform plan -var env=prod data.aws_ami.amzn: Reading... data.aws_ami.amzn: Read complete after 1s [id=ami-05fa00d4c63e32376] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_instance.foo will be created + resource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { + ami = \u0026#34;ami-05fa00d4c63e32376\u0026#34; + arn = (known after apply) + associate_public_ip_address = (known after apply) + availability_zone = (known after apply) + cpu_core_count = (known after apply) + cpu_threads_per_core = (known after apply) + disable_api_stop = (known after apply) + disable_api_termination = (known after apply) + ebs_optimized = (known after apply) + get_password_data = false + host_id = (known after apply) + id = (known after apply) + instance_initiated_shutdown_behavior = (known after apply) + instance_state = (known after apply) + instance_type = \u0026#34;m5.large\u0026#34; Also, with the help of conditions, I can specify whether to create a resource or not. In the previous example for the dev environment, I used a smaller server size, but in some cases it may be necessary to not create resource at all. To do this, you need to add the count attribute and specify in it - if prod, then create one server, and if not prod, then 0\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { count = var.env == \u0026#34;prod\u0026#34; ? 1 : 0 ami = data.aws_ami.amzn.id instance_type = \u0026#34;m5.large\u0026#34; subnet_id = \u0026#34;subnet-5f70bf74\u0026#34; vpc_security_group_ids = [\u0026#34;sg-b04b8cd4\u0026#34;] tags = { Env = \u0026#34;Dev\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } } $ terraform plan data.aws_ami.amzn: Reading... data.aws_ami.amzn: Read complete after 1s [id=ami-05fa00d4c63e32376] No changes. Your infrastructure matches the configuration. And if I run it with the value prod, the server will be created\n$ terraform plan -var env=prod data.aws_ami.amzn: Reading... data.aws_ami.amzn: Read complete after 1s [id=ami-05fa00d4c63e32376] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_instance.foo[0] will be created + resource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { + ami = \u0026#34;ami-05fa00d4c63e32376\u0026#34; The only thing to pay attention to when the count attribute is used that the resource will no longer be able to be addressed as before, for example aws.instance.foo.private_ip. There will be such an error\nterraform plan -var env=prod ╷ │ Error: Missing resource instance key │ │ on outputs.tf line 2, in output \u0026#34;instance_ip_addr\u0026#34;: │ 2: value = aws_instance.foo.private_ip │ │ Because aws_instance.foo has \u0026#34;count\u0026#34; set, its attributes must be accessed on specific instances. │ │ For example, to correlate with indices of a referring resource, use: │ aws_instance.foo[count.index] Now, in order to reference, you need to specify the index. Since I am creating only one server, my index will be 0, and the request will look like aws_instance.foo[0].private_ip. If more than one server is created, it will be possible to refer to them by their indexes 1,2,3 and so on.\nLookup With the conditions, I could specify that if prod, then one type of server, and if not, then another. And what to do if we have several environments such as dev, test, staging, and prod and each of them requires different types of servers. Lookup will help us with this. The Lookup function allows you to get the value from the map depending on the value of the key.\nTo begin with, you need to create a mapping server type to the environment.\nvariable \u0026#34;env\u0026#34; { default = \u0026#34;dev\u0026#34; description = \u0026#34;Environment name\u0026#34; type = string } variable \u0026#34;instance_types\u0026#34; { type = map(string) default = { \u0026#34;prod\u0026#34; = \u0026#34;m5.xlarge\u0026#34;, \u0026#34;staging\u0026#34; = \u0026#34;m5.large\u0026#34;, \u0026#34;test\u0026#34; = \u0026#34;t3.large\u0026#34;, \u0026#34;dev\u0026#34; = \u0026#34;t3.micro\u0026#34; } } This can now be used in the aws_instance code. The lookup function requires three arguments. The first map itself, the second is the value of the key by which lookup is done, and the third is the default value in case there is no key in the map\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { count = var.env == \u0026#34;prod\u0026#34; ? 1 : 0 ami = data.aws_ami.amzn.id instance_type = lookup(var.instance_types, var.env, \u0026#34;t3.micro\u0026#34;) subnet_id = \u0026#34;subnet-5f70bf74\u0026#34; vpc_security_group_ids = [\u0026#34;sg-b04b8cd4\u0026#34;] tags = { Env = \u0026#34;Dev\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } } And now whatever value of the env variable I would pass the desired server type will be used.\nVideo ","permalink":"https://mpostument.com/posts/programming/terraform/terraform-condition-and-lookup/","summary":"Hello!\nIn this programming, I will talk about conditions and lookups.\nConditions Conditions in terraform are used to select one value depending on another. For example, if it is a dev environment, then use the t3.micro server type, and if it is a production environment, then use m5.large. This is easy to do with a condition. Conditions are written like condition ? true_val : false_val. For server type conditions will look like var.","title":"Terraform Condition and Lookup"},{"content":"Hello!\nIn this programming, I will talk about variables in terraform and terraform data.\nVariables I will use the code from previous posts. I create a new file variables.tf and add the instance_type variable to it.\nvariable \u0026#34;instance_type\u0026#34; { default = \u0026#34;t2.micro\u0026#34; description = \u0026#34;Instance type for test instance\u0026#34; type = string } In this block, several attributes are added to the instance_type variable: default - the variable will have the value specified in this attribute unless otherwise specified description - description of the variable type - is the type of the variable\nThese attributes are optional. The instance_type variable can be created as follows\nvariable \u0026#34;instance_type\u0026#34; {} But it is more difficult to understand what this variable is. After the variable is created, it can be used in the terraform code as var.instance_type\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; instance_type = var.instance_type subnet_id = \u0026#34;subnet-db73f0ac\u0026#34; vpc_security_group_ids = [\u0026#34;sg-b04b8cd4\u0026#34;] tags = { Env = \u0026#34;Dev\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } } If I need to use a different type of instance, there are several options. The first is to pass it when running terraform with parameter var\n$ terraform apply -var instance_type=t3.micro # aws_instance.foo will be created + resource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { + ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; + arn = (known after apply) + associate_public_ip_address = (known after apply) + availability_zone = (known after apply) + cpu_core_count = (known after apply) + cpu_threads_per_core = (known after apply) + disable_api_stop = (known after apply) + disable_api_termination = (known after apply) + ebs_optimized = (known after apply) + get_password_data = false + host_id = (known after apply) + id = (known after apply) + instance_initiated_shutdown_behavior = (known after apply) + instance_state = (known after apply) + instance_type = \u0026#34;t3.micro\u0026#34; Another option is using a file with variables, I will create a file called prod.tfvars with content\ninstance_type = \u0026#34;m5.large\u0026#34; Then I will call terraform with -var-file option\n$ terraform plan -var-file=prod.tfvars Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_instance.foo will be created + resource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { + ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; + arn = (known after apply) + associate_public_ip_address = (known after apply) + availability_zone = (known after apply) + cpu_core_count = (known after apply) + cpu_threads_per_core = (known after apply) + disable_api_stop = (known after apply) + disable_api_termination = (known after apply) + ebs_optimized = (known after apply) + get_password_data = false + host_id = (known after apply) + id = (known after apply) + instance_initiated_shutdown_behavior = (known after apply) + instance_state = (known after apply) + instance_type = \u0026#34;m5.large\u0026#34; The next option is via env variables. To do this, you need to create an env variable called TF_VAR_name.\n$export TF_VAR_instance_type=r5.xlarge $ terraform apply Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_instance.foo will be created + resource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { + ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; + arn = (known after apply) + associate_public_ip_address = (known after apply) + availability_zone = (known after apply) + cpu_core_count = (known after apply) + cpu_threads_per_core = (known after apply) + disable_api_stop = (known after apply) + disable_api_termination = (known after apply) + ebs_optimized = (known after apply) + get_password_data = false + host_id = (known after apply) + id = (known after apply) + instance_initiated_shutdown_behavior = (known after apply) + instance_state = (known after apply) + instance_type = \u0026#34;r5.xlarge\u0026#34; And the last option uses a file named .auto.tfvars. Create a file with the name prod.auto.tfvars with the next content\ninstance_type = \u0026#34;c5.large\u0026#34; Once I launch terraform it will automatically read variables from this file\n$ terraform apply Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_instance.foo will be created + resource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { + ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; + arn = (known after apply) + associate_public_ip_address = (known after apply) + availability_zone = (known after apply) + cpu_core_count = (known after apply) + cpu_threads_per_core = (known after apply) + disable_api_stop = (known after apply) + disable_api_termination = (known after apply) + ebs_optimized = (known after apply) + get_password_data = false + host_id = (known after apply) + id = (known after apply) + instance_initiated_shutdown_behavior = (known after apply) + instance_state = (known after apply) + instance_type = \u0026#34;c5.large\u0026#34; Locals In addition to the global variables in terraform, you can also use locals. They are convenient to use when some value is repeated often, but there is no need to transfer it to the global variables. They are created in the local\u0026rsquo;s block. Example:\nlocals { subnet_id = \u0026#34;subnet-db73f0ac\u0026#34; } And now this local variable can be used in the code as local.subnet_id\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; instance_type = var.instance_type subnet_id = local.subnet_id vpc_security_group_ids = \u0026#34;sg-123\u0026#34; tags = { Env = \u0026#34;Dev\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } } Data Some attributes are not always convenient to use as variables, and it is better to dynamically receive values when starting terraform. For example, the ami attribute, if you need always to have the latest version of ami regardless of the region. Or there are other resources created by other terraform code or using the AWS UI, such as VPC or subnet_id and you need to use their IDs in your code. Terraform has a data block for this.\nI will show how you can dynamically get the value of ami. To do this, I will create a separate file named data.tf with such content\ndata \u0026#34;aws_ami\u0026#34; \u0026#34;amzn\u0026#34; { most_recent = true owners = [\u0026#34;amazon\u0026#34;] filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;amzn2-ami-kernel-*\u0026#34;] } } Inside the data block are several attributes: most_recent - whether to always get the latest version of AMI or not owners - which AWS account owns ami filter - by what criteria to filter. In this case, I\u0026rsquo;m filtering by a name that must start with amzn2-ami-kernel. And that way I\u0026rsquo;ll always have the ID of the latest amazon Linux 2 AMI no matter in what region I\u0026rsquo;m creating the resources.\nThis can now be used in our aws_instance resource as data.aws_ami.amzn.id\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = data.aws_ami.amzn.id instance_type = var.instance_type subnet_id = local.subnet_id vpc_security_group_ids = \u0026#34;sg-123\u0026#34; tags = { Env = \u0026#34;Dev\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } } Video ","permalink":"https://mpostument.com/posts/programming/terraform/terraform-variables/","summary":"Hello!\nIn this programming, I will talk about variables in terraform and terraform data.\nVariables I will use the code from previous posts. I create a new file variables.tf and add the instance_type variable to it.\nvariable \u0026#34;instance_type\u0026#34; { default = \u0026#34;t2.micro\u0026#34; description = \u0026#34;Instance type for test instance\u0026#34; type = string } In this block, several attributes are added to the instance_type variable: default - the variable will have the value specified in this attribute unless otherwise specified description - description of the variable type - is the type of the variable","title":"Terraform Variables"},{"content":"Hello!\nIn this post i will explain what is lifecycle, dependent_on and outputs in terraform.\nLifecycle create_before_destroy Lifecycle is a meta parameter supported by most resources. And with its help, you can control the behavior when creating/deleting resources. I will take the terraform ec2 creation code from one of the previous posts and add the lifecycle block to it. Inside of it I will specify create_before_destroy = true. Now, if I make changes that require recreating ec2, terraform will first create a new server and only then delete the old one.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; subnet_id = \u0026#34;subnet-222a93327f9a744ed\u0026#34; vpc_security_group_ids = [\u0026#34;sg-2220a119757753b6e\u0026#34;] tags = { Env = \u0026#34;Dev\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } lifecycle { create_before_destroy = true } } Plan: 1 to add, 0 to change, 1 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: yes aws_instance.foo: Creating... aws_instance.foo: Still creating... [10s elapsed] aws_instance.foo: Still creating... [20s elapsed] aws_instance.foo: Still creating... [30s elapsed] aws_instance.foo: Creation complete after 34s [id=i-0c9601ce9bd144f5e] aws_instance.foo (deposed object aba959f0): Destroying... [id=i-082a6b3816e202929] aws_instance.foo: Still destroying... [id=i-082a6b3816e202929, 10s elapsed] aws_instance.foo: Still destroying... [id=i-082a6b3816e202929, 20s elapsed] aws_instance.foo: Still destroying... [id=i-082a6b3816e202929, 30s elapsed] aws_instance.foo: Destruction complete after 31s prevent_destroy The next parameter is prevent_destroy. If it is specified, then terraform will not be able to delete the resource in which prevent_destroy = true is specified.\nlifecycle { prevent_destroy = true } I immediately get an error as it is not possible to delete the resource.\nterraform apply aws_instance.foo: Refreshing state... [id=i-0c9601ce9bd144f5e] ╷ │ Error: Instance cannot be destroyed │ │ on main.tf line 1: │ 1: resource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { │ │ Resource aws_instance.foo has lifecycle.prevent_destroy set, but the plan calls for this resource to be destroyed. To avoid this error and continue with the plan, either disable lifecycle.prevent_destroy or reduce the │ scope of the plan using the -target flag. ignore_changes The next parameter is ignore_changes. It receives a list of attributes which terraform should ignore. For example, a new version of the AMI for EC2 was released, but now i don\u0026rsquo;t want to rebuild server with new AMI. In that case, I can tell terraform to ignore the AMI attribute\nlifecycle { ignore_changes = [ ami ] } terraform plan aws_instance.foo: Refreshing state... [id=i-0c9601ce9bd144f5e] No changes. Your infrastructure matches the configuration. Terraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed. replace_triggered_by The last parameter is replace_triggered_by. It recreates the resource in case some attribute of resource has changed (Added in terraform version 1.2)\nlifecycle { replace_triggered_by = [ aws_instance.foo ] } I will add a new resource that will generate a random string and specify it as a replace trigger.\nresource \u0026#34;random_id\u0026#34; \u0026#34;server\u0026#34; { keepers = { ami_id = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; } byte_length = 12 } resource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; subnet_id = \u0026#34;subnet-db73f0ac\u0026#34; vpc_security_group_ids = [\u0026#34;sg-b04b8cd4\u0026#34;] tags = { Env = \u0026#34;Dev222\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } lifecycle { replace_triggered_by = [ random_id.server ] } } And as soon as the generated string changes, the server will also be recreated\nterraform apply random_id.server: Refreshing state... [id=dS2PD5rErdg] aws_instance.foo: Refreshing state... [id=i-0747f2f517d71ae85] Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: -/+ destroy and then create replacement Terraform will perform the following actions: # aws_instance.foo will be replaced due to changes in replace_triggered_by -/+ resource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ~ arn = \u0026#34;arn:aws:ec2:us-east-1:241394805508:instance/i-0747f2f517d71ae85\u0026#34; -\u0026gt; (known after apply) ~ associate_public_ip_address = true -\u0026gt; (known after apply) ~ availability_zone = \u0026#34;us-east-1c\u0026#34; -\u0026gt; (known after apply) ~ cpu_core_count = 1 -\u0026gt; (known after apply) ~ cpu_threads_per_core = 2 -\u0026gt; (known after apply) ~ disable_api_stop = false -\u0026gt; (known after apply) ~ disable_api_termination = false -\u0026gt; (known after apply) ~ ebs_optimized = false -\u0026gt; (known after apply) - hibernation = false -\u0026gt; null + host_id = (known after apply) ~ id = \u0026#34;i-0747f2f517d71ae85\u0026#34; -\u0026gt; (known after apply) ~ instance_initiated_shutdown_behavior = \u0026#34;stop\u0026#34; -\u0026gt; (known after apply) ~ instance_state = \u0026#34;running\u0026#34; -\u0026gt; (known after apply) ~ ipv6_address_count = 0 -\u0026gt; (known after apply) ~ ipv6_addresses = [] -\u0026gt; (known after apply) + key_name = (known after apply) ~ monitoring = false -\u0026gt; (known after apply) + outpost_arn = (known after apply) + password_data = (known after apply) + placement_group = (known after apply) + placement_partition_number = (known after apply) ~ primary_network_interface_id = \u0026#34;eni-034b12b8b4d792825\u0026#34; -\u0026gt; (known after apply) ~ private_dns = \u0026#34;ip-172-31-38-182.ec2.internal\u0026#34; -\u0026gt; (known after apply) ~ private_ip = \u0026#34;172.31.38.182\u0026#34; -\u0026gt; (known after apply) ~ public_dns = \u0026#34;ec2-3-85-112-28.compute-1.amazonaws.com\u0026#34; -\u0026gt; (known after apply) ~ public_ip = \u0026#34;3.85.112.28\u0026#34; -\u0026gt; (known after apply) ~ secondary_private_ips = [] -\u0026gt; (known after apply) ~ security_groups = [ - \u0026#34;default\u0026#34;, ] -\u0026gt; (known after apply) tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev222\u0026#34; } ~ tenancy = \u0026#34;default\u0026#34; -\u0026gt; (known after apply) + user_data = (known after apply) + user_data_base64 = (known after apply) # (9 unchanged attributes hidden) ~ capacity_reservation_specification { ~ capacity_reservation_preference = \u0026#34;open\u0026#34; -\u0026gt; (known after apply) + capacity_reservation_target { + capacity_reservation_id = (known after apply) + capacity_reservation_resource_group_arn = (known after apply) } } - credit_specification { - cpu_credits = \u0026#34;unlimited\u0026#34; -\u0026gt; null } + ebs_block_device { + delete_on_termination = (known after apply) + device_name = (known after apply) + encrypted = (known after apply) + iops = (known after apply) + kms_key_id = (known after apply) + snapshot_id = (known after apply) + tags = (known after apply) + throughput = (known after apply) + volume_id = (known after apply) + volume_size = (known after apply) + volume_type = (known after apply) } ~ enclave_options { ~ enabled = false -\u0026gt; (known after apply) } + ephemeral_block_device { + device_name = (known after apply) + no_device = (known after apply) + virtual_name = (known after apply) } ~ maintenance_options { ~ auto_recovery = \u0026#34;default\u0026#34; -\u0026gt; (known after apply) } ~ metadata_options { ~ http_endpoint = \u0026#34;enabled\u0026#34; -\u0026gt; (known after apply) ~ http_put_response_hop_limit = 1 -\u0026gt; (known after apply) ~ http_tokens = \u0026#34;optional\u0026#34; -\u0026gt; (known after apply) ~ instance_metadata_tags = \u0026#34;disabled\u0026#34; -\u0026gt; (known after apply) } + network_interface { + delete_on_termination = (known after apply) + device_index = (known after apply) + network_card_index = (known after apply) + network_interface_id = (known after apply) } ~ private_dns_name_options { ~ enable_resource_name_dns_a_record = false -\u0026gt; (known after apply) ~ enable_resource_name_dns_aaaa_record = false -\u0026gt; (known after apply) ~ hostname_type = \u0026#34;ip-name\u0026#34; -\u0026gt; (known after apply) } ~ root_block_device { ~ delete_on_termination = true -\u0026gt; (known after apply) ~ device_name = \u0026#34;/dev/xvda\u0026#34; -\u0026gt; (known after apply) ~ encrypted = false -\u0026gt; (known after apply) ~ iops = 100 -\u0026gt; (known after apply) + kms_key_id = (known after apply) ~ tags = {} -\u0026gt; (known after apply) ~ throughput = 0 -\u0026gt; (known after apply) ~ volume_id = \u0026#34;vol-0b353de4ae7778314\u0026#34; -\u0026gt; (known after apply) ~ volume_size = 8 -\u0026gt; (known after apply) ~ volume_type = \u0026#34;gp2\u0026#34; -\u0026gt; (known after apply) } } # random_id.server must be replaced -/+ resource \u0026#34;random_id\u0026#34; \u0026#34;server\u0026#34; { ~ b64_std = \u0026#34;dS2PD5rErdg=\u0026#34; -\u0026gt; (known after apply) ~ b64_url = \u0026#34;dS2PD5rErdg\u0026#34; -\u0026gt; (known after apply) ~ dec = \u0026#34;8443562173573410264\u0026#34; -\u0026gt; (known after apply) ~ hex = \u0026#34;752d8f0f9ac4add8\u0026#34; -\u0026gt; (known after apply) ~ id = \u0026#34;dS2PD5rErdg\u0026#34; -\u0026gt; (known after apply) + keepers = { + \u0026#34;ami_id\u0026#34; = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; } # forces replacement # (1 unchanged attribute hidden) } Plan: 2 to add, 0 to change, 2 to destroy. Depends On depends_on is another meta parameter supported by most terraform resources. And it allows you to create resources in a certain sequence. For example, I want to create an ec2 server and a security group. And I can specify that the server should be created only after the security group has already been created. I can do this by specifying the depends_on parameter in the aws_instance resource.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; subnet_id = \u0026#34;subnet-222a93327f9a744ed\u0026#34; vpc_security_group_ids = [\u0026#34;sg-2220a119757753b6e\u0026#34;] tags = { Env = \u0026#34;Dev\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } depends_on = [ aws_security_group.test_sg ] } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;test_sg\u0026#34; { name = \u0026#34;test-sg\u0026#34; description = \u0026#34;allow traffic to web app\u0026#34; vpc_id = \u0026#34;vcp-123\u0026#34; dynamic \u0026#34;ingress\u0026#34; { for_each = [8080, 443, 22, 80, 3000, 8082] content { from_port = ingress.value to_port = ingress.value protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [ \u0026#34;0.0.0.0/0\u0026#34; ] } } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } Plan: 2 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: yes aws_security_group.test_sg: Creating... aws_security_group.test_sg: Creation complete after 3s [id=sg-0512f173f605de311] aws_instance.foo: Creating... aws_instance.foo: Still creating... [10s elapsed] aws_instance.foo: Creation complete after 15s [id=i-070da129fdd9d4c92] Apply complete! Resources: 2 added, 0 changed, 0 destroyed. TODO: Add terraform output Outputs Outputs is similar to return in programming languages. It returns values ​​that will be printed to the console after terraform is executed, and these values ​​can also be used by other terraform code. Most often, this is used in modules when the child module transmits some values ​​to the parent module, or as a simple output to the terminal.\nIf terraform creates ec2 and after creation I want to display its IP address, then it can be done like this\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; subnet_id = \u0026#34;subnet-222a93327f9a744ed\u0026#34; vpc_security_group_ids = [\u0026#34;sg-2220a119757753b6e\u0026#34;] tags = { Env = \u0026#34;Dev\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } } output \u0026#34;instance_ip_addr\u0026#34; { value = aws_instance.foo.private_ip } Plan: 1 to add, 0 to change, 0 to destroy. Changes to Outputs: + instance_ip_addr = (known after apply) Do you want to perform these actions? Terraform will perform the actions described above. Only \u0026#39;yes\u0026#39; will be accepted to approve. Enter a value: yes aws_instance.foo: Creating... aws_instance.foo: Still creating... [10s elapsed] aws_instance.foo: Creation complete after 14s [id=i-02b842be3d4556465] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. Outputs: instance_ip_addr = \u0026#34;172.31.43.168\u0026#34; Video ","permalink":"https://mpostument.com/posts/programming/terraform/terraform-lifecycle/","summary":"Hello!\nIn this post i will explain what is lifecycle, dependent_on and outputs in terraform.\nLifecycle create_before_destroy Lifecycle is a meta parameter supported by most resources. And with its help, you can control the behavior when creating/deleting resources. I will take the terraform ec2 creation code from one of the previous posts and add the lifecycle block to it. Inside of it I will specify create_before_destroy = true. Now, if I make changes that require recreating ec2, terraform will first create a new server and only then delete the old one.","title":"Terraform Lifecycle and Outputs"},{"content":"Hello!\nIn this article, I will show how to use the dynamic block on the example of creating an AWS Security Group.\nTo begin with, I will create a Security Group without using the dynamic block. In this example, each rule is described in a separate ingress block.\nresource \u0026#34;aws_security_group\u0026#34; \u0026#34;test_sg\u0026#34; { name = \u0026#34;test-sg\u0026#34; description = \u0026#34;allow traffic to web app\u0026#34; vpc_id = \u0026#34;vcp-123\u0026#34; ingress { from_port = 8082 to_port = 8082 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [ \u0026#34;0.0.0.0/0\u0026#34; ] } ingress { from_port = 3000 to_port = 3000 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [ \u0026#34;0.0.0.0/0\u0026#34; ] } ingress { from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [ \u0026#34;0.0.0.0/0\u0026#34; ] } ingress { from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [ \u0026#34;0.0.0.0/0\u0026#34; ] } ingress { from_port = 443 to_port = 443 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [ \u0026#34;0.0.0.0/0\u0026#34; ] } ingress { from_port = 8080 to_port = 8080 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [ \u0026#34;0.0.0.0/0\u0026#34; ] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = { Name = \u0026#34;test-sg\u0026#34; } } This code can be significantly shortened with a single dynamic block. In this case, the dynamic block works as a for loop. In for_each parameter I describe a list of values that terraform should loop through. And in the content block, I use the values from the list in loop. And thus the code was reduced from 70 lines of code to 20.\nresource \u0026#34;aws_security_group\u0026#34; \u0026#34;test_sg\u0026#34; { name = \u0026#34;test-sg\u0026#34; description = \u0026#34;allow traffic to web app\u0026#34; vpc_id = \u0026#34;vcp-123\u0026#34; dynamic \u0026#34;ingress\u0026#34; { for_each = [8080, 443, 22, 80, 3000, 8082] content { from_port = ingress.value to_port = ingress.value protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [ \u0026#34;0.0.0.0/0\u0026#34; ] } } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } } Video ","permalink":"https://mpostument.com/posts/programming/terraform/terraform-dynamic/","summary":"Hello!\nIn this article, I will show how to use the dynamic block on the example of creating an AWS Security Group.\nTo begin with, I will create a Security Group without using the dynamic block. In this example, each rule is described in a separate ingress block.\nresource \u0026#34;aws_security_group\u0026#34; \u0026#34;test_sg\u0026#34; { name = \u0026#34;test-sg\u0026#34; description = \u0026#34;allow traffic to web app\u0026#34; vpc_id = \u0026#34;vcp-123\u0026#34; ingress { from_port = 8082 to_port = 8082 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [ \u0026#34;0.","title":"Terraform Dynamic"},{"content":"Hello!\nIn this article, I will show how in terraform you can use static files or generate files dynamically. I will show it on the example of user_data in EC2. With the help of user_data, you can transfer a script that will be executed on the server at the time of creation. This file can be predefined with static content and transferred using terraform to user_data, or the content can be dynamically generated and transferred.\nTerraform file I will create a file called setup_httpd.sh, which will contain the following content\n#!/bin/bash yum install httpd -y /sbin/chkconfig --levels 235 httpd on service httpd start echo \u0026#34;\u0026lt;h1\u0026gt;Maksym Website\u0026lt;/h1\u0026gt;\u0026#34; \u0026gt; /var/www/html/index.html This script will install httpd and create an html page. Now how can I use this file? I will take the terraform code from the previous article and add user_data to it.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; subnet_id = \u0026#34;subnet-222a93327f9a744ed\u0026#34; vpc_security_group_ids = [\u0026#34;sg-2220a119757753b6e\u0026#34;] user_data = file(\u0026#34;setup_httpd.sh\u0026#34;) tags = { Env = \u0026#34;Dev\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } } And now I will start terraform. Once server is created I can check generated script in the AWS console.\nAnd if you open the IP address of the server in the browser, you will see this\nTerraform Template But this file was static and if it is necessary to change, for example, the content of a web page, the file must be changed. Terraform provided method templatefile() that allows you to generate the content of the file dynamically. To do this, I will create a new file called `setup_httpd.sh.tpl\u0026rsquo;, the content of file will be almost the same as in the previous one, but with one difference. In the last line, instead of the text Maksym Website, I use ${user} Website. Where ${user} is a parameter that terraform will replace with the appropriate value.\n#!/bin/bash yum install httpd -y /sbin/chkconfig --levels 235 httpd on service httpd start echo \u0026#34;\u0026lt;h1\u0026gt;${user} Website\u0026lt;/h1\u0026gt;\u0026#34; \u0026gt;/var/www/html/index.html You also need to change the terraform code itself. I will replace the file method with templatefile and add the user parameter in addition to the file itself.\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; subnet_id = \u0026#34;subnet-222a93327f9a744ed\u0026#34; vpc_security_group_ids = [\u0026#34;sg-2220a119757753b6e\u0026#34;] user_data = templatefile(\u0026#34;setup_httpd.sh.tpl\u0026#34;, { user = \u0026#34;Jhon\u0026#34; }) tags = { Env = \u0026#34;Dev\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } } Just like the previous time, I will look at the generated script\nAnd open the IP address of the server\nVideo ","permalink":"https://mpostument.com/posts/programming/terraform/terraform-template/","summary":"Hello!\nIn this article, I will show how in terraform you can use static files or generate files dynamically. I will show it on the example of user_data in EC2. With the help of user_data, you can transfer a script that will be executed on the server at the time of creation. This file can be predefined with static content and transferred using terraform to user_data, or the content can be dynamically generated and transferred.","title":"Terraform Template"},{"content":"Hello!\nIn the previous article, I created an ec2 in AWS and then executed terraform destroy and terraform only deleted the created ec2 without affecting other resources. How did he do it?\nLocal State To understand this, you need to look at the terraform directory after executing terraform apply. Immediately after terraform apply is executed, a file with name terraform.tfstate will be created with the following content\n{ \u0026#34;version\u0026#34;: 4, \u0026#34;terraform_version\u0026#34;: \u0026#34;1.2.5\u0026#34;, \u0026#34;serial\u0026#34;: 1, \u0026#34;lineage\u0026#34;: \u0026#34;1c9d6b0b-cf06-d17a-c428-a609626d0016\u0026#34;, \u0026#34;outputs\u0026#34;: {}, \u0026#34;resources\u0026#34;: [ { \u0026#34;mode\u0026#34;: \u0026#34;managed\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;aws_instance\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;foo\u0026#34;, \u0026#34;provider\u0026#34;: \u0026#34;provider[\\\u0026#34;registry.terraform.io/hashicorp/aws\\\u0026#34;]\u0026#34;, \u0026#34;instances\u0026#34;: [ { \u0026#34;schema_version\u0026#34;: 1, \u0026#34;attributes\u0026#34;: { \u0026#34;ami\u0026#34;: \u0026#34;ami-0cff7528ff583bf9a\u0026#34;, \u0026#34;arn\u0026#34;: \u0026#34;arn:aws:ec2:us-east-1:id:instance/i-0c8ee996d854df034\u0026#34;, \u0026#34;associate_public_ip_address\u0026#34;: true, \u0026#34;availability_zone\u0026#34;: \u0026#34;us-east-1c\u0026#34;, \u0026#34;capacity_reservation_specification\u0026#34;: [ { \u0026#34;capacity_reservation_preference\u0026#34;: \u0026#34;open\u0026#34;, \u0026#34;capacity_reservation_target\u0026#34;: [] } ], \u0026#34;cpu_core_count\u0026#34;: 1, \u0026#34;cpu_threads_per_core\u0026#34;: 1, \u0026#34;credit_specification\u0026#34;: [ { \u0026#34;cpu_credits\u0026#34;: \u0026#34;standard\u0026#34; } ], \u0026#34;disable_api_stop\u0026#34;: false, \u0026#34;disable_api_termination\u0026#34;: false, \u0026#34;ebs_block_device\u0026#34;: [], \u0026#34;ebs_optimized\u0026#34;: false, \u0026#34;enclave_options\u0026#34;: [ { \u0026#34;enabled\u0026#34;: false } ], \u0026#34;ephemeral_block_device\u0026#34;: [], \u0026#34;get_password_data\u0026#34;: false, \u0026#34;hibernation\u0026#34;: false, \u0026#34;host_id\u0026#34;: null, \u0026#34;iam_instance_profile\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;i-0c8ee996d854df034\u0026#34;, \u0026#34;instance_initiated_shutdown_behavior\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;instance_state\u0026#34;: \u0026#34;running\u0026#34;, \u0026#34;instance_type\u0026#34;: \u0026#34;t2.micro\u0026#34;, \u0026#34;ipv6_address_count\u0026#34;: 0, \u0026#34;ipv6_addresses\u0026#34;: [], \u0026#34;key_name\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;launch_template\u0026#34;: [], \u0026#34;maintenance_options\u0026#34;: [ { \u0026#34;auto_recovery\u0026#34;: \u0026#34;default\u0026#34; } ], \u0026#34;metadata_options\u0026#34;: [ { \u0026#34;http_endpoint\u0026#34;: \u0026#34;enabled\u0026#34;, \u0026#34;http_put_response_hop_limit\u0026#34;: 1, \u0026#34;http_tokens\u0026#34;: \u0026#34;optional\u0026#34;, \u0026#34;instance_metadata_tags\u0026#34;: \u0026#34;disabled\u0026#34; } ], \u0026#34;monitoring\u0026#34;: false, \u0026#34;network_interface\u0026#34;: [], \u0026#34;outpost_arn\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;password_data\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;placement_group\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;placement_partition_number\u0026#34;: null, \u0026#34;primary_network_interface_id\u0026#34;: \u0026#34;eni-07dfb8493ee40b4b8\u0026#34;, \u0026#34;private_dns\u0026#34;: \u0026#34;ip-172-31-46-55.ec2.internal\u0026#34;, \u0026#34;private_dns_name_options\u0026#34;: [ { \u0026#34;enable_resource_name_dns_a_record\u0026#34;: false, \u0026#34;enable_resource_name_dns_aaaa_record\u0026#34;: false, \u0026#34;hostname_type\u0026#34;: \u0026#34;ip-name\u0026#34; } ], \u0026#34;private_ip\u0026#34;: \u0026#34;172.31.46.55\u0026#34;, \u0026#34;public_dns\u0026#34;: \u0026#34;ec2-54-89-34-118.compute-1.amazonaws.com\u0026#34;, \u0026#34;public_ip\u0026#34;: \u0026#34;54.89.34.118\u0026#34;, \u0026#34;root_block_device\u0026#34;: [ { \u0026#34;delete_on_termination\u0026#34;: true, \u0026#34;device_name\u0026#34;: \u0026#34;/dev/xvda\u0026#34;, \u0026#34;encrypted\u0026#34;: false, \u0026#34;iops\u0026#34;: 100, \u0026#34;kms_key_id\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;tags\u0026#34;: {}, \u0026#34;throughput\u0026#34;: 0, \u0026#34;volume_id\u0026#34;: \u0026#34;vol-0a26dbaed6df0e005\u0026#34;, \u0026#34;volume_size\u0026#34;: 8, \u0026#34;volume_type\u0026#34;: \u0026#34;gp2\u0026#34; } ], \u0026#34;secondary_private_ips\u0026#34;: [], \u0026#34;security_groups\u0026#34;: [ \u0026#34;default\u0026#34; ], \u0026#34;source_dest_check\u0026#34;: true, \u0026#34;subnet_id\u0026#34;: \u0026#34;subnet-db73f0ac\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;Env\u0026#34;: \u0026#34;Dev\u0026#34; }, \u0026#34;tags_all\u0026#34;: { \u0026#34;Env\u0026#34;: \u0026#34;Dev\u0026#34; }, \u0026#34;tenancy\u0026#34;: \u0026#34;default\u0026#34;, \u0026#34;timeouts\u0026#34;: null, \u0026#34;user_data\u0026#34;: null, \u0026#34;user_data_base64\u0026#34;: null, \u0026#34;user_data_replace_on_change\u0026#34;: false, \u0026#34;volume_tags\u0026#34;: { \u0026#34;Env\u0026#34;: \u0026#34;Dev\u0026#34; }, \u0026#34;vpc_security_group_ids\u0026#34;: [ \u0026#34;sg-b04b8cd4\u0026#34; ] }, \u0026#34;sensitive_attributes\u0026#34;: [], \u0026#34;private\u0026#34;: \u0026#34;\u0026#34; } ] } ] } This file stores all the information about the created ec2. And if you run terraform destroy, terraform will go to this file, see what resources were created and delete them. But what will happen if you delete this file and run terraform again? In this case, if you run terraform apply, then terraform will don\u0026rsquo;t know that such a server already exists and will create it again, and similarly with terraform destroy, terraform will check that the server does not exist and report that there is nothing to delete.\n$ terraform destroy No changes. No objects need to be destroyed. Either you have not created any objects yet or the existing objects were already deleted outside of Terraform. Destroy complete! Resources: 0 destroyed. Remote state How then can several developers work on the same code? Terraform has such a concept as remote backend. The remote backend allows you to store state in remote storage such as s3 and every time you call terraform, the state file will be used from remote storage and it will always be the same for anyone who calls terraform and has the appropriate permissions. But there is another problem, if several developers run terraform at the same time, whose changes will be in the state file? This problem is solved by another mechanism provided by the remote backend called state lock. But not all backends support state lock. In AWS, you need to use dynamodb in addition to s3 for this.\nLet\u0026rsquo;s try to add the remote backend now. From the previous article, provider.tf looked like this\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;4.23.0\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; profile = \u0026#34;default\u0026#34; } To add the remote backend to the terraform section, the additional configuration needs to be added. And then the file will look like this\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;4.23.0\u0026#34; } } backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;BUCKET_NAME\u0026#34; key = \u0026#34;tfstate/app1.tf\u0026#34; region = \u0026#34;us-east-1\u0026#34; profile = \u0026#34;default\u0026#34; encrypt = true } } provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; profile = \u0026#34;default\u0026#34; } It is also recommended to enable versioning in s3, in order to have a history of changes to the state file and be able to return to the previous version if necessary. State also can have secure data such as password, access key and etc, so better to use encryption to protect data.\nAnd now if you run terraform plan there will be an error\n$ terraform plan ╷ │ Error: Backend initialization required, please run \u0026#34;terraform init\u0026#34; │ │ Reason: Initial configuration of the requested backend \u0026#34;s3\u0026#34; │ │ The \u0026#34;backend\u0026#34; is the interface that Terraform uses to store state, │ perform operations, etc. If this message is showing up, it means that the │ Terraform configuration you\u0026#39;re using is using a custom configuration for │ the Terraform backend. │ │ Changes to backend configurations require reinitialization. This allows │ Terraform to set up the new configuration, copy existing state, etc. Please run │ \u0026#34;terraform init\u0026#34; with either the \u0026#34;-reconfigure\u0026#34; or \u0026#34;-migrate-state\u0026#34; flags to │ use the current configuration. │ │ If the change reason above is incorrect, please verify your configuration │ hasn\u0026#39;t changed and try again. At this point, no changes to your existing │ configuration or state have been made. You need to run terraform init again to configure the remote backend\n$ terraform init Initializing the backend... Successfully configured the backend \u0026#34;s3\u0026#34;! Terraform will automatically use this backend unless the backend configuration changes. Initializing provider plugins... - Reusing previous version of hashicorp/aws from the dependency lock file - Using previously-installed hashicorp/aws v4.23.0 Terraform has been successfully initialized! You may now begin working with Terraform. Try running \u0026#34;terraform plan\u0026#34; to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. And now the state file will be in s3\nState Lock In order to use state lock in s3, you need to create a dynamodb table, if you create it using terraform, in code would look like this, but table also can be created manually.\nresource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;state-lock\u0026#34; { name = \u0026#34;terraform-state-lock\u0026#34; hash_key = \u0026#34;LockID\u0026#34; attribute { name = \u0026#34;LockID\u0026#34; type = \u0026#34;S\u0026#34; } tags { Name = \u0026#34;Terraform state lock\u0026#34; } } And add the name of the table in the terraform backend configuration\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;4.23.0\u0026#34; } } backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;BUCKET_NAME\u0026#34; key = \u0026#34;tfstate/app1.tf\u0026#34; region = \u0026#34;us-east-1\u0026#34; profile = \u0026#34;default\u0026#34; encrypt = true dynamodb_table = \u0026#34;terraform-state-lock\u0026#34; } } After that, only one terraform execution will work at the same time, if someone starts terraform in parallel, he will receive a message that state is now locked.\nVideo ","permalink":"https://mpostument.com/posts/programming/terraform/terraform-state/","summary":"Hello!\nIn the previous article, I created an ec2 in AWS and then executed terraform destroy and terraform only deleted the created ec2 without affecting other resources. How did he do it?\nLocal State To understand this, you need to look at the terraform directory after executing terraform apply. Immediately after terraform apply is executed, a file with name terraform.tfstate will be created with the following content\n{ \u0026#34;version\u0026#34;: 4, \u0026#34;terraform_version\u0026#34;: \u0026#34;1.","title":"Terraform State"},{"content":"Hello!\nIn the previous article, we learn how to install terraform. And now we will learn how using terraform you can create and delete resources in AWS.\nIn order for terraform to manage resources in AWS, you need to install terraform AWS provider. Terraform will do this automatically if I specify AWS provider in the terraform code and call terraform init.\nI will create a terraform_code directory and in this directory, I will create a file with the name provider.tf. The name of the file can be any, but I prefer to call it provider.tf, because it is easier to understand what is in this file based on a name.\nmkdir terraform_code touch provider.tf I will add the following content to provider.tf\nterraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;4.23.0\u0026#34; } } } The terraform section indicates which provider to use and which version. AWS Provider release information can be found on GitHub.\nOnce a file is saved, you can immediately execute terraform init to load the provider. But before that, you need to choose which version of terraform will be used. To do this, I will create a file named .terraform-version and write 1.2.6 into it. Then tfenv will do everything automatically.\n$ terraform init Initializing the backend... Initializing provider plugins... - Finding hashicorp/aws versions matching \u0026#34;4.23.0\u0026#34;... - Installing hashicorp/aws v4.23.0... - Installed hashicorp/aws v4.23.0 (signed by HashiCorp) Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \u0026#34;terraform init\u0026#34; in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \u0026#34;terraform plan\u0026#34; to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. When the command is finished successfully, a file named .terraform.lock.hcl and a directory named .terraform will be created. The content of .terraform.lock.hcl will look something like this\n# This file is maintained automatically by \u0026#34;terraform init\u0026#34;. # Manual edits may be lost in future updates. provider \u0026#34;registry.terraform.io/hashicorp/aws\u0026#34; { version = \u0026#34;4.23.0\u0026#34; constraints = \u0026#34;4.23.0\u0026#34; hashes = [ \u0026#34;h1:JDJLmKK61GLw8gHQtCzmvlwPNZIu46/M5uBg/TDlBa0=\u0026#34;, \u0026#34;zh:17adbedc9a80afc571a8de7b9bfccbe2359e2b3ce1fffd02b456d92248ec9294\u0026#34;, \u0026#34;zh:23d8956b031d78466de82a3d2bbe8c76cc58482c931af311580b8eaef4e6a38f\u0026#34;, \u0026#34;zh:343fe19e9a9f3021e26f4af68ff7f4828582070f986b6e5e5b23d89df5514643\u0026#34;, \u0026#34;zh:6b8ff83d884b161939b90a18a4da43dd464c4b984f54b5f537b2870ce6bd94bc\u0026#34;, \u0026#34;zh:7777d614d5e9d589ad5508eecf4c6d8f47d50fcbaf5d40fa7921064240a6b440\u0026#34;, \u0026#34;zh:82f4578861a6fd0cde9a04a1926920bd72d993d524e5b34d7738d4eff3634c44\u0026#34;, \u0026#34;zh:9b12af85486a96aedd8d7984b0ff811a4b42e3d88dad1a3fb4c0b580d04fa425\u0026#34;, \u0026#34;zh:a08fefc153bbe0586389e814979cf7185c50fcddbb2082725991ed02742e7d1e\u0026#34;, \u0026#34;zh:ae789c0e7cb777d98934387f8888090ccb2d8973ef10e5ece541e8b624e1fb00\u0026#34;, \u0026#34;zh:b4608aab78b4dbb32c629595797107fc5a84d1b8f0682f183793d13837f0ecf0\u0026#34;, \u0026#34;zh:ed2c791c2354764b565f9ba4be7fc845c619c1a32cefadd3154a5665b312ab00\u0026#34;, \u0026#34;zh:f94ac0072a8545eebabf417bc0acbdc77c31c006ad8760834ee8ee5cdb64e743\u0026#34;, ] } In this file, all the versions of the dependencies used by terraform will be specified, and the versions specified in the file will be used when I run terraform init next time. The .terraform directory will contain downloaded modules and providers\n|-- .terraform | `-- providers | `-- registry.terraform.io | `-- hashicorp | `-- aws | `-- 4.23.0 | `-- linux_amd64 | `-- terraform-provider-aws_v4.23.0_x5 Also, in order to manage AWS resources, you need to generate an Access Key and Secret Key in AWS for your user and save them in the file ~/.aws/credentials\n[default] aws_access_key_id = ACCESS_KEY aws_secret_access_key = SECRET_KEY Let\u0026rsquo;s start writing the code. I will create a file named main.tf, which should be in the same directory as provider.tf. The following content can be added both in provider.tf and in main.tf or you create a completely new file, for example, terraform.tf. I will add the next lines to the provider.tf, because it is also the configuration of provider\nprovider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; profile = \u0026#34;default\u0026#34; } I specify which region and which AWS profile to use. A few steps above I created a profile named default when saved Access Key and Secret Key. One terraform code can use several profiles which will use different regions or different AWS accounts.\nIn main.tf i will add next\nresource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; subnet_id = \u0026#34;subnet-222a93327f9a744ed\u0026#34; vpc_security_group_ids = [\u0026#34;sg-2220a119757753b6e\u0026#34;] tags = { Env = \u0026#34;Dev\u0026#34; } volume_tags = { \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } } This code describes with which parameters to create an ec2 instance. I specify which AMI to use (Amazon Linux 2), in which subnet ,and with which security group to create a server. I also indicate which tags to use for the server and disk.\nI save the file and run terraform plan -out tf.plan\n$ terraform plan -out tf.plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # aws_instance.foo will be created + resource \u0026#34;aws_instance\u0026#34; \u0026#34;foo\u0026#34; { + ami = \u0026#34;ami-0cff7528ff583bf9a\u0026#34; + arn = (known after apply) + associate_public_ip_address = (known after apply) + availability_zone = (known after apply) + cpu_core_count = (known after apply) + cpu_threads_per_core = (known after apply) + disable_api_stop = (known after apply) + disable_api_termination = (known after apply) + ebs_optimized = (known after apply) + get_password_data = false + host_id = (known after apply) + id = (known after apply) + instance_initiated_shutdown_behavior = (known after apply) + instance_state = (known after apply) + instance_type = \u0026#34;t2.micro\u0026#34; + ipv6_address_count = (known after apply) + ipv6_addresses = (known after apply) + key_name = (known after apply) + monitoring = (known after apply) + outpost_arn = (known after apply) + password_data = (known after apply) + placement_group = (known after apply) + placement_partition_number = (known after apply) + primary_network_interface_id = (known after apply) + private_dns = (known after apply) + private_ip = (known after apply) + public_dns = (known after apply) + public_ip = (known after apply) + secondary_private_ips = (known after apply) + security_groups = (known after apply) + source_dest_check = true + subnet_id = \u0026#34;subnet-222a93327f9a744ed\u0026#34; + tags = { + \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } + tags_all = { + \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } + tenancy = (known after apply) + user_data = (known after apply) + user_data_base64 = (known after apply) + user_data_replace_on_change = false + volume_tags = { + \u0026#34;Env\u0026#34; = \u0026#34;Dev\u0026#34; } + vpc_security_group_ids = [ + \u0026#34;sg-2220a119757753b6e\u0026#34;, ] + capacity_reservation_specification { + capacity_reservation_preference = (known after apply) + capacity_reservation_target { + capacity_reservation_id = (known after apply) + capacity_reservation_resource_group_arn = (known after apply) } } + ebs_block_device { + delete_on_termination = (known after apply) + device_name = (known after apply) + encrypted = (known after apply) + iops = (known after apply) + kms_key_id = (known after apply) + snapshot_id = (known after apply) + tags = (known after apply) + throughput = (known after apply) + volume_id = (known after apply) + volume_size = (known after apply) + volume_type = (known after apply) } + enclave_options { + enabled = (known after apply) } + ephemeral_block_device { + device_name = (known after apply) + no_device = (known after apply) + virtual_name = (known after apply) } + maintenance_options { + auto_recovery = (known after apply) } + metadata_options { + http_endpoint = (known after apply) + http_put_response_hop_limit = (known after apply) + http_tokens = (known after apply) + instance_metadata_tags = (known after apply) } + network_interface { + delete_on_termination = (known after apply) + device_index = (known after apply) + network_card_index = (known after apply) + network_interface_id = (known after apply) } + private_dns_name_options { + enable_resource_name_dns_a_record = (known after apply) + enable_resource_name_dns_aaaa_record = (known after apply) + hostname_type = (known after apply) } + root_block_device { + delete_on_termination = (known after apply) + device_name = (known after apply) + encrypted = (known after apply) + iops = (known after apply) + kms_key_id = (known after apply) + tags = (known after apply) + throughput = (known after apply) + volume_id = (known after apply) + volume_size = (known after apply) + volume_type = (known after apply) } } Plan: 1 to add, 0 to change, 0 to destroy. ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── Saved the plan to: tf.plan To perform exactly these actions, run the following command to apply: terraform apply \u0026#34;tf.plan\u0026#34; The terraform plan -out tf.plan command will analyze what should be created and display it on the screen. Some of the values will be specified as, for example, + ami = \u0026quot;ami-0cff7528ff583bf9a\u0026quot; the rest will have the value user_data = (known after apply), which means that at the moment terraform does not yet know what the values of these parameters will be and they will be known after the resources are created. Using the -out tf.plan key, the result will be saved to a file. And if you agree with resources which terraform going to create/delete/update, you need to run terraform apply \u0026quot;tf.plan\u0026quot; and the corresponding resources will be create/delete/update.\nIf resources created by terraform are not needed anymore they can be deleted with terraform destroy\nVideo ","permalink":"https://mpostument.com/posts/programming/terraform/terraform-basics/","summary":"Hello!\nIn the previous article, we learn how to install terraform. And now we will learn how using terraform you can create and delete resources in AWS.\nIn order for terraform to manage resources in AWS, you need to install terraform AWS provider. Terraform will do this automatically if I specify AWS provider in the terraform code and call terraform init.\nI will create a terraform_code directory and in this directory, I will create a file with the name provider.","title":"Terraform Basics"},{"content":"Hello!\nThis is the first article in a series of articles about terraform. In it, we will learn how to install terraform using tfenv and how to configure vscode to work with terraform.\ntfenv You will need tfenv to get started. It makes it very easy to install and manage different versions of terraform. The tfenv code is located at github.\nTo install, you need to clone git repository\ngit clone --depth=1 https://github.com/tfutils/tfenv.git ~/.tfenv Now if I call the tfenv command I will get an error. To make tfenv work, you need to add tfenv to the PATH or execute the command ln -s ~/.tfenv/bin/* /usr/local/bin. And now if I call tfenv I will see the result\n$ tfenv tfenv 2.2.2-158-gc05c364 Usage: tfenv \u0026lt;command\u0026gt; [\u0026lt;options\u0026gt;] Commands: install Install a specific version of Terraform use Switch a version to use uninstall Uninstall a specific version of Terraform list List all installed versions list-remote List all installable versions version-name Print current version init Update environment to use tfenv correctly. pin Write the current active version to ./.terraform-version Finally tfenv is ready to use. In order to install terraform, you can call the tfenv install command, which downloads the latest available version of terraform. To install a specific version, you need to execute the command tfenv install 0.13.2, and terraform version 0.13.2 will be installed.\nTo see the list of versions available for installation, you need to execute the tfenv list-remote command\n$ tfenv list-remote 1.3.0-alpha20220706 1.3.0-alpha20220622 1.3.0-alpha20220608 1.2.5 1.2.4 1.2.3 1.2.2 1.2.1 1.2.0 1.2.0-rc2 1.2.0-rc1 1.2.0-beta1 1.2.0-alpha20220413 1.2.0-alpha 1.1.9 1.1.8 ... You can also check the versions that are already installed using tfenv list\n$ tfenv list 1.2.4 1.1.9 1.1.6 1.0.11 1.0.5 1.0.1 1.0.0 0.14.11 0.14.10784023458628 And to specify which version of terraform to use, you need to execute the command tfenv use VERSION.\n$ tfenv use 0.14.6 Switching default version to v0.14.6 Switching completed $ terraform version Terraform v0.14.6 Your version of Terraform is out of date! The latest version is 1.2.5. You can update by downloading from https://www.terraform.io/downloads.html Make sure terraform version is already installed\nIn addition, tfenv supports the .terraform-version file. In this file, you can specify the version of terraform and tfenv will automatically download and start using the correct version of terraform. For example, I will create a file .terraform-version with the content 1.1.1 and in the same directory I execute the command tfenv use\n$ tfenv use No installed versions of terraform matched \u0026#39;1.1.1:^1.1.1$\u0026#39;. Trying to install a matching version since TFENV_AUTO_INSTALL=true Installing Terraform v1.1.1 Downloading release tarball from https://releases.hashicorp.com/terraform/1.1.1/terraform_1.1.1_linux_amd64.zip ################################################################################################################################################ 100,0% Downloading SHA hash file from https://releases.hashicorp.com/terraform/1.1.1/terraform_1.1.1_SHA256SUMS Not instructed to use Local PGP (/home/maksym/.tfenv/use-{gpgv,gnupg}) \u0026amp; No keybase install found, skipping OpenPGP signature verification Archive: /tmp/tfenv_download.xtUx15/terraform_1.1.1_linux_amd64.zip inflating: /home/maksym/.tfenv/versions/1.1.1/terraform Installation of terraform v1.1.1 successful. To make this your default version, run \u0026#39;tfenv use 1.1.1\u0026#39; Switching default version to v1.1.1 Default version file overridden by /tmp/.terraform-version, changing the default version has no effect Default version (when not overridden by .terraform-version or TFENV_TERRAFORM_VERSION) is now: 1.1.1 $ Terraform v1.1.1 on linux_amd64 Your version of Terraform is out of date! The latest version is 1.2.5. You can update by downloading from https://www.terraform.io/downloads.html Although it is not necessary to call tfenv use. I will change the version of terraform in .terraform-version to 1.1.2 and execute terraform. And version will be automatically installed and selected for use\n$ terraform version version \u0026#39;1.1.2\u0026#39; is not installed (set by /tmp/.terraform-version). Installing now as TFENV_AUTO_INSTALL==true Installing Terraform v1.1.2 Downloading release tarball from https://releases.hashicorp.com/terraform/1.1.2/terraform_1.1.2_linux_amd64.zip ################################################################################################################################################ 100,0% Downloading SHA hash file from https://releases.hashicorp.com/terraform/1.1.2/terraform_1.1.2_SHA256SUMS Not instructed to use Local PGP (/home/maksym/.tfenv/use-{gpgv,gnupg}) \u0026amp; No keybase install found, skipping OpenPGP signature verification Archive: /tmp/tfenv_download.6MKarM/terraform_1.1.2_linux_amd64.zip inflating: /home/maksym/.tfenv/versions/1.1.2/terraform Installation of terraform v1.1.2 successful. To make this your default version, run \u0026#39;tfenv use 1.1.2\u0026#39; Terraform v1.1.2 on linux_amd64 Your version of Terraform is out of date! The latest version is 1.2.5. You can update by downloading from https://www.terraform.io/downloads.html With .terraform-version it is very easy to control which version of terraform is used in different projects.\nVSCode To make it easier to write terraform code, I use the Visual Studio Code. After the vscode is installed, you need to install HashiCorp Terraform extension for it. To install the extension, you need to press Ctrl+Shift+X or click on . Enter terraform in the search, select HashiCorp Terraform and click Install\nterraform has a command called terraform fmt and it formats terraform code. This process can be automated using vscode. To do this, you need to press Ctrl+Shift+P and in the panel that appears, type \u0026gt;Open Settings (Json) and select the corresponding item in the menu and add editor.formatOnSave: true in the right window that opens\nEvery time a file is saved, terraform fmt will automatically be called for that file.\nVideo ","permalink":"https://mpostument.com/posts/programming/terraform/terraform-install/","summary":"Hello!\nThis is the first article in a series of articles about terraform. In it, we will learn how to install terraform using tfenv and how to configure vscode to work with terraform.\ntfenv You will need tfenv to get started. It makes it very easy to install and manage different versions of terraform. The tfenv code is located at github.\nTo install, you need to clone git repository\ngit clone --depth=1 https://github.","title":"Manage terraform with tfenv"},{"content":"Hi there!\nThis is the first part of the article about docker SDK. In the current article, we will see how with the help of golang it is possible to make pull and list docker images.\nDocker pull As usual, initialize the go module, load the docker SDK and create the main.go file\ngo mod init go get github.com/docker/docker/client Let\u0026rsquo;s create a PullImage method in which we will create a docker client using the client.NewClientWithOpts method, as arguments to this method we will pass client.FromEnv which will take the docker configuration from environment variables and client.WithAPIVersionNegotiation() which will automatically determine the available version docker API. Now with the help of the client and the method ImagePull you can download the desired docker image. In the ImagePull method you need to pass the context, the name of the docker image that can be downloaded, and types.ImagePullOptions{}. In ImagePullOptions, you can pass authorization data if you need to download the docker image. At the very end, call io.Copy(os.Stdout, out) to display the pull command result. In the main function, call PullImage to load the docker image.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;io\u0026#34; \u0026#34;os\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/docker/docker/api/types\u0026#34; \u0026#34;github.com/docker/docker/client\u0026#34; ) func PullImage(imageName string) error { ctx := context.Background() dockerClient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation()) if err != nil { return err } out, err := dockerClient.ImagePull(ctx, imageName, types.ImagePullOptions{}) if err != nil { return err } defer out.Close() io.Copy(os.Stdout, out) return nil } func main() { err := PullImage(\u0026#34;debian\u0026#34;) if err != nil { log.Fatalln(err) } } Running we will see the result\n{\u0026#34;status\u0026#34;:\u0026#34;Pulling from library/debian\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;latest\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;Digest: sha256:b42494c466d101bf06038e959e2e5acd227e1251987e79528e7d8b1f4040deaf\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;Status: Image is up to date for debian:latest\u0026#34;} Docker list Now let\u0026rsquo;s write a method for displaying the docker images list. To do this, create the method ListAllImages. As in the previous example, create a docker client. Next, you need a filter that can be used to display the docker image according to some criteria. Since I want to display all docker images, the filter will remain empty. Now we will call a method ImageList in which it is necessary to pass a context and structure ImageListOptions which has two fields All and Filters. In Filters we will pass the previously created filter, and in All we will specify false. If you set All to true, then all intermediate docker images will also be displayed. You can now print information about all containers in the loop.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;github.com/docker/docker/api/types\u0026#34; \u0026#34;github.com/docker/docker/api/types/filters\u0026#34; \u0026#34;github.com/docker/docker/client\u0026#34; ) func ListAllImages() error { ctx := context.Background() dockerClient, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation()) if err != nil { return err } filter := filters.NewArgs() images, err := dockerClient.ImageList(ctx, types.ImageListOptions{ All: false, Filters: filter, }) if err != nil { return err } for _, i := range images { fmt.Println(i) } return nil } func main() { err := ListAllImages() if err != nil { log.Fatalln(err) } } As a result, we will see:\n{-1 1623251330 sha256:c0a98e816f894adc33b1c39d87925a5979391bde533b38d6e6eb41326b904e65 map[maintainer:Sebastian Ramirez \u0026lt;tiangolo@gmail.com\u0026gt;] [tiangolo/uvicorn-gunicorn-fastapi@sha256:f444216a887d0d4360edd5b69c78c1de530d0083a797176f9abf20f3f341b196] [tiangolo/uvicorn-gunicorn-fastapi:python3.8] -1 1014169179 1014169179} {-1 1611933531 sha256:714c659c9f6f4a167fe294f9330387581007f1e86429d3460d1fc7bd7ee52ff7 map[] [lambci/lambda@sha256:22e9fbb4df8270efcebed96905edf0244dd595a8d6250f24200ad558c0a201bc] [lambci/lambda:build-python3.8] -1 1962950336 1962950336} {-1 1568356668 sha256:db427f282c433e8470846e3daddd3f666cfdbf85235ff66b3b6814d3840648f4 map[] [selenoid/chrome@sha256:7be4763d02d4cd76c5081a4793485f9263ce4c25e56194b82567ecf597425f83] [selenoid/chrome:77.0] -1 891512369 891512369} You can also display individual fields because the previous result is not easy to read. To do this in the loop, this call fmt.Println(i) can be replaced by fmt.Println(i.RepoTags, i.Size) and we get the following result:\n[tiangolo/uvicorn-gunicorn-fastapi:python3.8] 1014169179 [lambci/lambda:build-python3.8] 1962950336 [selenoid/chrome:77.0] 891512369 ","permalink":"https://mpostument.com/posts/programming/golang/docker-sdk-part-one/","summary":"Hi there!\nThis is the first part of the article about docker SDK. In the current article, we will see how with the help of golang it is possible to make pull and list docker images.\nDocker pull As usual, initialize the go module, load the docker SDK and create the main.go file\ngo mod init go get github.com/docker/docker/client Let\u0026rsquo;s create a PullImage method in which we will create a docker client using the client.","title":"Docker Sdk Part One"},{"content":"Hi there!\nToday we will look at how to use mongo with golang. And let\u0026rsquo;s start with the launch of mongo. I will use docker-compose. To do this, create a file named docker-compose.yml. In the file, you need to specify which version of the mongo to use. I\u0026rsquo;m not specifying the version which means that the latest version of mongo will be used. You must also specify a username and password and the port that will be open for access.\nversion: \u0026#39;3.1\u0026#39; services: mongo: image: mongo restart: always environment: MONGO_INITDB_ROOT_USERNAME: root MONGO_INITDB_ROOT_PASSWORD: password ports: - 27017:27017 Once the file is saved, run the docker-compose up -d (Docker and docker-compose must be installed) and the mongo container will be launched.\nThe mongo configuration is complete and we can get started with the code. Initialize the go module and download the mongo driver\ngo mod init go get go.mongodb.org/mongo-driver/mongo Let\u0026rsquo;s create main.go and start coding. We will start with the mongoInsert method which will write data to the database. The method calls mongo.Connect in which the context and parameters are passed. As parameters, I pass a link to mongo in the format mongodb://username:password@mongoAddress:mongoPort. I also create a timeout context to automatically terminate queries if they last longer than necessary. To make a query, you need to create a structure that will have fields that need to be written to the database. I have a structure User with two fields Name and Surname of type string. An annotation should be added to each of the fields, indicating how the field will be named in the database. The method for writing data is InsertOne and it returns a mongo record ID. Which will be printed after a successful insert request. The database and collection will be created automatically after the insert operation.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/mongo\u0026#34; \u0026#34;go.mongodb.org/mongo-driver/mongo/options\u0026#34; ) type User struct { Name string `bson:\u0026#34;name\u0026#34;` Surname string `bson:\u0026#34;surname\u0026#34;` } func main() { err := mongoInsert() if err != nil { log.Fatalln(err) } } func mongoInsert() error { ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() client, err := mongo.Connect(ctx, options.Client().ApplyURI(\u0026#34;mongodb://root:password@127.0.0.1:27017\u0026#34;)) if err != nil { return err } defer client.Disconnect(ctx) collection := client.Database(\u0026#34;mpostument\u0026#34;).Collection(\u0026#34;users\u0026#34;) user := User{ Name: \u0026#34;Maksym\u0026#34;, Surname: \u0026#34;Postument\u0026#34;, } res, err := collection.InsertOne(ctx, user) if err != nil { return err } id := res.InsertedID log.Printf(\u0026#34;Insert ID, %s\u0026#34;, id) return nil } You can now connect to the database and see if there are any entries. To do this, connect to the docker container\ndocker ps # Copy CONTAINER ID and execute docker exec -it a61578b802e7 bash Now let\u0026rsquo;s call the console client\nmongo -u root -p password Let\u0026rsquo;s connect to the created database\n\u0026gt; use mpostument switched to db mpostument And get data from the database\n\u0026gt; db.users.findOne() { \u0026#34;_id\u0026#34; : ObjectId(\u0026#34;6231a26576e5effdd561a9d8\u0026#34;), \u0026#34;name\u0026#34; : \u0026#34;Maksym\u0026#34;, \u0026#34;surname\u0026#34; : \u0026#34;Postument\u0026#34; } Now let\u0026rsquo;s see how you can read the data from mongo with golang. The beginning of the method is the same, a client is created and connected to the collection. Next, create a variable user with type User(a structure which we created) and call the method FindOne in which I pass the context and filter to search. In my version, we are looking for an object in which the name field is equal to Maksym. And decode results in to a variable user\nfunc main() { err = mongoGet() if err != nil { log.Fatalln(err) } } func mongoGet() error { ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() client, err := mongo.Connect(ctx, options.Client().ApplyURI(\u0026#34;mongodb://root:password@127.0.0.1:27017\u0026#34;)) if err != nil { return err } defer client.Disconnect(ctx) collection := client.Database(\u0026#34;mpostument\u0026#34;).Collection(\u0026#34;users\u0026#34;) var user User res := collection.FindOne(ctx, bson.D{{\u0026#34;name\u0026#34;, \u0026#34;Maksym\u0026#34;}}) res.Decode(\u0026amp;user) fmt.Println(user) return nil } You can also use the Find method instead of FindOne, which will return many objects.\n","permalink":"https://mpostument.com/posts/programming/golang/using-mongo-with-go/","summary":"Hi there!\nToday we will look at how to use mongo with golang. And let\u0026rsquo;s start with the launch of mongo. I will use docker-compose. To do this, create a file named docker-compose.yml. In the file, you need to specify which version of the mongo to use. I\u0026rsquo;m not specifying the version which means that the latest version of mongo will be used. You must also specify a username and password and the port that will be open for access.","title":"Using Mongo With Go"},{"content":"Hi there!\nI recently embarked on a journey to automate the creation of AWS Synthetics using Terraform. Along the way, I encountered a few challenges that I managed to overcome. In this post, I will share my experiences and the solutions I found.\nlocals { common_tags = { cost-center = \u0026#34;observability\u0026#34; label = \u0026#34;canary\u0026#34; environment = \u0026#34;staging\u0026#34; } } data \u0026#34;archive_file\u0026#34; \u0026#34;login\u0026#34; { type = \u0026#34;zip\u0026#34; source_dir = \u0026#34;${path.module}/functions/login/\u0026#34; output_path = \u0026#34;${path.module}/functions/login.zip\u0026#34; } resource \u0026#34;aws_synthetics_canary\u0026#34; \u0026#34;login\u0026#34; { name = \u0026#34;login_test\u0026#34; artifact_s3_location = \u0026#34;s3://my_canary_bucket/\u0026#34; execution_role_arn = \u0026#34;role_arn\u0026#34; handler = \u0026#34;index.handler\u0026#34; zip_file = data.archive_file.login.output_path runtime_version = \u0026#34;syn-nodejs-puppeteer-3.4\u0026#34; start_canary = true schedule { expression = \u0026#34;rate(5 minutes)\u0026#34; } run_config { timeout_in_seconds = 300 } tags = merge( local.common_tags ) } Problem 1: Tagging the Automatically Created Lambda The first obstacle I faced was the lack of tags on the Lambda function automatically created by the canary. To address this, I utilized the null_resource in Terraform. The Lambda ARN can be obtained using aws_synthetics_canary.login.engine_arn. However, the ARN includes the lambda version, which is not required for tagging purposes. To remove the version from the Lambda ARN, I added the following code to locals:\nlambda_regex = \u0026#34;arn:aws:lambda:[a-z]{2}-[a-z]+-\\\\d{1}:\\\\d{12}:function:[a-zA-Z0-9-_]+\u0026#34; login_lambda_name = regex(local.lambda_regex, aws_synthetics_canary.login.engine_arn) By using the updated ARN with null_resource, I was able to tag the Lambda. The null_resource runs whenever there are changes in the common_tags folder or when the aws_synthetics_canary ID changes. In the provisioner \u0026quot;local-exec\u0026quot; block, I utilized the aws lambda tag-resource command to tag the Lambda. However, it\u0026rsquo;s important to note that aws lambda tag-resource supports tags in the format KeyName1=string,KeyName2=string, while the tags in my code were in a map. To convert them to a string, I added the following code to locals:\ntags = trimsuffix(join(\u0026#34; \u0026#34;, formatlist(\u0026#34;%s=%s,\u0026#34;, keys(local.common_tags), values(local.common_tags))), \u0026#34;,\u0026#34;) After creating the aws_synthetics_canary, the Lambda will be tagged using the null_resource.\nresource \u0026#34;null_resource\u0026#34; \u0026#34;tag_login_backend_lambda\u0026#34; { triggers = { tags = local.tags canary = aws_synthetics_canary.login.id } provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;aws lambda tag-resource --resource ${local.login_lambda_name} --tags \u0026#39;${local.tags}\u0026#39;\u0026#34; environment = { AWS_PROFILE = \u0026#34;production } } depends_on = [aws_synthetics_canary.login] } Problem 2: Environment Variables Support Another challenge I encountered was the lack of support for environment variables in the aws_synthetics_canary resource. To overcome this limitation, I used the null_resource once again. I added the environment variables to locals, creating a map with the desired variables. In my case, there was only one variable, app_url:\ncanary_variables = { \u0026#34;app_url\u0026#34; = \u0026#34;https://mpostument.com\u0026#34; } Triggers look similar to the previous example, only canary_variables has a type map and needs to be converted to string because triggers only support the string type. The aws synthetics update-canary command needs to include additional parameters besides environment variables. If they are not provided, the aws cli will set them to default. In my case, I am using values retrieved from the Terraform resource aws_synthetics_canary.login. This guarantees that those parameters will be the same for aws_synthetics_canary.login and null_resource.\nresource \u0026#34;null_resource\u0026#34; \u0026#34;add_environment_variables_login_canary\u0026#34; { triggers = { canary_variables = join(\u0026#34;,\u0026#34;, [for key, value in local.canary_variables : \u0026#34;${key}=${value}\u0026#34;]) canary = aws_synthetics_canary.login.id } provisioner \u0026#34;local-exec\u0026#34; { command = \u0026#34;aws synthetics update-canary --name ${aws_synthetics_canary.login.name} --run-config \u0026#39;${jsonencode({ TimeoutInSeconds : aws_synthetics_canary.create.run_config[0].timeout_in_seconds, MemoryInMB : aws_synthetics_canary.create.run_config[0].memory_in_mb, ActiveTracing : aws_synthetics_canary.create.run_config[0].active_tracing, EnvironmentVariables : var.canary_variables })}\u0026#39;\u0026#34; environment = { AWS_PROFILE = var.aws_profile_name } } depends_on = [aws_synthetics_canary.create] } Problem 3: Automatic Canary Code Update One of the challenges I encountered was the lack of automatic updates for the canary code. Currently, the canary code does not update automatically, and to trigger an update, the archive name must be different each time. To achieve this, I utilized the filemd5 function, which calculates the hash of the file. By incorporating this function into the archive_file configuration, the canary code is updated whenever the file\u0026rsquo;s hash changes.\nlocals { login_function_location = \u0026#34;${path.module}/functions/login/nodejs/node_modules/index.js\u0026#34; } data \u0026#34;archive_file\u0026#34; \u0026#34;login\u0026#34; { type = \u0026#34;zip\u0026#34; source_dir = \u0026#34;${path.module}/functions/login/\u0026#34; output_path = \u0026#34;${path.module}/functions/login-${filemd5(local.login_function_location)}.zip\u0026#34; } By incorporating the filemd5 function into the configuration of the archive_file, the hash of the file will be calculated each time. If the hash has changed, a new archive will be created, ensuring that the canary code is updated accordingly.\nProblem 4: Cleanup Issue with terraform destroy One significant issue I encountered is related to the cleanup process when executing terraform destroy. Unfortunately, the automatically created lambda function, aws_synthetics_canary, is not removed as expected. I am currently working on finding a solution to address this problem and ensure proper cleanup of resources when destroying the infrastructure.\nUpdate: Environment Variables Support An exciting update regarding the AWS provider! The latest version, 4.5.0, has introduced support for environment_variables. This means you can now leverage this feature by specifying a map of the required environment variables within the run_config block. It provides a more streamlined approach to managing environment variables for your AWS Synthetics canary.\nI hope these improvements provide better clarity and understanding. If you have any additional questions or suggestions, please let me know.\nlocals { canary_variables = { \u0026#34;app_url\u0026#34; = \u0026#34;https://mpostument.com\u0026#34; } login_function_location = \u0026#34;${path.module}/functions/login/nodejs/node_modules/index.js\u0026#34; } data \u0026#34;archive_file\u0026#34; \u0026#34;login\u0026#34; { type = \u0026#34;zip\u0026#34; source_dir = \u0026#34;${path.module}/functions/login/\u0026#34; output_path = \u0026#34;${path.module}/functions/login-${filemd5(local.login_function_location)}.zip\u0026#34; resource \u0026#34;aws_synthetics_canary\u0026#34; \u0026#34;login\u0026#34; { name = \u0026#34;login_test\u0026#34; artifact_s3_location = \u0026#34;s3://my_canary_bucket/\u0026#34; execution_role_arn = \u0026#34;role_arn\u0026#34; handler = \u0026#34;index.handler\u0026#34; zip_file = data.archive_file.login.output_path runtime_version = \u0026#34;syn-nodejs-puppeteer-3.4\u0026#34; start_canary = true schedule { expression = \u0026#34;rate(5 minutes)\u0026#34; } run_config { timeout_in_seconds = 300 environment_variables = local.canary_variables } tags = merge( local.common_tags ) } ","permalink":"https://mpostument.com/posts/programming/terraform/aws-synthetics-with-terraform/","summary":"Hi there!\nI recently embarked on a journey to automate the creation of AWS Synthetics using Terraform. Along the way, I encountered a few challenges that I managed to overcome. In this post, I will share my experiences and the solutions I found.\nlocals { common_tags = { cost-center = \u0026#34;observability\u0026#34; label = \u0026#34;canary\u0026#34; environment = \u0026#34;staging\u0026#34; } } data \u0026#34;archive_file\u0026#34; \u0026#34;login\u0026#34; { type = \u0026#34;zip\u0026#34; source_dir = \u0026#34;${path.module}/functions/login/\u0026#34; output_path = \u0026#34;${path.","title":"AWS Synthetics With Terraform: Overcoming Challenges and Enhancing Automation"},{"content":"Hello!\nToday we will look at how we can use golang to connect to the Postgres database and execute queries. To work with the database we will use github.com/jackc/pgx.\nLet\u0026rsquo;s start with initializing the module and installing the driver for the database.\ngo mod init go get github.com/jackc/pgx/v4 After that pgx will be available for use in code.\nFor the database, I am using docker, but it can be another type of installation. To do this, create a file docker-compose.yml with content:\nversion: \u0026#39;3\u0026#39; services: Postgres: image: Postgres tty: true restart: always ports: - \u0026#34;5432:5432\u0026#34; environment: - POSTGRES_PASSWORD=postgres And execute the command docker-compose up -d. Docker-compose and docker must be installed. The database is ready to work, and now we can start writing code.\nLet\u0026rsquo;s start with the connection to the database. First, we create a configuration cfg which will have the address, port, user, password, and database to which we connect. Since my database works locally, I specify localhost as the host. When the configuration is ready, call pgx.Connect (cfg) to create a connection. After finishing work with the database connection it is necessary to close therefore we will add defer conn.Close() and we check for errors. Also to check that the database is responding we will run conn.Ping(context.Background())\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/jackc/pgx\u0026#34; ) func main() { cfg := pgx.ConnConfig{ Host: \u0026#34;localhost\u0026#34;, Port: 5432, User: \u0026#34;postgres\u0026#34;, Password: \u0026#34;postgres\u0026#34;, Database: \u0026#34;postgres\u0026#34;, } conn, err := pgx.Connect(cfg) if err != nil { fmt.Fprintf(os.Stderr, \u0026#34;Unable to connect to database: %v\\n\u0026#34;, err) log.Fatalf(\u0026#34;Connect to database failed: %v\\n\u0026#34;, err) } defer conn.Close() err = conn.Ping(context.Background()) if err != nil { log.Fatalf(\u0026#34;Ping Failed: %v\\n\u0026#34;, err) } } We can start making inquiries. To begin, create a new table and write data to it. The Exec method is used to call requests that do not return anything.\n_, err = conn.Exec(` CREATE TABLE IF NOT EXISTS users( id SERIAL PRIMARY KEY, name TEXT, email TEXT UNIQUE NOT NULL ); `) if err != nil { log.Fatalln(err) } name := \u0026#34;TestUser1\u0026#34; email := \u0026#34;testuser@gmail.com\u0026#34; _, err = conn.Exec(` INSERT INTO users(name, email) VALUES ($1, $2); `, name, email) if err != nil { log.Fatalln(err) } The data has been created and we can try to read it. If one record is returned from the database, you can use the QueryRow method, if many, then Query. Also, in order to read the data you need the structure, we will have the structure name User which will have the same parameters as the fields in the table.\ntype User struct { Id int Name string Email string } And the request itself to get the data\nuser := User{} err = conn.QueryRow(\u0026#34;SELECT * FROM users WHERE name=$1;\u0026#34;, name).Scan(\u0026amp;user.Id, \u0026amp;user.Name, \u0026amp;user.Email) if err != nil { fmt.Println(err) return } fmt.Println(user) Pgx can also be used as a driver for the built-in sql package\n","permalink":"https://mpostument.com/posts/programming/golang/using-postgres-with-go/","summary":"Hello!\nToday we will look at how we can use golang to connect to the Postgres database and execute queries. To work with the database we will use github.com/jackc/pgx.\nLet\u0026rsquo;s start with initializing the module and installing the driver for the database.\ngo mod init go get github.com/jackc/pgx/v4 After that pgx will be available for use in code.\nFor the database, I am using docker, but it can be another type of installation.","title":"Using Postgres With Golang"},{"content":"Hello!\nIn this task, the input is given a sorted list, and the goal is to create a new list that will contain the squared sorted numbers from the first list. The simplest solution would be to loop through the first list, square all the numbers, add to our new list, and sort. But this solution will not be optimal.\nLet\u0026rsquo;s consider a more optimal solution that will run linearly.\nFor example, take the following input list - [-9, -1, 2, 3, 5, 6]\nTo solve this problem in linear time, we use two pointers that will indicate the beginning and end of the list. We also need a variable that will indicate where in the new list we need to insert a number. Initially, it will be len (array) - 1 because we will find the square of the largest number and add it to the end of the list.\nfunc SortedSquaredArray(array []int) []int { result := make([]int, len(array)) leftIdx := 0 rightIdx := len(array) - 1 pos := len(array) - 1 } As a helper which will calculate absolute value. The math package has such a function. But it only accepts float64. So let\u0026rsquo;s create our own\nfunc Abs(x int) int { if x \u0026lt; 0 { return -x } return x } Now when everything is ready, we need to create a loop that will work until the left pointer is equal to the right. In the loop, we will check whether the list item with the index rightIdx is larger than leftIdx. If the condition is met, we count the square of the number under the index rightIdx and write it to the list result [pos] = array [rightIdx] * array [rightIdx], rightIdx we need to reduce by one to go to the next element in list on the from the right side. If the condition is not met, it means that the number from the list under leftIdx is larger, so we need to write it to the list result [pos] = array [leftIdx] * array [leftIdx] and we need to increase leftIdx by one, to go to the next list item on the left side. And at the end value of pos must also be reduced by one to write the new number in the correct place in the list.\nfunc SortedSquaredArray(array []int) []int { result := make([]int, len(array)) leftIdx := 0 rightIdx := len(array) - 1 pos := len(array) - 1 for leftIdx \u0026lt;= rightIdx { if Abs(array[leftIdx]) \u0026lt; Abs(array[rightIdx]) { result[pos] = array[rightIdx] * array[rightIdx] rightIdx -= 1 } else { result[pos] = array[leftIdx] * array[leftIdx] leftIdx += 1 } pos -= 1 } return result } If we call this method, we get the result [1, 4, 9, 25, 36, 81]\n","permalink":"https://mpostument.com/posts/programming/algorithms/sorted-squared-array/","summary":"Hello!\nIn this task, the input is given a sorted list, and the goal is to create a new list that will contain the squared sorted numbers from the first list. The simplest solution would be to loop through the first list, square all the numbers, add to our new list, and sort. But this solution will not be optimal.\nLet\u0026rsquo;s consider a more optimal solution that will run linearly.","title":"Sorted Squared Array"},{"content":"Hello!\nToday we will learn what is WaitGroup in golang and we will look how to apply them on an example of the program for files search.\nWaitGroup is waiting for the completion of go routines. The main go routine calls the Add method, and then each go routine calls the Done method when the routine ends.\nLet\u0026rsquo;s start with the findFile method. In this method we read all files and directories in the root folder. We loop through them and if the name of the file what we are looking for matches the name of the file from the loop then we add it to the list of matches. If it is a directory, then recursively call a search in it\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;strings\u0026#34; ) var ( matches []string ) func findFile(root string, filename string) { fmt.Println(\u0026#34;Searching in\u0026#34;, root) files, _ := ioutil.ReadDir(root) for _, file := range files { if strings.Contains(file.Name(), filename) { matches = append(matches, filepath.Join(root, file.Name())) } if file.IsDir() { findFile(filepath.Join(root, file.Name()), filename) } } } func main() { findFile(\u0026#34;/var/\u0026#34;, \u0026#34;secret.txt\u0026#34;) for _, file := range matches { fmt.Println(\u0026#34;Matched\u0026#34;, file) } } The search works, but it takes a long time. You can speed it up with go routines. To do this, add the word go before calling the findFile method and before the recursive call. But in this case, the search will not work because the script will exit immediately, because there is nothing to wait for the go routines. To fix this WaitGroup can be used. The script will only complete when all go routines call the Done method.\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;path/filepath\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;sync\u0026#34; ) var ( matches []string waitGroup = sync.WaitGroup{} lock = sync.Mutex{} ) func findFile(root string, filename string) { fmt.Println(\u0026#34;Searching in\u0026#34;, root) files, _ := ioutil.ReadDir(root) for _, file := range files { if strings.Contains(file.Name(), filename) { lock.Lock() matches = append(matches, filepath.Join(root, file.Name())) lock.Unlock() } if file.IsDir() { waitGroup.Add(1) go findFile(filepath.Join(root, file.Name()), filename) } } waitGroup.Done() } func main() { waitGroup.Add(1) go findFile(\u0026#34;/var/\u0026#34;, \u0026#34;secret.txt\u0026#34;) waitGroup.Wait() for _, file := range matches { fmt.Println(\u0026#34;Matched\u0026#34;, file) } } A few things have been added here. In the findFile method, lock was added before adding file names to list. To ensure that routines do not overwrite data. Another thing that has been added is waitGroup. Before the each go routine call, the waitGroup.Add(1) call is added, and after the go routine, waitGroup.Done() is called. And to the main method added waitGroup.Wait() which will wait while all go routines finish work. And then a list of all files will be displayed.\n","permalink":"https://mpostument.com/posts/programming/golang/wait-groups/","summary":"Hello!\nToday we will learn what is WaitGroup in golang and we will look how to apply them on an example of the program for files search.\nWaitGroup is waiting for the completion of go routines. The main go routine calls the Add method, and then each go routine calls the Done method when the routine ends.\nLet\u0026rsquo;s start with the findFile method. In this method we read all files and directories in the root folder.","title":"WaitGroup"},{"content":"Hello!\nToday we will learn what is RWMutex and how to use it. RWMutex is a read/write exclusion lock. A lock can have any number of readers or one writer.\nRWMutex has the following methods:\nfunc (rw *RWMutex) Lock() // Write Lock func (rw *RWMutex) RLock() // Read Lock func (rw *RWMutex) RUnlock() // Read Unlock func (rw *RWMutex) Unlock() // Write Unlock Now let\u0026rsquo;s see how it can be used. In the first example, let\u0026rsquo;s try read lock\npackage main import ( \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; \u0026#34;fmt\u0026#34; ) var ( m = sync.RWMutex{} ) func main() { go read(1) go read(2) time.Sleep(2 * time.Second) } func read(i int) { fmt.Println(\u0026#34;Acquiring lock\u0026#34;, i) m.RLock() fmt.Println(\u0026#34;Reading\u0026#34;, i) time.Sleep(1 * time.Second) m.RUnlock() fmt.Println(\u0026#34;Released lock\u0026#34;, i) } When the script is executed, in the results output, we can see that several go routines were able to perform a read operation at the same time.\nAcquiring lock 1 Reading 1 Acquiring lock 2 Reading 2 Released lock 1 Released lock 2 Now let\u0026rsquo;s see how Write Lock works\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; \u0026#34;time\u0026#34; ) var ( m = sync.RWMutex{} ) func main() { go write(1) go read(2) go write(3) time.Sleep(5 * time.Second) } func read(i int) { fmt.Println(\u0026#34;Acquiring read lock\u0026#34;, i) m.RLock() fmt.Println(\u0026#34;Reading\u0026#34;, i) time.Sleep(1 * time.Second) m.RUnlock() fmt.Println(\u0026#34;Released read lock\u0026#34;, i) } func write(i int) { fmt.Println(\u0026#34;Acquiring write lock\u0026#34;, i) m.Lock() fmt.Println(\u0026#34;Writing\u0026#34;, i) time.Sleep(1 * time.Second) m.Unlock() fmt.Println(\u0026#34;Released write lock\u0026#34;, i) } From script output, we can see that at first Write Lock was acquired, write operation completed, and lock released. Then Read Lock acquired, reading operation completed, lock released. And finally last write operation completed.\nAcquiring write lock 3 Writing 3 Acquiring write lock 1 Acquiring read lock 2 Reading 2 Released write lock 3 Released read lock 2 Writing 1 Released write lock 1 ","permalink":"https://mpostument.com/posts/programming/golang/rwlock/","summary":"Hello!\nToday we will learn what is RWMutex and how to use it. RWMutex is a read/write exclusion lock. A lock can have any number of readers or one writer.\nRWMutex has the following methods:\nfunc (rw *RWMutex) Lock() // Write Lock func (rw *RWMutex) RLock() // Read Lock func (rw *RWMutex) RUnlock() // Read Unlock func (rw *RWMutex) Unlock() // Write Unlock Now let\u0026rsquo;s see how it can be used.","title":"Read and Write lock in Go"},{"content":"Hello!\nToday we will consider the Depth-first Search algorithm. We have a tree and you need to go through all its elements from left to right. Take a tree for example\n1 - 6 / \\ 2 - 4 10 - 6 / \\ \\ 3 - 2 5 - 4 23 - 7 So there are 6 elements in this tree. We start from the root and go to the left branch to 4, this branch has two daughter branches, go to the left to 2, go back to 4 and go to the right to 5. Go back to the root and go to the right to 10 and the last element remains so let\u0026rsquo;s move on to it. As a result, we get 6, 4, 2, 5, 10, 23.\nAn example of an algorithm on golang\nimport \u0026#34;fmt\u0026#34; type Node struct { Value int Children []*Node } func main() { tree := Node{ Value: 5, Children: []*Node{{Value: 3, Children: []*Node{{Value: 6}}}, {Value: 7, Children: []*Node{{Value: 8}}}}, } fmt.Println(tree.DepthFirstSearch([]int{})) } func (n *Node) DepthFirstSearch(array []int) []int { array = append(array, n.Value) for _, child := range n.Children { array = child.DepthFirstSearch(array) } return array } ","permalink":"https://mpostument.com/posts/programming/algorithms/depth-first-search/","summary":"Hello!\nToday we will consider the Depth-first Search algorithm. We have a tree and you need to go through all its elements from left to right. Take a tree for example\n1 - 6 / \\ 2 - 4 10 - 6 / \\ \\ 3 - 2 5 - 4 23 - 7 So there are 6 elements in this tree. We start from the root and go to the left branch to 4, this branch has two daughter branches, go to the left to 2, go back to 4 and go to the right to 5.","title":"Depth First Search"},{"content":"Hello!\nToday we will look at simple algorithm that can find node depth in tree. As input to the script given tree and the depth of the tree must be returned. Node depth is the sum of the depths of all nodes.\nFor example, take a tree\n6 / \\ 4 10 / \\ \\ 2 5 23 / 3 The depth of the root is always 0, and the next elements will always be the depth of the previous element +1. Then we will have\n6 - 0 / \\ 1 - 4 10 - 1 / \\ \\ 2 5 23 / 3 6 - 0 / \\ 1 - 4 10 - 1 / \\ \\ 2 - 2 5 - 2 23 - 2 / 3 6 - 0 / \\ 1 - 4 10 - 1 / \\ \\ 2 - 2 5 - 2 23 - 2 / 3 - 1 Having the depth of all the elements and adding them we get the depth of the tree. 0 + 1 + 1 + 2 + 2 + 2 + 3 = 11. Thus the depth of the tree is 11.\nExample code on golang\npackage main import \u0026#34;fmt\u0026#34; type Tree struct { root *Node } type Node struct { key int left *Node right *Node } func main() { tree := Tree{ \u0026amp;Node{ key: 5, left: \u0026amp;Node{ key: 4, left: \u0026amp;Node{ key: 3, }, }, right: \u0026amp;Node{ key: 10, right: \u0026amp;Node{ key: 13, }, }, }, } fmt.Println(nodeDepth(tree.root, 0)) } func nodeDepth(node *Node, depth int) int { if node == nil { return 0 } return depth + nodeDepth(node.left, depth+1) + nodeDepth(node.right, depth+1) } ","permalink":"https://mpostument.com/posts/programming/algorithms/node-depths/","summary":"Hello!\nToday we will look at simple algorithm that can find node depth in tree. As input to the script given tree and the depth of the tree must be returned. Node depth is the sum of the depths of all nodes.\nFor example, take a tree\n6 / \\ 4 10 / \\ \\ 2 5 23 / 3 The depth of the root is always 0, and the next elements will always be the depth of the previous element +1.","title":"Node Depths"},{"content":"Hello!\nIn this problem, a tree is input and you need to find the sum of all the branches in the tree. The algorithm for solving this problem is simple. Let\u0026rsquo;s take a tree\n6 / \\ 4 10 / \\ \\ 2 5 23 We start from the root, the sum here is 6. Go further down the tree in the left branch it will be 6 + 4 = 10 and in the right 10 + 6 = 16.\n6 + 0 / \\ 6 + 4 10 + 16 / \\ \\ 2 5 23 We move further on a tree in the left part it turns out 10 + 2 = 12 and 10 + 5 = 115 and in the right 16 + 23 = 39\n6 + 0 / \\ 6 + 4 10 + 6 / \\ \\ 10 + 2 5 + 10 23 + 16 We go further down the tree but there is no more data and therefore the sum we get 12, 15 and 39\npackage main import ( \u0026#34;fmt\u0026#34; ) type Tree struct { root *Node } type Node struct { key int left *Node right *Node } func main() { tree := Tree{ \u0026amp;Node{ key: 5, left: \u0026amp;Node{ key: 4, }, right: \u0026amp;Node{ key: 10, }, }, } branchSum(tree) } func branchSum(tree Tree) { sum := []int{} branchSumHelper(tree.root, 0, \u0026amp;sum) fmt.Println(sum) } func branchSumHelper(node *Node, runningSum int, sum *[]int) *[]int { if node == nil { return sum } newRunningSum := runningSum + node.key if node.left == nil \u0026amp;\u0026amp; node.right == nil { *sum = append(*sum, newRunningSum) return sum } branchSumHelper(node.left, newRunningSum, sum) branchSumHelper(node.right, newRunningSum, sum) return sum } ","permalink":"https://mpostument.com/posts/programming/algorithms/branch-sum/","summary":"Hello!\nIn this problem, a tree is input and you need to find the sum of all the branches in the tree. The algorithm for solving this problem is simple. Let\u0026rsquo;s take a tree\n6 / \\ 4 10 / \\ \\ 2 5 23 We start from the root, the sum here is 6. Go further down the tree in the left branch it will be 6 + 4 = 10 and in the right 10 + 6 = 16.","title":"Branch Sum"},{"content":"Hello!\nToday we will look at how to find the nearest value in BST. The input is given a binary tree and a number, and you need to find the closest number to the given one. For example, we have a tree:\n6 / \\ 4 10 / \\ \\ 2 5 23 And the number 13. You need to find the number that is closest to 13 in the tree. To do this, we need another number (the closest value), which has a high value such as infinity.\nWe start with the root, in this case, it has a value of 6. From the goal (number 13) module subtract 6 and get 7. Repeat the same with a current nearest value which is equal to infinity, |infinity - 13| = infinity. We compare two numbers 7 and infinity, 7 is less so the new nearest number will be 6. Next, we use the attributes of the binary tree. Since the number 13 is greater than the root 6 we can completely discard the left side of the tree. And it will remain only\n10 \\ 23 And the nearest number is 6. Do the same with the next element of the tree which is equal to 10. By the module, I subtract from the 10 values ​​that we are looking for. | 10 - 13 | = 3. And compares it with the current nearest number (13-6 = 7). 3 is less than 7. Hence, the new nearest number is 10.\nThe last element of a tree remained. Again, I subtract the value from the tree with goal number | 23 - 10 | = 13. And compare with the current nearest number (13-10 = 3). 13 is more than 3, so the nearest number remains 10. There are no more elements in the tree and therefore the closest number to 13 is 10.\nAn example of a solution to golang\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;math\u0026#34; ) type Tree struct { root *Node } type Node struct { key int left *Node right *Node } func main() { tree := Tree{ \u0026amp;Node{ key: 5, left: \u0026amp;Node{ key: 4, }, right: \u0026amp;Node{ key: 10, }, }, } findClosestValue(tree, 3) } func findClosestValue(tree Tree, target int) { fmt.Println(findClosestValueHelper(tree.root, target, math.MaxInt32)) } func findClosestValueHelper(node *Node, target int, closest int) int { if node == nil { return closest } else { if math.Abs(float64(target)-float64(closest)) \u0026gt; math.Abs(float64(target)-float64(node.key)) { closest = node.key } if target \u0026lt; node.key { return findClosestValueHelper(node.left, target, closest) } else if target \u0026gt; node.key { return findClosestValueHelper(node.right, target, closest) } } return closest } ","permalink":"https://mpostument.com/posts/programming/algorithms/find-closest-value-in-bst/","summary":"Hello!\nToday we will look at how to find the nearest value in BST. The input is given a binary tree and a number, and you need to find the closest number to the given one. For example, we have a tree:\n6 / \\ 4 10 / \\ \\ 2 5 23 And the number 13. You need to find the number that is closest to 13 in the tree.","title":"Find Closest Value in BST"},{"content":"Hi!\nThe first input parameter will be a list with numbers, the second will also be a list with numbers. The essence of the problem is to find out whether the second list is a subsequence of the first.\nTo do this, we will need a pointer that will point to items from the second list. Let\u0026rsquo;s start moving on each element from the first list and check whether it is equal to the element from the second list indicated by the pointer.\nFor example, the first list 5, 3, 22, 25, 9, -5, 8, 7 and the second 3, 9, -5, 7. The pointer points to 3\n\u0026gt;3, 9, -5, 7 We begin to sort the numbers from the first list and compare them with the pointer. 3 is not equal to 5 and therefore we pass to the following element. 3 = 3 means we move the pointer to one to the right\n3,\u0026gt; 9, -5, 7 Compare with the next item in the first list 22! = 9 25! = 9 9 = 9 match again, and move the pointer to the right again\n3, 9,\u0026gt; -5, 7 Next, sort the elements of the first list -5 = -5 again move the pointer to the right and move on the first list 8! = 7 7 = 7\nThus we found the last element from the second list and therefore it is a subsequence of the first.\nAn example of a solution on go\npackage main import \u0026#34;fmt\u0026#34; func main() { sequence := []int{5, 3, 22, 25, 9, -5, 8, 7} subsequence := []int{3, 9, -5, 7} fmt.Println(validateSubsequence(sequence, subsequence)) } func validateSubsequence(sequence []int, subsequence []int) bool { subsequencePtr := 0 for _, num := range sequence { if num == subsequence[subsequencePtr] { subsequencePtr++ } } return subsequencePtr == len(subsequence) } ","permalink":"https://mpostument.com/posts/programming/algorithms/validate-subsequence/","summary":"Hi!\nThe first input parameter will be a list with numbers, the second will also be a list with numbers. The essence of the problem is to find out whether the second list is a subsequence of the first.\nTo do this, we will need a pointer that will point to items from the second list. Let\u0026rsquo;s start moving on each element from the first list and check whether it is equal to the element from the second list indicated by the pointer.","title":"Validate Subsequence"},{"content":"Hello!\nThe first input parameter will be a list of numbers, the second will be a number - the sum you need to find. The essence of the problem is to find which pair of numbers in the sum is equal to the second argument.\nThere are several possible solutions for solving this problem, we will consider the option with two pointers. To do this, we will need a sorted list and two pointers, one pointing to the first item in the list and the other to the last.\nFor example, we will take this list 8, -2, 11, 13, -1, 6, 4 and the sum which needs to be found 15. After sorting we get:\n\u0026gt;-2, -1, 4, 6, 8, 11, 13\u0026lt; One pointer points to -2 and the other to 13. We calculate the sum of these elements -2 + 13 = 11 which is less than the amount we are looking for (15). So the left pointer must be moved forward\n-2, \u0026gt;-1, 4, 6, 8, 11, 13\u0026lt; We look for the sum -1 + 13 = 12 and again it less than 15, therefore we move the left pointer on 1 position to the right\n-2, -1, \u0026gt;4, 6, 8, 11, 13\u0026lt; Again, count the sum of 4 + 13 = 17, which is more than 15 means the right pointer must be moved to the left\n-2, -1, \u0026gt;4, 6, 8, 11\u0026lt; , 13 We calculate the sum 4 + 11 = 15 which is equal to the sum we are looking for and therefore the result will be 4, 11\nAn example of solving the problem on go\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sort\u0026#34; ) func main() { fmt.Println(twoNumberSum([]int{8, -2, 11, 13, -1, 6, 4}, 15)) } func twoNumberSum(array []int, targetSum int) []int { sort.Ints(array) left := 0 right := len(array) - 1 for { if left \u0026gt;= right { break } currentSum := array[left] + array[right] if currentSum == targetSum { return []int{array[left], array[right]} } else if currentSum \u0026lt; targetSum { left++ } else { right-- } } return []int{} } ","permalink":"https://mpostument.com/posts/programming/algorithms/two-number-sum/","summary":"Hello!\nThe first input parameter will be a list of numbers, the second will be a number - the sum you need to find. The essence of the problem is to find which pair of numbers in the sum is equal to the second argument.\nThere are several possible solutions for solving this problem, we will consider the option with two pointers. To do this, we will need a sorted list and two pointers, one pointing to the first item in the list and the other to the last.","title":"Two Number Sum"},{"content":"Hello!\nToday I will show how to automatically generate GitHub Readme with the latest blog posts using go. In GitHub you need to create a repository named as your GitHub profile. My GitHub profile is called mpostument, so I\u0026rsquo;ll create a repository with the same name.\nI clone the repository and call go mod init in it, in order to initialize the go module which will generate readme. At the root of the repository you need to create main.go which will be executed.\nIn the beginning there will be imports and the structure in which posts will be stored.\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;text/template\u0026#34; \u0026#34;github.com/mmcdole/gofeed\u0026#34; ) type ReadmeData struct { Title string Link string } The gofeed module is needed to read the rss feed and generate a list of posts based on it. The code itself is quite simple, read the template using template.ParseFiles. After that create gofeed.NewParser() which will be used to get rss feed from my site. I loop through all posts from rss feed and add data about posts in a slice which I will transfer to a template and I will save in a file by means of template.Execute.\nfunc main() { template, err := template.ParseFiles(\u0026#34;README.tmpl\u0026#34;) if err != nil { log.Fatalln(err) } fp := gofeed.NewParser() feed, err := fp.ParseURL(\u0026#34;https://mpostument.com/index.xml\u0026#34;) if err != nil { log.Fatalln(err) } postList := []ReadmeData{} for index, post := range feed.Items { if index == 7 { break } readmeData := ReadmeData{ Title: post.Title, Link: post.Link, } postList = append(postList, readmeData) } f, err := os.Create(\u0026#34;README.MD\u0026#34;) if err != nil { log.Fatalln(err) } err = template.Execute(f, postList) if err != nil { log.Fatalln(err) } } The next step is to create a template README.tmpl with content:\n## 📝 My Latest Blog Posts {{range .}} - [{{.Title}}](https://mpostument.com{{.Link}}/) {{end}} If you run the code, it will create a file README.MD with a list of recent posts.\nYou can use GithubActions to generate a GitHub readme automatically. You need to create the .github/workflows folders at the root of the repository. And inside create a file generate_readme.yml. This file is a configuration of the steps that GitHubActions will call. The action will run every day at midnight. First, the repository is cloned, go is installed and all dependencies are loaded, the code is compiled, script started and Readme.md is generated. And if there are changes in the file, they will be pushed in the repository and will be available in the profile.\nname: Generate readme on: schedule: - cron: \u0026#34;0 0 * * *\u0026#34; workflow_dispatch: jobs: update-readme-with-blog: name: Update this repo\u0026#39;s README with latest blog posts runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v2 - name: Setup go uses: actions/setup-go@v2 with: go-version: \u0026#34;1.16\u0026#34; - name: Go tidy run: go mod tidy - name: Build project run: go build - name: Generate readme run: ./mpostument - name: Commit and push if changed run: |- git diff git config --global user.email \u0026#34;actions@users.noreply.github.com\u0026#34; git config --global user.name \u0026#34;README-bot\u0026#34; git add -A git commit -m \u0026#34;Updated readme content\u0026#34; || exit 0 git push ","permalink":"https://mpostument.com/posts/programming/golang/latest-post-in-github-readme/","summary":"Hello!\nToday I will show how to automatically generate GitHub Readme with the latest blog posts using go. In GitHub you need to create a repository named as your GitHub profile. My GitHub profile is called mpostument, so I\u0026rsquo;ll create a repository with the same name.\nI clone the repository and call go mod init in it, in order to initialize the go module which will generate readme. At the root of the repository you need to create main.","title":"Latest posts in GitHub readme"},{"content":"About project ebs-autoresize provides a command line tool to automatically resize AWS EBS based on used spaced. Can be used with cron for regular disk size check and resize.\nInstall Download the latest binary from releases\nDocs https://github.com/mpostument/ebs-autoresize/blob/master/README.MD\n","permalink":"https://mpostument.com/posts/projects/ebs-autoresize/","summary":"About project ebs-autoresize provides a command line tool to automatically resize AWS EBS based on used spaced. Can be used with cron for regular disk size check and resize.\nInstall Download the latest binary from releases\nDocs https://github.com/mpostument/ebs-autoresize/blob/master/README.MD","title":"ebs-autoresize"},{"content":"Hi there!\nToday we will make a script for automatic resizing of EBS volume using golang and AWS SDK version 2. To do this, I need to perform a few steps: get a list of volumes, filter those volumes in which the amount of free memory is less than the threshold, find the ebs id that corresponds to this volume. Resize by a specified percentage and increase the disk size on the file system.\nI\u0026rsquo;ll start with the structure in which all the information on the disk will be stored and specify all the necessary imports.\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;math\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os/exec\u0026#34; \u0026#34;regexp\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/config\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/ec2\u0026#34; \u0026#34;github.com/aws/aws-sdk-go-v2/service/ec2/types\u0026#34; \u0026#34;github.com/aws/smithy-go\u0026#34; \u0026#34;github.com/mvisonneau/go-ebsnvme/pkg/ebsnvme\u0026#34; \u0026#34;github.com/shirou/gopsutil/v3/disk\u0026#34; ) type DiskData struct { VolumeID string DeviceName string MountPoint string TotalUsed float64 TotalSpace uint64 FsType string VolumeSize int32 } } The first method will write all the necessary information about the disks in the DiskData structure. Gopsutil will be used to retrieve information from the system. If in a system the disk is displayed as nvme it needs to be brought to standard record (/dev/sda) which can be used to filter the necessary ebs volumes. But this alone will not be enough because disks on different servers can be mounted under the same name. To prevent modification of the wrong volume an instance id will be used as an additional condition in a filter. It can be obtained from the server metadata.\nFirst I get a list of all disks using disk.Partitions(false), also I create ec2client and get instanceID from metadata. These methods will be created later. Next, in cycle, i loop through all the disks and check if nvme is in the disk name. If yes with ebsnvme i get disk name in format /dev/sda. And save the result in ebsDevice. If not that I change a disk name from xvd to sd. This is the format required for AWS. Now I can query aws to get the volumeID. Using the Usage method from AWS SDK. I get disk usage and write it to the DiskData.\nfunc filterDisks() ([]DiskData, error) { parts, err := disk.Partitions(false) if err != nil { return nil, err } client, err := getEc2Client() if err != nil { return nil, err } instanceID, err := getInstanceID() if err != nil { return nil, err } diskData := []DiskData{} for _, p := range parts { var ebsDevice string if strings.Contains(p.Device, \u0026#34;nvme\u0026#34;) { volumeMapping, err := ebsnvme.ScanDevice(p.Device) if err != nil { return nil, err } ebsDevice = volumeMapping.Name } else { ebsDevice = strings.Replace(p.Device, \u0026#34;xvd\u0026#34;, \u0026#34;sd\u0026#34;, 1) } filter := \u0026amp;ec2.DescribeVolumesInput{Filters: []types.Filter{ { Name: aws.String(\u0026#34;attachment.device\u0026#34;), Values: []string{ ebsDevice, }, }, { Name: aws.String(\u0026#34;attachment.instance-id\u0026#34;), Values: []string{ instanceID, }, }, }, } volumeInfo, err := client.DescribeVolumes(context.Background(), filter) if err != nil { return nil, err } usage, _ := disk.Usage(p.Mountpoint) disk := DiskData{ VolumeID: *volumeInfo.Volumes[0].VolumeId, DeviceName: p.Device, MountPoint: p.Mountpoint, TotalUsed: usage.UsedPercent, TotalSpace: usage.Total, VolumeSize: volumeInfo.Volumes[0].Size, FsType: p.Fstype, } diskData = append(diskData, disk) } return diskData, nil } I need to create the methods used by filterDisks, starting from getInstanceID. This method queries the ec2 instance metadata and receives an InstanceID in response.\nfunc getInstanceID() (string, error) { cfg, err := config.LoadDefaultConfig(context.TODO()) if err != nil { return \u0026#34;\u0026#34;, err } client := imds.NewFromConfig(cfg) instanceID, err := client.GetInstanceIdentityDocument(context.TODO(), \u0026amp;imds.GetInstanceIdentityDocumentInput{}) if err != nil { return \u0026#34;\u0026#34;, err } return instanceID.InstanceID, nil } getEc2Client gets the region name from the metadata and returns the ec2 client.\nfunc getEc2Client() (*ec2.Client, error) { cfg, err := config.LoadDefaultConfig(context.TODO()) if err != nil { return nil, err } client := imds.NewFromConfig(cfg) region, err := client.GetRegion(context.TODO(), \u0026amp;imds.GetRegionInput{}) if err != nil { return nil, err } cfg.Region = region.Region return ec2.NewFromConfig(cfg), err } What is left is to calculate what amounts of gigabytes disk should be resized. This method converts the disk size obtained in filterDisks from bytes to gigabytes and determines new size of ebs volume.\nfunc findNewSize(oldSize uint64, increasePercent float64) int32 { gbSize := float64(oldSize) / math.Pow(1024, 3) newSize := ((gbSize * increasePercent) / 100) + gbSize return int32(newSize) } The resize will take place in three steps:\nIncrease the size of the ebs\nCall the grow part to increase the size of the particle if it exists.\nIncrease the size of the file system.\nI\u0026rsquo;ll start with the first step. In this method, I get the ec2 client using the getEc2Client method and make a ModifyVolume request to AWS. Here it is important to filter out certain errors. The first is VolumeModificationRateExceeded. AWS allows you to resize once every 6 hours, so I don\u0026rsquo;t want the script to fail if the limit is exceeded.\nThe next error is IncorrectModificationState. After resizing the disk, its status changes to Optimizing, in which case I also do not want to quit, and go to the next disk.\nfunc ebsResize(newSize int32, volumeID string) error { client, err := getEc2Client() if err != nil { return err } log.Println(\u0026#34;Starting resize of ebs volume\u0026#34;, volumeID) input := \u0026amp;ec2.ModifyVolumeInput{VolumeId: \u0026amp;volumeID, Size: newSize} if _, err := client.ModifyVolume(context.Background(), input); err != nil { var ae smithy.APIError if errors.As(err, \u0026amp;ae) { switch ae.ErrorCode() { case \u0026#34;VolumeModificationRateExceeded\u0026#34;: log.Println(\u0026#34;Ebs was already resized, wait for 6 hours before next resize\u0026#34;) return errVolumeRetryLater case \u0026#34;IncorrectModificationState\u0026#34;: log.Println(ae.ErrorMessage()) return errVolumeRetryLater } } else { return err } } err = waitForEbsResize(volumeID) if err != nil { return err } return nil } Once the resize is started, I need to wait for it to complete, so I will create the following method waitForEbsResize. The waitForEbsResize method uses the ec2 client to make a request of type DescribeVolumesModifications and checks whether the status of the disk is modifying. If yes, it waits for 15 seconds and runs the method recursively again.\nfunc waitForEbsResize(volumeID string) error { client, err := getEc2Client() if err != nil { return err } input := \u0026amp;ec2.DescribeVolumesModificationsInput{VolumeIds: []string{volumeID}} status, err := client.DescribeVolumesModifications(context.Background(), input) if status.VolumesModifications[0].ModificationState == \u0026#34;modifying\u0026#34; { log.Println(\u0026#34;Ebs modification in progress. Waiting for 15 second\u0026#34;) time.Sleep(15 * time.Second) if err := waitForEbsResize(volumeID); err != nil { return err } } return nil } Now I can proceed to step 2 growPartition.\nI need to determine if there is a partition, if there is no partition grow part doesn\u0026rsquo;t need to be executed. Depending on what type of disk I will determine whether there are partitions or no. Several checks are required. The first check if there are no numbers in the device name. If they are not present then it is not partition and it is not required to do grow part. Next check for nvme devices. If there is a symbol p in the name, then it is a partition. For example /dev/nvme1n1 - disk and /dev/nvme0n1p1 - partition. And the last case if there is an xvd in the name of the device, this check only to filter usual ebs devices with numbers in the name. And depending on each case the growpart is executed. For ebs - growpart /dev/xvdf 1 and for nvme - growpart /dev/nvme0n1 1.\nfunc growPartition(partition string) error { log.Println(\u0026#34;Starting growpart for\u0026#34;, partition) var cmd *exec.Cmd i := strings.LastIndex(partition, \u0026#34;p\u0026#34;) isLetter := regexp.MustCompile(`^/dev/+[a-zA-Z]+$`).MatchString if isLetter(partition) { log.Println(\u0026#34;Grow partition for\u0026#34;, partition, \u0026#34;not required\u0026#34;) } else if i != -1 { cmd = exec.Command(\u0026#34;growpart\u0026#34;, partition[:i], partition[i+1:]) return cmd.Run() } else if strings.Contains(partition, \u0026#34;xvd\u0026#34;) { re := regexp.MustCompile(`\\D+`) m := re.FindString(partition) cmd = exec.Command(\u0026#34;growpart\u0026#34;, m, partition[len(m):]) return cmd.Run() } return nil } The last step is to increase the size of the file system. How the resize will take place depends on the file system. If it is xfs then called xfs_growfs in other cases it is resize2fs.\nfunc fsResize(filesystem string, mountPoint string, partition string) error { log.Println(\u0026#34;Starting system volume resize for\u0026#34;, partition) var cmd *exec.Cmd if filesystem == \u0026#34;xfs\u0026#34; { cmd = exec.Command(\u0026#34;xfs_growfs\u0026#34;, \u0026#34;-d\u0026#34;, mountPoint) } else { cmd = exec.Command(\u0026#34;resize2fs\u0026#34;, partition) } return cmd.Run() } Now it remains to call all the methods in the correct order:\nGet a list of disks\nLoop in a cycle on all of disks. If the condition disk.TotalUsed \u0026lt; 70 is false then resize not required.\nCalculate the new disk size\nCall ebsResize\nCall growPartition\nCall fsResize\nfunc main() { disksInfo, err := filterDisks() if err != nil { log.Fatalln(err) } for _, disk := range disksInfo { if disk.TotalUsed \u0026lt; 70 { log.Println(\u0026#34;Resize for\u0026#34;, disk.DeviceName, \u0026#34;not required\u0026#34;) continue } log.Println(\u0026#34;Starting resize of\u0026#34;, disk.DeviceName) newSize := findNewSize(disk.TotalSpace, 30) if err := ebsResize(int32(newSize), disk.VolumeID); err == errVolumeRetryLater { continue } else if err != nil { log.Fatalln(err) } if err := growPartition(disk.DeviceName); err != nil { log.Fatalln(err) } if err := fsResize(disk.FsType, disk.MountPoint, disk.DeviceName); err != nil { log.Fatalln(err) } } } After a few seconds, the disk has a new size.\n","permalink":"https://mpostument.com/posts/programming/golang/ebs-auto-resize/","summary":"Hi there!\nToday we will make a script for automatic resizing of EBS volume using golang and AWS SDK version 2. To do this, I need to perform a few steps: get a list of volumes, filter those volumes in which the amount of free memory is less than the threshold, find the ebs id that corresponds to this volume. Resize by a specified percentage and increase the disk size on the file system.","title":"Ebs Auto Resize"},{"content":"Hello!\nToday I want to show how with the help of AWS Lambda and Golang you can get messages in slack when Autoscaling could not create an Ec2 server. Deploy I automate using serverless.\nLet\u0026rsquo;s start with the installation of node and serverless. I use nvm to manage node versions.\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.37.2/install.sh | bash nvm install v14.15.3 npm install -g serverless I will create a folder and initialize the go module in it.\nmkdir failed-asg cd failed-asg go mod init touch main.go In the beginning, there will be a package, imports, and main function. Main function will be responsible for executing the handler function.\npackage main import ( \u0026#34;bytes\u0026#34; \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/aws/aws-lambda-go/events\u0026#34; \u0026#34;github.com/aws/aws-lambda-go/lambda\u0026#34; ) func main() { lambda.Start(handleRequest) } The main function will be handleRequest. It will generate a message based on AutoScalingEvent and send a message to slack.\nfunc handleRequest(ctx context.Context, event events.AutoScalingEvent) (string, error) { url := \u0026#34;https://hooks.slack.com/services/token\u0026#34; var sb strings.Builder sb.WriteString(\u0026#34;:siren: *EC2 Instance Launch Unsuccessful* :siren:\\n\u0026#34;) sb.WriteString(\u0026#34;*Autoscaling Group Name*:\u0026#34;) sb.WriteString(event.Detail[\u0026#34;AutoScalingGroupName\u0026#34;].(string)) sb.WriteString(\u0026#34;\\n\u0026#34;) sb.WriteString(\u0026#34;*StatusMessage*:\u0026#34;) sb.WriteString(event.Detail[\u0026#34;StatusMessage\u0026#34;].(string)) sb.WriteString(\u0026#34;\\n\u0026#34;) payload := map[string]interface{}{ \u0026#34;username\u0026#34;: \u0026#34;ASGFailedEvents\u0026#34;, \u0026#34;channel\u0026#34;: \u0026#34;#test-db-alerts\u0026#34;, \u0026#34;text\u0026#34;: sb.String(), } _, err := SendSlackNotification(url, payload) if err != nil { log.Fatalln(\u0026#34;Not able to send slack message\u0026#34;, err) } return sb.String(), nil } Url needs to be replaced with the value of your Slack Webhook. Using strings.Builder I create a message to be sent, by adding to it the name AutoScalingGroup and an error message. Then I form payload in which it is necessary to specify the name of the user from whom message will be sent, the channel and the text of the message and SendSlackNotification will send the message.\nLet\u0026rsquo;s move on to creating the SendSlackNotification function.\nfunc SendSlackNotification(url string, data interface{}) (bodyString string, err error) { jsonString, _ := json.Marshal(data) req, err := http.NewRequest(\u0026#34;programming\u0026#34;, url, bytes.NewBuffer(jsonString)) req.Header.Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) client := \u0026amp;http.Client{} resp, err := client.Do(req) if err != nil { panic(err) } defer resp.Body.Close() body, _ := ioutil.ReadAll(resp.Body) bodyString = string(body) if resp.StatusCode != 200 { fmt.Println(\u0026#34;Response Status:\u0026#34;, resp.Status) // 200 OK fmt.Println(\u0026#34;Response Headers:\u0026#34;, resp.Header) fmt.Println(\u0026#34;Response Body:\u0026#34;, bodyString) return bodyString, errors.New(\u0026#34;can\u0026#39;t work with 42\u0026#34;) } return bodyString, nil } Here json is formed from the previously created payload and with the help of http the request made to Webhook Url. Then it is checked whether the request was successfully sent.\nLet\u0026rsquo;s go back to serverless for lambda deployment. At the root of the repository, I create a file serverless.yml with content\nservice: asg-failed-events provider: name: aws runtime: go1.x package: exclude: - ./** include: - ./bin/** functions: lambda-time: handler: bin/failed-asg-event-notification events: - cloudwatchEvent: event: source: - \u0026#34;aws.autoscaling\u0026#34; detail-type: - \u0026#34;EC2 Instance Launch Unsuccessful\u0026#34; This configuration describes the name of the lambda that will be created and lambda trigger, in this case, it is EC2 Instance Launch Unsuccessful event from aws.autoscaling passed to the lambda by cloudwatchEvent.\nNow you need to install all the dependencies, compile the code and deploy.\ngo mod tidy OOS=linux GOARCH=amd64 go build -o bin/failed-asg-event-notification . sls deploy After that, if there is an event of the type EC2 Instance Launch Unsuccessful from aws.autoscaling the corresponding message will be sent to the slack.\n","permalink":"https://mpostument.com/posts/programming/golang/failed-asg-event-notification/","summary":"Hello!\nToday I want to show how with the help of AWS Lambda and Golang you can get messages in slack when Autoscaling could not create an Ec2 server. Deploy I automate using serverless.\nLet\u0026rsquo;s start with the installation of node and serverless. I use nvm to manage node versions.\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.37.2/install.sh | bash nvm install v14.15.3 npm install -g serverless I will create a folder and initialize the go module in it.","title":"Failed Asg Event Notification"},{"content":"Hello!\nIn the previous post I showed how you can export dashboards from Grafana. Now let\u0026rsquo;s see how they can be imported into Grafana.\nAs always at the beginning there will be imports and main function\npackage grafana import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/grafana-tools/sdk\u0026#34; ) func main() { } I will add some parameters:\nvar ( filesInDir []os.FileInfo rawBoard []byte err error grafanaURL string apiKey string directory string ) filesInDir will store all dashboards from the file system. And rawBoard will contain the content of the file. directory directory in which dashboards will be read from. grafanaURL - Grafana url with port in the format http://127.0.0.1: 3030 and apiKey for authorization in Grafana. You need to enter your value in grafanaUR, apiKey and directory.\nI am creating a Grafana client with which I will make requests to Grafana Api\nctx := context.Background() c := sdk.NewClient(grafanaURL, apiKey, sdk.DefaultHTTPClient) What left is to read the files and export them.\nfilesInDir, err = ioutil.ReadDir(directory) if err != nil { log.Fatal(err) } Now I loop through all files. Dashboards should be in json, if there other files except for json i need to skip them. If the file is json I write its content in rawBoard. And I am unmarshal this file into structure sdk.Board. I also create an object of type sdk.SetDashboardParams in which I specify in which folder to import the dashboard and whether to overwrite if such dashboard already exists. When all the parameters are ready, I call the SetDashboard method, which will export.\nfor _, file := range filesInDir { if strings.HasSuffix(file.Name(), \u0026#34;.json\u0026#34;) { if rawBoard, err = ioutil.ReadFile(fmt.Sprintf(\u0026#34;%s/%s\u0026#34;, directory, file.Name())); err != nil { log.Println(err) continue } var board sdk.Board if err = json.Unmarshal(rawBoard, \u0026amp;board); err != nil { log.Println(err) continue } params := sdk.SetDashboardParams{ FolderID: sdk.DefaultFolderId, Overwrite: true, } _, err := c.SetDashboard(ctx, board, params) if err != nil { log.Printf(\u0026#34;error on importing dashboard %s\u0026#34;, board.Title) continue } } } ","permalink":"https://mpostument.com/posts/programming/golang/import-grafana-dashboards-with-go/","summary":"Hello!\nIn the previous post I showed how you can export dashboards from Grafana. Now let\u0026rsquo;s see how they can be imported into Grafana.\nAs always at the beginning there will be imports and main function\npackage grafana import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/grafana-tools/sdk\u0026#34; ) func main() { } I will add some parameters:\nvar ( filesInDir []os.FileInfo rawBoard []byte err error grafanaURL string apiKey string directory string ) filesInDir will store all dashboards from the file system.","title":"Import Grafana Dashboards With Go"},{"content":"Hello!\nLet\u0026rsquo;s check how you can use go to export dashboards from Grafana. For interaction with Grafana I use sdk. Sdk can be installed using go get github.com/grafana-tools/sdk.\nLet\u0026rsquo;s start writing code. At the beginning of the file will be the package, import and main function\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/grafana-tools/sdk\u0026#34; ) func main() { } ) I will add some parameters:\nvar ( boardLinks []sdk.FoundBoard rawBoard sdk.Board meta sdk.BoardProperties err error grafanaURL string apiKey string directory string ) Links to dashboards in Grafana will be added to boardLinks. The rawBoard will store the dashboard itself. meta will be used to get the name of the dashboard and save it with that name in a file. directory directory in which dashboards will be stored. grafanaURL - Grafana url with port in the format http:/127.0.0.1:3030 and apiKey for authorization in Grafana. You need to enter your value in grafanaUR, apiKey and directory.\nNext i am creating a Grafana client\nctx := context.Background() c := sdk.NewClient(grafanaURL, apiKey, sdk.DefaultHTTPClient) And now with the help of the client you can make requests to Grafana api. And the first thing you can get is a list of links to dashboards\nif boardLinks, err = c.Search(ctx); err != nil { log.Fatalln(err) } Now that the link to the dashboards has been obtained, you can get the dashboards themselves and write them to a file.\nfor _, link := range boardLinks { if rawBoard, meta, err = c.GetDashboardByUID(ctx, link.UID); err != nil { log.Printf(\u0026#34;%s for %s\\n\u0026#34;, err, link.URI) continue } rawBoard.ID = 0 writeDashboardToFile(directory, rawBoard, meta.Slug) } In a cycle I pass on all links to dashboards in slice boardLinks and on each of them I call method GetDashboardByUID which I accept in parameters a context and UID of a dashboard.\nIt is also important that the ID field is not present in saved dashboard, otherwise Grafana will not allow you to export it. Because in sdk.Board it has type uint and omitempty I can give it a value of 0 and this field will not be added.\nID uint `json:\u0026#34;id,omitempty\u0026#34;` And in the end the writeDashboardToFile method which accepts three parameters the directory where to store a dashboard, the dashboard itself, and the name of a dashboard which can be received from parameter meta.\nWhat is left is to create the writeDashboardToFile method\nfunc writeDashboardToFile(directory string, dashboard sdk.Board, name string, tag string) { var ( err error dashboardFile *os.File fileName string ) if _, err = os.Stat(directory); os.IsNotExist(err) { os.MkdirAll(directory, 0755) } fileName = fmt.Sprintf(\u0026#34;%s/%s.json\u0026#34;, directory, name) dashboardFile, err = os.Create(fileName) if err != nil { log.Printf(\u0026#34;failed to create file for dashboard %s\u0026#34;, fileName) } defer dashboardFile.Close() err = json.NewEncoder(dashboardFile).Encode(dashboard) if err != nil { log.Printf(\u0026#34;failed to encode dashboard json to file %s\u0026#34;, fileName) } } In this method I check if there is a directory, if not I create it. Next in this directory, I create a file for dashboard using meta parameter as name and use json.NewEncoder to write json to a file.\n","permalink":"https://mpostument.com/posts/programming/golang/export-grafana-dashboards-with-go/","summary":"Hello!\nLet\u0026rsquo;s check how you can use go to export dashboards from Grafana. For interaction with Grafana I use sdk. Sdk can be installed using go get github.com/grafana-tools/sdk.\nLet\u0026rsquo;s start writing code. At the beginning of the file will be the package, import and main function\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; \u0026#34;github.com/grafana-tools/sdk\u0026#34; ) func main() { } ) I will add some parameters:\nvar ( boardLinks []sdk.","title":"Export Grafana Dashboards With Go"},{"content":"About project grafana-sync provides a command line tool to ease import grafana dashboards in json format and export them back to grafana.\nInstall Download the latest binary from releases\nDocs https://github.com/mpostument/grafana-sync/blob/master/README.MD\n","permalink":"https://mpostument.com/posts/projects/grafana-sync/","summary":"About project grafana-sync provides a command line tool to ease import grafana dashboards in json format and export them back to grafana.\nInstall Download the latest binary from releases\nDocs https://github.com/mpostument/grafana-sync/blob/master/README.MD","title":"grafana-sync"},{"content":"Hello!\nToday I will show how you can filter ebs snapshots by date with golang.\nTo begin with I will create a folder and in it, I will initialize the go module\nmkdir snapshotFilter cd snapshotFilter go mod init snapshotFilter You need to create main.go which will contain the code. Let\u0026rsquo;s start with the main function and imports\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;sort\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/session\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/ec2\u0026#34; ) func main() { sess := session.Must(session.NewSession()) ec2Client := ec2.New(sess) filterDate := flag.String(\u0026#34;snapshot-date\u0026#34;, time.Now().Format(time.RFC3339), \u0026#34;Provide snapshot creation date\u0026#34;) snapshotName := flag.String(\u0026#34;snapshot-name\u0026#34;, nil, \u0026#34;Provide snapshot name\u0026#34;) flag.Parse() parsedTime, err := time.Parse(time.RFC3339, *filterDate) if err != nil { log.Fatalln(\u0026#34;Not able to parse time\u0026#34;, err) } if *snapshotName == \u0026#34;\u0026#34; { log.Fatalln(\u0026#34;Please provide snapshot name\u0026#34;) } fmt.Println(filterSnapshotByDate(parsedTime, snapshotName, ec2Client)) } The file starts with the name of the package and the imports. Next is the main function. Main function creates the ec2 client, snapshotName and the filterDate command line parameter. These parameters will be used to enter the date of the snapshot from the terminal and also snapshot name. If filterDate parameter is not entered the current date will be used in RFC3339 format (2006-01-02T15: 04: 05Z07: 00). To specify this value from the console it will look like this - go run main.go -snapshot-date=2020-03-20T06:24:05Z. If snapshotName parameter is not specified, the program will end with the message Please provide snapshot name. I also need to convert the date passed by the user of type string to type time.Time. I will use the time.Parse method, the first argument is the layout, and because I know that my format is RFC3339 I can specify time.RFC3339, the second parameter it is the time value provided by the user. filterSnapshotByDate(filterDate, snapshotName, ec2Client) this is the method that will do the filtering.\nTo filter a snapshot, first get a list of all snapshots. I will use the Name tag to get exactly the snapshot I need.\nfunc filterSnapshotByDate(filterDate time.Time, snapshotName *string, client *ec2.EC2) string { input := \u0026amp;ec2.DescribeSnapshotsInput{Filters: []*ec2.Filter{ {Name: aws.String(\u0026#34;tag:Name\u0026#34;), Values: []*string{snapshotName}}, }} var result []*ec2.Snapshot err := client.DescribeSnapshotsPages(input, func(page *ec2.DescribeSnapshotsOutput, lastPage bool) bool { result = append(result, page.Snapshots...) return !lastPage }) if err != nil { log.Fatalln(\u0026#34;Was not able to get list of snapshots\u0026#34;, err) } In the input variable, I will create a filter that will filter the snapshots by name. And then using the pagination function DescribeSnapshotsPages I will get a list of all snapshots and add them to the list var result []*ec2.Snapshot.\nIn order to filter the snapshot by date, I must first sort them.\nsort.Slice(result, func(i, j int) bool { return result[i].StartTime.After(*result[j].StartTime) }) Because the objects I filter are of type time.Time, I can\u0026rsquo;t compare them with \u0026gt; or \u0026lt;. For such cases go has an After method. With his help I sort the slice with snapshots.\nNow I can filter the snapshots, with cycle I will walk through the sorted snapshots and when the date entered by the user will be greater than the date of the snapshot I will return the snapshot id.\nfor _, snapshot := range result { if filterDate.After(*snapshot.StartTime) { log.Println(\u0026#34;Found snapshot with date\u0026#34;, *snapshot.StartTime, \u0026#34;which are the closest date to\u0026#34;, filterDate.String) return *snapshot.SnapshotId } } If such a snapshot is not found, I will return the id of the last snapshot\nlog.Println(\u0026#34;Not found spashot with specific date\u0026#34;, filterDate.String, \u0026#34;going to use latest\u0026#34;, *result[0].StartTime) return *result[0].SnapshotId } ","permalink":"https://mpostument.com/posts/programming/golang/filter-ebs-snapshot-by-date-with-go/","summary":"Hello!\nToday I will show how you can filter ebs snapshots by date with golang.\nTo begin with I will create a folder and in it, I will initialize the go module\nmkdir snapshotFilter cd snapshotFilter go mod init snapshotFilter You need to create main.go which will contain the code. Let\u0026rsquo;s start with the main function and imports\npackage main import ( \u0026#34;flag\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;sort\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/session\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/ec2\u0026#34; ) func main() { sess := session.","title":"Filter Ebs Snapshot By Date With Go"},{"content":"Hello!\nToday I want to show how you can add custom DNS for the GithubPages site. This requires a few steps. Let\u0026rsquo;s start with domain name registration. I chose Namecheap to register my domain name. Open Namecheap website and in the search you need to enter the name you want to buy. Once you found what you are looking for and add it to your card.\nIt says Taken next to my domain name because I already bought it.\nOnce the domain name is purchased you need to go to your profile and click Manage.\nIn the Advanced DNS section, I will add some records according to the Github documentation. Current addresses can also be found in documentation\nList of A records to create:\n185.199.108.153 185.199.109.153 185.199.110.153 185.199.111.153 You also need to create a CNAME that will point to the GitHub Pages url. In my case it is mpostument.github.io.\nThe next step is to add a file named CNAME to the root of the GitHub Pages repository. In the first line of this file you need to write a new domain name, for me it is mpostument.com. I use a static site generator Hugo with Github Actions (You can read about it in my previous programming). In order for CNAME to get to the root after deployment, it must be placed in the static folder.\nThe last step is to specify a new domain name in the GitHub Pages repository settings. To do this, open the repository and click Settings\nAnd find the section responsible for GitHub Pages. In the Custom domain field, specify your value and click Save. The Enforce HTTPS option will be available within 24 hours, I recommend that you also enable it.\nAnd now after going to mpostument.github.io you will be immediately redirected to mpostument.com\n","permalink":"https://mpostument.com/posts/programming/ci/custom-dns-for-github-pages-copy/","summary":"Hello!\nToday I want to show how you can add custom DNS for the GithubPages site. This requires a few steps. Let\u0026rsquo;s start with domain name registration. I chose Namecheap to register my domain name. Open Namecheap website and in the search you need to enter the name you want to buy. Once you found what you are looking for and add it to your card.\nIt says Taken next to my domain name because I already bought it.","title":"Custom Dns for Github Pages"},{"content":"Hello!\nI want to show how you can deploy a hugo website in GitHub Pages using github actions. That\u0026rsquo;s how I build my site.\nInstall Hugo Hugo can be downloaded from GitHub. Put the binary in the desired location.\nGitHub setup In github you need to create two repositories, one is for .md files and the theme (content-mpostument in my case) and another will be used for GitHub Pages in which will be stored generated website with hugo (mpostument.github.io).\nCode will be deployed to mpostument.github.io repository via github actions. No manual changes allowed here.\nCreate new site with hugo. Hugo will create a new directory with the site name. After directory is created initialize empty git project and connect it with github repository\nhugo new site hugodemo cd hugodemo git init git remote add origin git@github.com:mpostument/content-mpostument.git Now you need to choose a theme for the site. Themes can be reviewed at themes.gohugo\nI chose Terminal theme for my site . The theme can be downloaded and unpacked in the themes folder or added as a submodule. I will add as a submodule to always have the latest version of the theme.\ngit submodule add https://github.com/panr/hugo-theme-terminal.git themes/terminal.\nUsually, each theme has an example configuration file. I will copy it from the github theme page and save it in the config.toml file. You can adjust it to your needs.\nNow you can start the site and see how it will look like hugo serve -D\nGithub Actions For GitHub actions to work, at the root of the repository, you need to create a folder .github/workflows in which you create a yml configuration file for Github Actions.\nIn the on section, I specify to run the build only on push to the master branch.\nIn steps I call several actions in order. The pipeline begins with cloning the repository. After that, I call git submodule update --init --recursive to update the theme using submodule. The next step is to install the required version of hugo and call hugo --minify to generate a static site from .md files and theme. And the last step is to deploy content from the ./Public directory. Directory is created after executing the command hugo --minify. As parameters, you also need to pass the git username, email, branch and repository to which do deployment.\nname: CI on: push: branches: - master jobs: deploy: runs-on: ubuntu-18.04 steps: - name: Git checkout uses: actions/checkout@v2.3.3 - name: Update theme run: git submodule update --init --recursive - name: Setup hugo uses: peaceiris/actions-hugo@v2.4.13 with: hugo-version: \u0026#39;0.76.5\u0026#39; - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3.7.3 with: personal_token: ${{ secrets.GH_TOKEN }} external_repository: mpostument/mpostument.github.io publish_dir: ./public user_name: mpostument user_email: 777rip777@gmail.com publish_branch: master Now it can be committed to the github repository. Merge to the master branch will start build.\nAs soon as the build passed, the new files should immediately appear in the Github Pages repository mpostument.github.io\nIn a few minutes, the website will be available at mpostument.github.io\n","permalink":"https://mpostument.com/posts/programming/ci/deploy-hugo-with-github-actions-and-pages/","summary":"Hello!\nI want to show how you can deploy a hugo website in GitHub Pages using github actions. That\u0026rsquo;s how I build my site.\nInstall Hugo Hugo can be downloaded from GitHub. Put the binary in the desired location.\nGitHub setup In github you need to create two repositories, one is for .md files and the theme (content-mpostument in my case) and another will be used for GitHub Pages in which will be stored generated website with hugo (mpostument.","title":"Deploy Hugo With Github Actions and Pages"},{"content":"Hello!\nToday we will look at how to add authorization to Blazor Server. To begin with we will create the new project in which authentication will be enabled.\nThere are several additional files in the authentication project that are responsible for authentication. A connection string will also be added to the database in the configuration file.\n\u0026#34;ConnectionStrings\u0026#34;: { \u0026#34;DefaultConnection\u0026#34;: \u0026#34;Server=(localdb)\\\\mssqllocaldb;Database=aspnet-BlazorAuth-BD4B8646-B1A3-438C-8FFC-8C3A29438C24;Trusted_Connection=True;MultipleActiveResultSets=true\u0026#34; }, In the package management console, you need to run the Update-Database command to create the database and the required tables.\nNow you can run applications and try to log in.\nYou can add roles to restrict access to certain pages depending on the role.\nAutorization Role support must be added to Startup.cs in the ConfigureService method.\nservices.AddDefaultIdentity\u0026lt;IdentityUser\u0026gt;(options =\u0026gt; options.SignIn.RequireConfirmedAccount = true) .AddRoles\u0026lt;IdentityRole\u0026gt;() .AddEntityFrameworkStores\u0026lt;ApplicationDbContext\u0026gt;(); Generate Roles I will add a new razor component to the Pages folder. When you go to this page, basic roles will be created, it will be available via /addroles. You also need to use DI to inject several objects.\n@page \u0026#34;/addroles\u0026#34; @using Microsoft.AspNetCore.Identity @inject RoleManager\u0026lt;IdentityRole\u0026gt; RoleManager @inject UserManager\u0026lt;IdentityUser\u0026gt; UserManager I will create roles using RoleManager. To begin, create a list that will have all the necessary roles. Then I go through the cycle on all roles, if the role exists then nothing will happen. If not, a new role will be created.\nprivate async Task GenerateRoles() { const string adminRole = \u0026#34;Administrator\u0026#34;; string[] roles = { AdminRole }; foreach (var role in roles) { var roleExist = await RoleManager.RoleExistsAsync(role); if (roleExist == false) { await RoleManager.CreateAsync(new IdentityRole(role)); } } } Create admin The next step is to add users to the roles. In appsettings.json I will add a parameter that will contain the default admin email.\n\u0026#34;AdminUser\u0026#34;: \u0026#34;777rip777@gmail.com\u0026#34; I\u0026rsquo;ll go back to the addroles page and create a new method. I check if the user with the email from the configuration file exists, if, so I add it to the group Administrator. In order to be able to read the data from the configuration file, you need to get an object of type IConfiguration. Add @using Microsoft.Extensions.Configuration and @inject IConfiguration config at the top of the page.\nprivate async Task AddAdmin() { var user = await UserManager.FindByEmailAsync(config.GetValue\u0026lt;string\u0026gt;(\u0026#34;AdminUser\u0026#34;)); if (user != null) { await UserManager.AddToRoleAsync(user, \u0026#34;Administrator\u0026#34;); } } In order for these methods to be called when the pages are open, you need to call them in the method OnParametersSetAsync.\nFull code addroles page.\n@page \u0026#34;/addroles\u0026#34; @using Microsoft.AspNetCore.Identity @using Microsoft.Extensions.Configuration @inject RoleManager\u0026lt;IdentityRole\u0026gt; RoleManager @inject UserManager\u0026lt;IdentityUser\u0026gt; UserManager @inject IConfiguration config @code { protected override async Task OnParametersSetAsync() { await GenerateRoles(); await AddAdmin(); } private async Task GenerateRoles() { const string adminRole = \u0026#34;Administrator\u0026#34;; string[] roles = { adminRole }; foreach (var role in roles) { var roleExist = await RoleManager.RoleExistsAsync(role); if (roleExist == false) { await RoleManager.CreateAsync(new IdentityRole(role)); } } } private async Task AddAdmin() { var user = await UserManager.FindByEmailAsync(config.GetValue\u0026lt;string\u0026gt;(\u0026#34;AdminUser\u0026#34;)); if (user != null) { await UserManager.AddToRoleAsync(user, \u0026#34;Administrator\u0026#34;); } } } Now opening the page /addroles in the browser will create roles and the user (if user exist) to the role.\nRestrict access to pages Now that we have roles, we can restrict access to the pages. For the FetchData page, I will add @attribute [Authorize]. This will give access to the page only for authorized users. And for the Counter page I will specify @attribute [Authorize (Roles = \u0026quot;Administrator\u0026quot;)] this will give access to the page only for users with the role of administrator.\nYou can replace the Not Authorized error in App.razor in the NotAutorized section.\n\u0026lt;CascadingAuthenticationState\u0026gt; \u0026lt;Router AppAssembly=\u0026#34;@typeof(Program).Assembly\u0026#34;\u0026gt; \u0026lt;Found Context=\u0026#34;routeData\u0026#34;\u0026gt; \u0026lt;AuthorizeRouteView RouteData=\u0026#34;@routeData\u0026#34; DefaultLayout=\u0026#34;@typeof(MainLayout)\u0026#34;\u0026gt; \u0026lt;NotAuthorized\u0026gt; \u0026lt;h1\u0026gt;You don\u0026#39;t have access\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Ask your administrator to give you permission\u0026lt;/p\u0026gt; \u0026lt;/NotAuthorized\u0026gt; \u0026lt;/AuthorizeRouteView\u0026gt; /\u0026gt; \u0026lt;/Found\u0026gt; \u0026lt;NotFound\u0026gt; \u0026lt;LayoutView Layout=\u0026#34;@typeof(MainLayout)\u0026#34;\u0026gt; \u0026lt;p\u0026gt;Sorry, there\u0026#39;s nothing at this address.\u0026lt;/p\u0026gt; \u0026lt;/LayoutView\u0026gt; \u0026lt;/NotFound\u0026gt; \u0026lt;/Router\u0026gt; \u0026lt;/CascadingAuthenticationState\u0026gt; You can also completely hide a site element for unregistered users. In NavMenu.razor I will add menu items to \u0026lt;AuthorizeView\u0026gt;\u0026lt;/AuthorizeView\u0026gt;.\n\u0026lt;div class=\u0026#34;top-row pl-4 navbar navbar-dark\u0026#34;\u0026gt; \u0026lt;a class=\u0026#34;navbar-brand\u0026#34; href=\u0026#34;\u0026#34;\u0026gt;BlazorAuth\u0026lt;/a\u0026gt; \u0026lt;button class=\u0026#34;navbar-toggler\u0026#34; @onclick=\u0026#34;ToggleNavMenu\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;navbar-toggler-icon\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;@NavMenuCssClass\u0026#34; @onclick=\u0026#34;ToggleNavMenu\u0026#34;\u0026gt; \u0026lt;ul class=\u0026#34;nav flex-column\u0026#34;\u0026gt; \u0026lt;li class=\u0026#34;nav-item px-3\u0026#34;\u0026gt; \u0026lt;NavLink class=\u0026#34;nav-link\u0026#34; href=\u0026#34;\u0026#34; Match=\u0026#34;NavLinkMatch.All\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;oi oi-home\u0026#34; aria-hidden=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; Home \u0026lt;/NavLink\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;AuthorizeView Roles=\u0026#34;Administrator\u0026#34;\u0026gt; \u0026lt;li class=\u0026#34;nav-item px-3\u0026#34;\u0026gt; \u0026lt;NavLink class=\u0026#34;nav-link\u0026#34; href=\u0026#34;counter\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;oi oi-plus\u0026#34; aria-hidden=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; Counter \u0026lt;/NavLink\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/AuthorizeView\u0026gt; \u0026lt;AuthorizeView\u0026gt; \u0026lt;li class=\u0026#34;nav-item px-3\u0026#34;\u0026gt; \u0026lt;NavLink class=\u0026#34;nav-link\u0026#34; href=\u0026#34;fetchdata\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;oi oi-list-rich\u0026#34; aria-hidden=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; Fetch data \u0026lt;/NavLink\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/AuthorizeView\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; @code { private bool collapseNavMenu = true; private string NavMenuCssClass =\u0026gt; collapseNavMenu ? \u0026#34;collapse\u0026#34; : null; private void ToggleNavMenu() { collapseNavMenu = !collapseNavMenu; } } Now counter in the menu will be seen only by users with the role Administrator and fetchdata will be available to registered users.\nIn the index page I will add messages to users to log in.\n@page \u0026#34;/\u0026#34; \u0026lt;AuthorizeView\u0026gt; \u0026lt;Authorized\u0026gt; \u0026lt;h1\u0026gt;Thank you for logging in\u0026lt;/h1\u0026gt; \u0026lt;/Authorized\u0026gt; \u0026lt;NotAuthorized\u0026gt; \u0026lt;h3 class=\u0026#34;text-danger\u0026#34;\u0026gt;Log in please\u0026lt;/h3\u0026gt; \u0026lt;/NotAuthorized\u0026gt; \u0026lt;/AuthorizeView\u0026gt; \u0026lt;AuthorizeView Roles=\u0026#34;Administrator\u0026#34;\u0026gt; \u0026lt;Authorized\u0026gt; Welcome back Admin \u0026lt;/Authorized\u0026gt; \u0026lt;/AuthorizeView\u0026gt; If the user is not logged in, there will be a message Log in please. Otherwise, Thank you for logging in. And if the user is an admin, there will be an additional message Welcome back Admin.\n","permalink":"https://mpostument.com/posts/programming/dotnet/blazor-server-authorization/","summary":"Hello!\nToday we will look at how to add authorization to Blazor Server. To begin with we will create the new project in which authentication will be enabled.\nThere are several additional files in the authentication project that are responsible for authentication. A connection string will also be added to the database in the configuration file.\n\u0026#34;ConnectionStrings\u0026#34;: { \u0026#34;DefaultConnection\u0026#34;: \u0026#34;Server=(localdb)\\\\mssqllocaldb;Database=aspnet-BlazorAuth-BD4B8646-B1A3-438C-8FFC-8C3A29438C24;Trusted_Connection=True;MultipleActiveResultSets=true\u0026#34; }, In the package management console, you need to run the Update-Database command to create the database and the required tables.","title":"Blazor Server Authorization"},{"content":"Hello!\nLet\u0026rsquo;s see how you can create a form in BlazorServer. Start with the model. I will create a file Models/User.cs, which will contain fields for the form.\nnamespace BlazorLearn.Models { public class User { public string UserName { get; set; } public string LastName { get; set; } public string Email { get; set; } public int Age { get; set; } public string Gender { get; set; } } } For the model to be available in razor components, it must be added to _Imports.razor as @using BlazorLearn.Models.\nBasic Form Now I will create the razor component Register.razor which will contain the form. To be able to open the form in the browser, I will add @page \u0026quot;/register\u0026quot; at the top of the component. In the code section you need to create a new object of class User, as well as an empty string in which the data will be written after the submission of the form. And I will add \u0026lt;p\u0026gt;@_formResult\u0026lt;/p\u0026gt; to display the data entered after the submission.\nNow let\u0026rsquo;s move on to the form itself.\nIn blazor, the form is created using the EditForm tag to which you want to pass the model. We have an object of type User which is created in the section code - \u0026lt;EditForm Model = \u0026quot;_user\u0026quot;\u0026gt;\u0026lt;/EditForm\u0026gt;. Now you need to add input fields to the form. For text tag will be \u0026lt;InputText\u0026gt;\u0026lt;/InputText\u0026gt; for other data types have their own tags, for example InputNumber for numeric types. I will add an input field for each element of the model. You can also call a method on events like OnValidSubmit or OnInvalidSubmit. For this purpose I will make a method HandleValidSubmit which will write UserName and LastName from the form to empty string which was created in code section.\n@page \u0026#34;/register\u0026#34; \u0026lt;p\u0026gt;@_formResult\u0026lt;/p\u0026gt; \u0026lt;EditForm Model=\u0026#34;_user\u0026#34; OnValidSubmit=\u0026#34;HandleValidSubmit\u0026#34; On\u0026gt; \u0026lt;InputText id=\u0026#34;UserName\u0026#34; @bind-Value=\u0026#34;_user.UserName\u0026#34;\u0026gt;\u0026lt;/InputText\u0026gt; \u0026lt;InputText id=\u0026#34;LastName\u0026#34; @bind-Value=\u0026#34;_user.LastName\u0026#34;\u0026gt;\u0026lt;/InputText\u0026gt; \u0026lt;InputText id=\u0026#34;Email\u0026#34; @bind-Value=\u0026#34;_user.Email\u0026#34;\u0026gt;\u0026lt;/InputText\u0026gt; \u0026lt;InputSelect id=\u0026#34;Gender\u0026#34; @bind-Value=\u0026#34;_user.Gender\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;\u0026#34;\u0026gt;Select Gender\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;1\u0026#34;\u0026gt;Male\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;2\u0026#34;\u0026gt;Female\u0026lt;/option\u0026gt; \u0026lt;/InputSelect\u0026gt; \u0026lt;InputNumber id=\u0026#34;Age\u0026#34; @bind-Value=\u0026#34;_user.Age\u0026#34;\u0026gt;\u0026lt;/InputNumber\u0026gt; \u0026lt;button class=\u0026#34;btn btn-primary\u0026#34; type=\u0026#34;submit\u0026#34;\u0026gt;Submit\u0026lt;/button\u0026gt; \u0026lt;/EditForm\u0026gt; @code { private User _user = new User(); private string _formResult = \u0026#34;\u0026#34;; private void HandleValidSubmit() { _formResult = $\u0026#34;{_user.UserName}, {_user.LastName} was created\u0026#34;; } } The form works, but in the Age field I entered -30 and no validation took place. Let\u0026rsquo;s now add validation\nForm Validation Validation is added using attributes. Let\u0026rsquo;s go back to the model and make some changes.\nusing System.ComponentModel.DataAnnotations; namespace BlazorLearn.Models { public class User { [Required] [StringLength(maximumLength: 20, MinimumLength = 5, ErrorMessage = \u0026#34;Invalid UserName length\u0026#34;)] public string UserName { get; set; } [Required] [StringLength(maximumLength: 10, MinimumLength = 5, ErrorMessage = \u0026#34;Invalid LastName length\u0026#34;)] public string LastName { get; set; } [Required] [EmailAddress(ErrorMessage = \u0026#34;Provide a valid email address\u0026#34;)] public string Email { get; set; } [Required] [Range(1, 100, ErrorMessage = \u0026#34;You need to enter valid range\u0026#34;)] public int Age { get; set; } public string Gender { get; set; } } } Length validation is now enabled for UserName and LestName and these fields are also required. Email validation has been added for Email. And for Age added validation for number between 1 and 100. These fields are also required. No validation has been added for the Gender field.\nIn order for validation to work place, you need to add a \u0026lt;DataAnnotationsValidator\u0026gt;\u0026lt;/DataAnnotationsValidator\u0026gt; tag to the form.\nNow the validation works but no messages are displayed when validation failed. I will add another tag \u0026lt;ValidationSummary\u0026gt;\u0026lt;/ValidationSummary\u0026gt; to form. As a result, the form will look like this\n@page \u0026#34;/register\u0026#34; \u0026lt;p\u0026gt;@_formResult\u0026lt;/p\u0026gt; \u0026lt;EditForm Model=\u0026#34;_user\u0026#34; OnValidSubmit=\u0026#34;HandleValidSubmit\u0026#34;\u0026gt; \u0026lt;DataAnnotationsValidator\u0026gt;\u0026lt;/DataAnnotationsValidator\u0026gt; \u0026lt;ValidationSummary\u0026gt;\u0026lt;/ValidationSummary\u0026gt; \u0026lt;label\u0026gt; First Name: \u0026lt;InputText id=\u0026#34;UserName\u0026#34; @bind-Value=\u0026#34;_user.UserName\u0026#34;\u0026gt;\u0026lt;/InputText\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;label\u0026gt; Last Name: \u0026lt;InputText id=\u0026#34;LastName\u0026#34; @bind-Value=\u0026#34;_user.LastName\u0026#34;\u0026gt;\u0026lt;/InputText\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;label\u0026gt; Email: \u0026lt;InputText id=\u0026#34;Email\u0026#34; @bind-Value=\u0026#34;_user.Email\u0026#34;\u0026gt;\u0026lt;/InputText\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;label\u0026gt; Gender: \u0026lt;InputSelect id=\u0026#34;Gender\u0026#34; @bind-Value=\u0026#34;_user.Gender\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;\u0026#34;\u0026gt;Select Gender\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;1\u0026#34;\u0026gt;Male\u0026lt;/option\u0026gt; \u0026lt;option value=\u0026#34;2\u0026#34;\u0026gt;Female\u0026lt;/option\u0026gt; \u0026lt;/InputSelect\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;label\u0026gt; Age: \u0026lt;InputNumber id=\u0026#34;Age\u0026#34; @bind-Value=\u0026#34;_user.Age\u0026#34;\u0026gt;\u0026lt;/InputNumber\u0026gt; \u0026lt;/label\u0026gt; \u0026lt;button class=\u0026#34;btn btn-primary\u0026#34; type=\u0026#34;submit\u0026#34;\u0026gt;Submit\u0026lt;/button\u0026gt; \u0026lt;/EditForm\u0026gt; @code { private User _user = new User(); private string _formResult = \u0026#34;\u0026#34;; private void HandleValidSubmit() { _formResult = $\u0026#34;{_user.UserName}, {_user.LastName} was created\u0026#34;; } } ","permalink":"https://mpostument.com/posts/programming/dotnet/blazor-server-forms/","summary":"Hello!\nLet\u0026rsquo;s see how you can create a form in BlazorServer. Start with the model. I will create a file Models/User.cs, which will contain fields for the form.\nnamespace BlazorLearn.Models { public class User { public string UserName { get; set; } public string LastName { get; set; } public string Email { get; set; } public int Age { get; set; } public string Gender { get; set; } } } For the model to be available in razor components, it must be added to _Imports.","title":"Blazor Forms"},{"content":"Hello!\nLet\u0026rsquo;s see how you can transfer data from one component to another.\nButton I will create a component called ThumbButton which will contain two buttons. Depending on which button is pressed, the corresponding method will be called and the data will be transferred to the parrent component. @Onclick specifies which method will be executed when the button is pressed.\nTo pass data to the parrent component, a property of type EventCallback\u0026lt;Thumb\u0026gt; is used, where Thumb is enum. Instead of enum there can be any other type of data. In the method ThumbUp and ThumbDown I call OnVote.InvokeAsync in which the corresponding enum is passed. This call passes data to the parrent component.\n\u0026lt;button class=\u0026#34;btn btn-primary mx-2\u0026#34; @onclick=\u0026#34;ThumbUp\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;oi oi-thumb-up\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;button class=\u0026#34;btn btn-danger mx-2\u0026#34; @onclick=\u0026#34;ThumbDown\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;oi oi-thumb-down\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; @code { public EventCallback\u0026lt;Thumb\u0026gt; OnVote { get; set; } private void ThumbUp() { OnVote.InvokeAsync(Thumb.Up); } private void ThumbDown() { OnVote.InvokeAsync(Thumb.Down); } public enum Thumb { Up, Down } } You can now reuse this component on any page.\nButton Component on page Now take the page Counter. And add the newly created component with a button. Let\u0026rsquo;s start with the code. I will create two properties isThumbUp and isThumbDown of type bool. Depending on which property is set to true, the corresponding data will be displayed. Also, if one of these properties becomes true, the button will disappear.\nTo get data from a button component, you need a handler that will change the values ​​of isThumbUp and isThumbDown in response to the button press. It added in the same way as any other component. But method handler need to be added to retreive data \u0026lt;ThumbButton OnVote =\u0026quot;OnVotedHandler\u0026quot;\u0026gt;\u0026lt;/ThumbButton\u0026gt;. The handler method must accept a parameter of the type received from the button. private void OnVotedHandler(ThumbButton.Thumb result). Depending on what value came to the input, the values ​​of the parameters isThumbUp and isThumbDown are set.\n@page \u0026#34;/counter\u0026#34; \u0026lt;h1\u0026gt;Counter\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Current count: @currentCount\u0026lt;/p\u0026gt; \u0026lt;button class=\u0026#34;btn btn-primary\u0026#34; @onclick=\u0026#34;IncrementCount\u0026#34;\u0026gt;Click me\u0026lt;/button\u0026gt; @if (isThumbUp) { \u0026lt;p class=\u0026#34;text-success\u0026#34;\u0026gt;Your thumb is up\u0026lt;/p\u0026gt; } @if (isThumbDown) { \u0026lt;p class=\u0026#34;text-danger\u0026#34;\u0026gt;Your thumb is down\u0026lt;/p\u0026gt; } @if (!isThumbDown \u0026amp;\u0026amp; !isThumbUp) { \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;col-12\u0026#34;\u0026gt; \u0026lt;ThumbButton OnVote=\u0026#34;OnVotedHandler\u0026#34;\u0026gt;\u0026lt;/ThumbButton\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; } @code { private bool isThumbUp = false; private bool isThumbDown = false; private void OnVotedHandler(ThumbButton.Thumb result) { if (result == ThumbButton.Thumb.Up) { isThumbUp = true; } else if (result == ThumbButton.Thumb.Down) { isThumbDown = true; } } } ","permalink":"https://mpostument.com/posts/programming/dotnet/events-in-blazor/","summary":"Hello!\nLet\u0026rsquo;s see how you can transfer data from one component to another.\nButton I will create a component called ThumbButton which will contain two buttons. Depending on which button is pressed, the corresponding method will be called and the data will be transferred to the parrent component. @Onclick specifies which method will be executed when the button is pressed.\nTo pass data to the parrent component, a property of type EventCallback\u0026lt;Thumb\u0026gt; is used, where Thumb is enum.","title":"Events in Blazor"},{"content":"Hello!\nLet\u0026rsquo;s see how you can use components in BlazorServer and pass parameters to them. To do this, make two classes Student and School which will be displayed on the page. These classes will be in the Models folder.\nModel using System.Collections.Generic; namespace BlazorLearn.Models { public class School { public string Name { get; set; } public List\u0026lt;Student\u0026gt; Students { get; set; } } } namespace BlazorLearn.Models { public class Student { public string FirstName { get; set; } public string LastName { get; set; } } } Razor Component Now I will create razor components that will be responsible for displaying the data. In order for Models to be available for razor components you need to add @using BlazorLearn.Models. This can be added to _Imports.razor then Models will be available for all components or add to each component that needs a model.\nYou need to create three components, first will display information about students, the next will display information about the school and the last one will be responsible to combine both previous models.\nLet\u0026rsquo;s start with the component - SchoolComponent\n\u0026lt;div\u0026gt; \u0026lt;p\u0026gt;@SchoolInfo.Name\u0026lt;/p\u0026gt; @StudentFragment \u0026lt;/div\u0026gt; @code { [Parameter] public School SchoolInfo { get; set; } [Parameter] public RenderFragment StudentFragment { get; set; } } The amount of code is not so large, so I will store everything in .razor file sand will not create a separate class. In the code section, I create two properties, one of type School which will contain information about the school and the other of type RenderFragment this property will be responsible for displaying information about students.\nThe next component is StudentComponent\n\u0026lt;div\u0026gt; \u0026lt;p\u0026gt;@StudentInfo.FirstName @StudentInfo.LastName\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; @code { [Parameter] public Student StudentInfo { get; set; } } This component has one property of type Student which will contain information about the student. The Html part will display the first and last name.\nAnd now there is the last component that will also be a html page.\n@page \u0026#34;/school\u0026#34; \u0026lt;div class=\u0026#34;row\u0026#34;\u0026gt; @foreach (var s in Schools) { \u0026lt;div class=\u0026#34;col-4\u0026#34;\u0026gt; \u0026lt;SchoolComponent SchoolInfo=\u0026#34;@s\u0026#34;\u0026gt; \u0026lt;StudentFragment\u0026gt; @if (s.Students.Count \u0026gt; 0) { \u0026lt;h3\u0026gt;Students\u0026lt;/h3\u0026gt; } @foreach (var stu in s.Students) { \u0026lt;StudentComponent StudentInfo=\u0026#34;@stu\u0026#34;\u0026gt;\u0026lt;/StudentComponent\u0026gt; } \u0026lt;/StudentFragment\u0026gt; \u0026lt;/SchoolComponent\u0026gt; \u0026lt;/div\u0026gt; } \u0026lt;/div\u0026gt; @code { public List\u0026lt;School\u0026gt; Schools { get; set; } public AllSchools() { Schools = new List\u0026lt;School\u0026gt;() { new School() { Name = \u0026#34;Hogwarts\u0026#34;, Students = new List\u0026lt;Student\u0026gt;() { new Student() { FirstName = \u0026#34;Harry\u0026#34;, LastName = \u0026#34;Potter\u0026#34; }, new Student() { FirstName = \u0026#34;Ron\u0026#34;, LastName = \u0026#34;Weasley\u0026#34; } } }, new School() { Name = \u0026#34;EmptySchool\u0026#34;, Students = new List\u0026lt;Student\u0026gt;() } }; } } There is more code here than in the previous components, so it makes sense to make it a separate class. But I\u0026rsquo;ll leave it here for now. In the code section one property of type List \u0026lt;School\u0026gt; is created and in the constructor I add data to this list.\nThe Html part is more interesting, it starts with @page \u0026quot;/school\u0026quot; and it allows to open in a browser endpoint /school and get to this page. Next is the rendering of components. To include a component on a page, simply specify \u0026lt;SchoolComponent\u0026gt;\u0026lt;/SchoolComponent\u0026gt;. Since our SchoolComponent accepts the SchoolInfo property, I can pass it as SchoolInfo=. But this does not apply to the StudentFragment property, it is passed as \u0026lt;StudentFragment\u0026gt;\u0026lt;/StudentFragment\u0026gt;. Inside it, I display \u0026lt;h3\u0026gt;Students\u0026lt;/h3\u0026gt; for each school if s.Students.Count is greater than 0. And then I cycle through the students and display the component \u0026lt;StudentComponent StudentInfo=\u0026quot;@stu\u0026quot;\u0026gt;\u0026lt;/StudentComponent\u0026gt; for each student.\n","permalink":"https://mpostument.com/posts/programming/dotnet/components-with-parameters/","summary":"Hello!\nLet\u0026rsquo;s see how you can use components in BlazorServer and pass parameters to them. To do this, make two classes Student and School which will be displayed on the page. These classes will be in the Models folder.\nModel using System.Collections.Generic; namespace BlazorLearn.Models { public class School { public string Name { get; set; } public List\u0026lt;Student\u0026gt; Students { get; set; } } } namespace BlazorLearn.Models { public class Student { public string FirstName { get; set; } public string LastName { get; set; } } } Razor Component Now I will create razor components that will be responsible for displaying the data.","title":"Component With Parameters"},{"content":"Hello!\nI want to show you how you can organize code in BlazorServer.\nCode in client The first option is to store the code on the client (in a razor file). Take the Counter.razor file which combines html and c# code.\n@page \u0026#34;/counter\u0026#34; \u0026lt;h1\u0026gt;Counter\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Current count: @currentCount\u0026lt;/p\u0026gt; \u0026lt;button class=\u0026#34;btn btn-primary\u0026#34; @onclick=\u0026#34;IncrementCount\u0026#34;\u0026gt;Click me\u0026lt;/button\u0026gt; @code { private int currentCount = 0; private void IncrementCount() { currentCount++; } } This page has a private currentCount variable and an IncrementCount method that increments this variable. On the html side, there is a Click me button that calls the c# IncrementCount method and display the currentCount variable value on the page. This option should only be used if the amount of code is not significant at all.\nInheritance Another option is to create a new class that will be inherited by razor page. This class must inherit from ComponentBase.\nusing Microsoft.AspNetCore.Components; namespace BlazorLearn.Pages { public class CounterBase : ComponentBase { protected int currentCount = 0; protected void IncrementCount() { currentCount++; } } } I moved the code from Counter.razor to the new class CounterBase. In this case it is necessary to change the access modifier from private to protected. In Counter.razor you need to add @inherits CounterBase.\n@page \u0026#34;/counter\u0026#34; @inherits CounterBase \u0026lt;h1\u0026gt;Counter\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Current count: @currentCount\u0026lt;/p\u0026gt; \u0026lt;button class=\u0026#34;btn btn-primary\u0026#34; @onclick=\u0026#34;IncrementCount\u0026#34;\u0026gt;Click me\u0026lt;/button\u0026gt; To add a logger or other component, the constructor cannot be used. But there is another way, property with the modifier Inject can be used for DI.\nusing Microsoft.AspNetCore.Components; using Microsoft.Extensions.Logging; namespace BlazorLearn.Pages { public class CounterBase : ComponentBase { [Inject] public ILogger\u0026lt;CounterBase\u0026gt; logger { get; set; } protected int currentCount = 0; protected void IncrementCount() { logger.LogInformation(\u0026#34;CounterExecuted\u0026#34;); currentCount++; } } } Partial class The third option si to use partial classes. To do this, I will create a new partial class with the same name as the razor page.\nnamespace BlazorLearn.Pages { public partial class Counter { private int currentCount = 0; private void IncrementCount() { currentCount++; } } } Access modifiers can remain private, and you don\u0026rsquo;t need to change anything on the razor page itself. Just moved all code from there to corresponding class.\n@page \u0026#34;/counter\u0026#34; \u0026lt;h1\u0026gt;Counter\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Current count: @currentCount\u0026lt;/p\u0026gt; \u0026lt;button class=\u0026#34;btn btn-primary\u0026#34; @onclick=\u0026#34;IncrementCount\u0026#34;\u0026gt;Click me\u0026lt;/button\u0026gt; Logger or other components can be added in the same way as in the second version. Using properties with the Inject attribute.\n[Inject] public ILogger\u0026lt;Counter\u0026gt; Logger { get; set; } ","permalink":"https://mpostument.com/posts/programming/dotnet/three-ways-to-organize-code/","summary":"Hello!\nI want to show you how you can organize code in BlazorServer.\nCode in client The first option is to store the code on the client (in a razor file). Take the Counter.razor file which combines html and c# code.\n@page \u0026#34;/counter\u0026#34; \u0026lt;h1\u0026gt;Counter\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Current count: @currentCount\u0026lt;/p\u0026gt; \u0026lt;button class=\u0026#34;btn btn-primary\u0026#34; @onclick=\u0026#34;IncrementCount\u0026#34;\u0026gt;Click me\u0026lt;/button\u0026gt; @code { private int currentCount = 0; private void IncrementCount() { currentCount++; } } This page has a private currentCount variable and an IncrementCount method that increments this variable.","title":"Three Ways to Organize Code in BlazorServer"},{"content":"Hello!\nToday we will look at how you can create a new page in BlazorServer and add it to the menu.\nAdd new page To do this, right-click on the Pages folder and select Razor Component...\nThe following window will appear in which you need to enter the name and click Add.\nThe page will be created with the following content:\n\u0026lt;h3\u0026gt;Test1\u0026lt;/h3\u0026gt; @code { } To be able to open it in the browser at the beginning of the file you need to add @page\u0026quot; /test1\u0026quot; where test1 is the name of the endpoint. And now run the application and enter it in the browser localhost:5001/test1\nBut our page is not displayed in the menu. Let\u0026rsquo;s add it now.\nAdd page in menu Select the file with menu Shared/NavMenu.razor\nThe file has the following content\n\u0026lt;div class=\u0026#34;top-row pl-4 navbar navbar-dark\u0026#34;\u0026gt; \u0026lt;a class=\u0026#34;navbar-brand\u0026#34; href=\u0026#34;\u0026#34;\u0026gt;BlazorLearn\u0026lt;/a\u0026gt; \u0026lt;button class=\u0026#34;navbar-toggler\u0026#34; @onclick=\u0026#34;ToggleNavMenu\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;navbar-toggler-icon\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; \u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;@NavMenuCssClass\u0026#34; @onclick=\u0026#34;ToggleNavMenu\u0026#34;\u0026gt; \u0026lt;ul class=\u0026#34;nav flex-column\u0026#34;\u0026gt; \u0026lt;li class=\u0026#34;nav-item px-3\u0026#34;\u0026gt; \u0026lt;NavLink class=\u0026#34;nav-link\u0026#34; href=\u0026#34;\u0026#34; Match=\u0026#34;NavLinkMatch.All\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;oi oi-home\u0026#34; aria-hidden=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; Home \u0026lt;/NavLink\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;nav-item px-3\u0026#34;\u0026gt; \u0026lt;NavLink class=\u0026#34;nav-link\u0026#34; href=\u0026#34;counter\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;oi oi-plus\u0026#34; aria-hidden=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; Counter \u0026lt;/NavLink\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026#34;nav-item px-3\u0026#34;\u0026gt; \u0026lt;NavLink class=\u0026#34;nav-link\u0026#34; href=\u0026#34;fetchdata\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;oi oi-list-rich\u0026#34; aria-hidden=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; Fetch data \u0026lt;/NavLink\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; @code { private bool collapseNavMenu = true; private string NavMenuCssClass =\u0026gt; collapseNavMenu ? \u0026#34;collapse\u0026#34; : null; private void ToggleNavMenu() { collapseNavMenu = !collapseNavMenu; } } What we need is to add another element to @NavMenuCssClass, it is this div that is responsible for displaying the menu elements. I will add the following item\n\u0026lt;li class=\u0026#34;nav-item px-3\u0026#34;\u0026gt; \u0026lt;NavLink class=\u0026#34;nav-link\u0026#34; href=\u0026#34;test1\u0026#34;\u0026gt; \u0026lt;span class=\u0026#34;oi oi-aperture\u0026#34; aria-hidden=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/span\u0026gt; Test1 \u0026lt;/NavLink\u0026gt; \u0026lt;/li\u0026gt; href - this is the endpoint of the page \u0026lt;span class=\u0026quot;oi oi-aperture\u0026quot; aria-hidden=\u0026quot;true\u0026quot;\u0026gt;\u0026lt;/span\u0026gt; - adds an icon to the menu item\nStart applications and get the result.\nFolder structure Pages can be organized into folders. For example, I will create a Demo folder inside Pages and move the page Test1.razor to it.\nAfter launching the application, the page will still work, even if we change it location. But with such a folder structure, it would be more logical to reflect this in the endopint. To do this in Test1.razor i will change the endpoint from@page \u0026quot;/test1\u0026quot;to@page \u0026quot;/demo/test1\u0026quot;. Also, I need to make change in the NavMenu.razor file from href = \u0026quot;test1\u0026quot; to href = \u0026quot;demo/test1\u0026quot;.\n","permalink":"https://mpostument.com/posts/programming/dotnet/create-page-in-blazor/","summary":"Hello!\nToday we will look at how you can create a new page in BlazorServer and add it to the menu.\nAdd new page To do this, right-click on the Pages folder and select Razor Component...\nThe following window will appear in which you need to enter the name and click Add.\nThe page will be created with the following content:\n\u0026lt;h3\u0026gt;Test1\u0026lt;/h3\u0026gt; @code { } To be able to open it in the browser at the beginning of the file you need to add @page\u0026quot; /test1\u0026quot; where test1 is the name of the endpoint.","title":"Create Page in Blazor"},{"content":"Hi!\nToday we will see how to configure logging in BlazorServer. Immediately after creating a project, the appsettings.json file will be in the root of the project.\n{ \u0026#34;Logging\u0026#34;: { \u0026#34;LogLevel\u0026#34;: { \u0026#34;Default\u0026#34;: \u0026#34;Information\u0026#34;, \u0026#34;Microsoft\u0026#34;: \u0026#34;Warning\u0026#34;, \u0026#34;Microsoft.Hosting.Lifetime\u0026#34;: \u0026#34;Information\u0026#34; } }, \u0026#34;AllowedHosts\u0026#34;: \u0026#34;*\u0026#34; } This is where logging settings are set. LogLevel starts with Default which have value Information. This means that the default log level will be Information. Next is Microsoft with the Warning level, respectively for namespace Microsoft the login level will be Warning. And at the very end Microsoft.Hosting.Lifetime with value Information. This means that for all namespace Microsoft except part Microsoft.Hosting.Lifetime which is part of Microsoft namespace log level will be Warning.\nIf you change the logging settings in this file and run the application, nothing will change. And all because launchSettings.json hase environement variable \u0026quot;ASPNETCORE_ENVIRONMENT\u0026quot;: \u0026quot;Development\u0026quot;. And for development environment another file is used - appsettings.Development.json. You can find it by clicking on the arrow next to appsettings.json.\nI will change all levels to Debug in appsettings.Development.json.\n{ \u0026#34;DetailedErrors\u0026#34;: true, \u0026#34;Logging\u0026#34;: { \u0026#34;LogLevel\u0026#34;: { \u0026#34;Default\u0026#34;: \u0026#34;Debug\u0026#34;, \u0026#34;Microsoft\u0026#34;: \u0026#34;Debug\u0026#34;, \u0026#34;Microsoft.Hosting.Lifetime\u0026#34;: \u0026#34;Debug\u0026#34; } } } After that I will launch the application. And there will be a lot of logs in the console.\nIf I need to include from all logs only any concrete i can do it. For example, I need to see logs from Microsoft.AspNetCore.Components.RenderTree.Renderer in Debug and all others in info. To do this, I will add it to appsettings.Development.json.\n{ \u0026#34;DetailedErrors\u0026#34;: true, \u0026#34;Logging\u0026#34;: { \u0026#34;LogLevel\u0026#34;: { \u0026#34;Default\u0026#34;: \u0026#34;Information\u0026#34;, \u0026#34;Microsoft\u0026#34;: \u0026#34;Warning\u0026#34;, \u0026#34;Microsoft.Hosting.Lifetime\u0026#34;: \u0026#34;Information\u0026#34;, \u0026#34;Microsoft.AspNetCore.Components.RenderTree.Renderer\u0026#34;: \u0026#34;Debug\u0026#34; } } } After that logs will look like this Logging from razor pages Let\u0026rsquo;s try to send a log message from razor pages. For example, take the page Counter.razor. To use the logger, you must first import the desired namespace @using Microsoft.Extensions.Logging. And with DI, inject the logger in the page @inject ILogger\u0026lt;Counter\u0026gt; Logger. Now you can use logger.\n@page \u0026#34;/counter\u0026#34; @using Microsoft.Extensions.Logging @inject ILogger\u0026lt;Counter\u0026gt; Logger \u0026lt;h1\u0026gt;Counter\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Current count: @currentCount\u0026lt;/p\u0026gt; \u0026lt;button class=\u0026#34;btn btn-primary\u0026#34; @onclick=\u0026#34;IncrementCount\u0026#34;\u0026gt;Click me\u0026lt;/button\u0026gt; @code { private int currentCount = 0; private void IncrementCount() { currentCount++; Logger.LogInformation(\u0026#34;Button clicked, current count #{count}\u0026#34;, currentCount); } } Every time I press the Click me button. The message appears in the console. Logging from class Similarly, log messages can be sent from classes using DI. First, the object private readonly ILogger\u0026lt;WeatherForecastService\u0026gt; _logger; is created and with the help of the constructor i inject logger.\nusing Microsoft.Extensions.Logging; using System; using System.Linq; using System.Threading.Tasks; namespace BlazorLearn.Data { public class WeatherForecastService { private readonly ILogger\u0026lt;WeatherForecastService\u0026gt; _logger; private static readonly string[] Summaries = new[] { \u0026#34;Freezing\u0026#34;, \u0026#34;Bracing\u0026#34;, \u0026#34;Chilly\u0026#34;, \u0026#34;Cool\u0026#34;, \u0026#34;Mild\u0026#34;, \u0026#34;Warm\u0026#34;, \u0026#34;Balmy\u0026#34;, \u0026#34;Hot\u0026#34;, \u0026#34;Sweltering\u0026#34;, \u0026#34;Scorching\u0026#34; }; public WeatherForecastService(ILogger\u0026lt;WeatherForecastService\u0026gt; logger) { _logger = logger; } public Task\u0026lt;WeatherForecast[]\u0026gt; GetForecastAsync(DateTime startDate) { _logger.LogInformation(\u0026#34;WeatherForecastService called\u0026#34;); var rng = new Random(); return Task.FromResult(Enumerable.Range(1, 5).Select(index =\u0026gt; new WeatherForecast { Date = startDate.AddDays(index), TemperatureC = rng.Next(1, 100), Summary = Summaries[rng.Next(Summaries.Length)] }).ToArray()); } } } Logger configuration from code If necessary, the logger can be configured from code and not from application.json. This can be done in Program.cs in the CreateHostBuilder method. By default, it looks like this.\npublic static IHostBuilder CreateHostBuilder(string[] args) =\u0026gt; Host.CreateDefaultBuilder(args) .ConfigureWebHostDefaults(webBuilder =\u0026gt; { webBuilder.UseStartup\u0026lt;Startup\u0026gt;(); }); To configure the logger to CreateDefaultBuilder you need to add ConfigureLogging\npublic static IHostBuilder CreateHostBuilder(string[] args) =\u0026gt; Host.CreateDefaultBuilder(args) .ConfigureLogging(logger =\u0026gt; { logger.ClearProviders(); logger.SetMinimumLevel(LogLevel.Information); logger.AddConsole(); }) .ConfigureWebHostDefaults(webBuilder =\u0026gt; { webBuilder.UseStartup\u0026lt;Startup\u0026gt;(); }); logger.ClearProviders() - clears all previous settings. And then you can immediately add another log provider, such as Seriolog or any other. logger.SetMinimumLevel(LogLevel.Information) - indicates the minimum level log. logger.AddConsole() - adds log output to the console.\n","permalink":"https://mpostument.com/posts/programming/dotnet/blazor-server-logging/","summary":"Hi!\nToday we will see how to configure logging in BlazorServer. Immediately after creating a project, the appsettings.json file will be in the root of the project.\n{ \u0026#34;Logging\u0026#34;: { \u0026#34;LogLevel\u0026#34;: { \u0026#34;Default\u0026#34;: \u0026#34;Information\u0026#34;, \u0026#34;Microsoft\u0026#34;: \u0026#34;Warning\u0026#34;, \u0026#34;Microsoft.Hosting.Lifetime\u0026#34;: \u0026#34;Information\u0026#34; } }, \u0026#34;AllowedHosts\u0026#34;: \u0026#34;*\u0026#34; } This is where logging settings are set. LogLevel starts with Default which have value Information. This means that the default log level will be Information. Next is Microsoft with the Warning level, respectively for namespace Microsoft the login level will be Warning.","title":"Blazor Server Logging"},{"content":"Hi!\nToday we will talk about how dependency injection works in blazor server. If you are familiar with dependency injection in asp.net core then there are no differences.\nDependency injection is configured in Startup.cs at the root of the project. After creating the project, it will look like this\npublic void ConfigureServices(IServiceCollection services) { services.AddRazorPages(); services.AddServerSideBlazor(); services.AddSingleton\u0026lt;WeatherForecastService\u0026gt;(); } Let\u0026rsquo;s see what happens here. services.AddRazorPages() - adds razor pages support to the project services.AddServerSideBlazor() - adds blazor server support to the project services.AddSingleton \u0026lt;WeatherForecastService\u0026gt;() - registers the WeatherForecastService class in DI.\nWeatherForecastService in the example is added as Singleton. Other ways to add is Scoped and Transient. We will look at them in more detail below.\nLet\u0026rsquo;s try to add a new class in DI and check how it works with different types (Singleton, Scoped, Transient).\nTo do this, in the Data folder, create a file that will generate a random number.\nusing System; namespace BlazorLearn.Data { public class RandomData { private readonly int randomData; public RandomData() { Random random = new Random(); randomData = random.Next(); } public int GetRandomData() { return randomData; } } } In the RandomData class, a random number is generated in the constructor. And the GetRandomData method returns it. In order to use a class in DI you need to create an interface. You can use it without interface. But thanks to the interface, you can easily replace this class with another implementation. This is how interface will look like.\nnamespace BlazorLearn.Data { public interface IRandomData { int GetRandomData(); } } Singleton The class can now be registered in DI. Let\u0026rsquo;s start with Singleton. To do this in the Startup.cs file in the method ConfigureServices you need to add:\nservices.AddSingleton\u0026lt;IRandomData, RandomData\u0026gt;(); Now every time someone uses the IRandomData interface, RandomData will be called. Now in index.razor I will call the GetRandomNumber method.\n@page \u0026#34;/\u0026#34; @using BlazorLearn.Data @inject IRandomData RandomData \u0026lt;h1\u0026gt;Hello, world!\u0026lt;/h1\u0026gt; Welcome to your new app. Your random number is @RandomData.GetRandomData() \u0026lt;SurveyPrompt Title=\u0026#34;How is Blazor working for you?\u0026#34; /\u0026gt; @inject IRandomData RandomData call constructor of our class.\nStart application. If I open the application in another browser or refresh page the random number will remain the same. This is how singleton works. The object is created once at the start of the application.\nScoped Let\u0026rsquo;s see now how scoped works. Replace AddSingleton with AddScoped\nservices.AddScoped\u0026lt;IRandomData, RandomData\u0026gt;(); And run the application. Now if I go to another page of the site and then go back the number will remain the same. But if I refresh the page, the number will change. Scoped objects are created once for each query.\nTransient And the last type is Transient. Now replace AddScoped with AddTransient\nservices.AddTransient\u0026lt;IRandomData, RandomData\u0026gt;(); Let\u0026rsquo;s run application. And now even if you switch from one tab to another the number changes every time. DI in Class Finally, let\u0026rsquo;s see how you can use the RandomData class in another class with DI. For example, take Data/WeatherForecastService.cs. In this class you need to make a variable of type IRandomData and initialize it in the constructor. And now it can be used anywhere in this class.\nusing System; using System.Linq; using System.Threading.Tasks; namespace BlazorLearn.Data { public class WeatherForecastService { private IRandomData _randomData; private static readonly string[] Summaries = new[] { \u0026#34;Freezing\u0026#34;, \u0026#34;Bracing\u0026#34;, \u0026#34;Chilly\u0026#34;, \u0026#34;Cool\u0026#34;, \u0026#34;Mild\u0026#34;, \u0026#34;Warm\u0026#34;, \u0026#34;Balmy\u0026#34;, \u0026#34;Hot\u0026#34;, \u0026#34;Sweltering\u0026#34;, \u0026#34;Scorching\u0026#34; }; public WeatherForecastService(IRandomData randomData) { _randomData = randomData; } public Task\u0026lt;WeatherForecast[]\u0026gt; GetForecastAsync(DateTime startDate) { var rng = new Random(); return Task.FromResult(Enumerable.Range(1, 5).Select(index =\u0026gt; new WeatherForecast { Date = startDate.AddDays(index), TemperatureC = _randomData.GetRandomData(), Summary = Summaries[rng.Next(Summaries.Length)] }).ToArray()); } } } _randomData.GetRandomData() result I will write in variable TemperatureC.Start application again. And on the FetchData page, the temperature will be generated using GetRandomData. It is important that DI WeatherForecastService and RandomData are added equally.\nservices.AddScoped\u0026lt;WeatherForecastService\u0026gt;(); services.AddScoped\u0026lt;IRandomData, RandomData\u0026gt;(); ","permalink":"https://mpostument.com/posts/programming/dotnet/blazor-server-dependency-injection/","summary":"Hi!\nToday we will talk about how dependency injection works in blazor server. If you are familiar with dependency injection in asp.net core then there are no differences.\nDependency injection is configured in Startup.cs at the root of the project. After creating the project, it will look like this\npublic void ConfigureServices(IServiceCollection services) { services.AddRazorPages(); services.AddServerSideBlazor(); services.AddSingleton\u0026lt;WeatherForecastService\u0026gt;(); } Let\u0026rsquo;s see what happens here. services.AddRazorPages() - adds razor pages support to the project services.","title":"Blazor Dependency Injection"},{"content":"Hello!\nToday we will look at how you can change the error messages in Blazor Server. There are two types of errors page does not exist and exception. You can change the message that will be displayed in both cases. The first thing to consider is how to change the message displayed to the user in the case of exception.\nException Error In the Pages/_Host.cshtml file, this section is responsible for displaying the error:\n\u0026lt;div id=\u0026#34;blazor-error-ui\u0026#34;\u0026gt; \u0026lt;environment include=\u0026#34;Staging,Production\u0026#34;\u0026gt; An error has occurred. This application may no longer respond until reloaded. \u0026lt;/environment\u0026gt; \u0026lt;environment include=\u0026#34;Development\u0026#34;\u0026gt; An unhandled exception has occurred. See browser dev tools for details. \u0026lt;/environment\u0026gt; \u0026lt;a href=\u0026#34;\u0026#34; class=\u0026#34;reload\u0026#34;\u0026gt;Reload\u0026lt;/a\u0026gt; \u0026lt;a class=\u0026#34;dismiss\u0026#34;\u0026gt;🗙\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; Depending on environment the required error is displayed. environment is determined by the environment variable ASPNETCORE_ENVIRONMENT if it is not specified then environment corresponds to Production. During development by default in Properties\\launchSettings.json it is specified as Development.\n{ \u0026#34;BlazorLearn\u0026#34;: { \u0026#34;commandName\u0026#34;: \u0026#34;Project\u0026#34;, \u0026#34;launchBrowser\u0026#34;: true, \u0026#34;applicationUrl\u0026#34;: \u0026#34;https://localhost:5001;http://localhost:5000\u0026#34;, \u0026#34;environmentVariables\u0026#34;: { \u0026#34;ASPNETCORE_ENVIRONMENT\u0026#34;: \u0026#34;Development\u0026#34; } } } Now I will change the message for the Development environment and get the following message. Page not found error The message that is displayed when the page is not found is in the file App.razor.\n\u0026lt;NotFound\u0026gt; \u0026lt;LayoutView Layout=\u0026#34;@typeof(MainLayout)\u0026#34;\u0026gt; \u0026lt;p\u0026gt;Sorry, there\u0026#39;s nothing at this address.\u0026lt;/p\u0026gt; \u0026lt;/LayoutView\u0026gt; \u0026lt;/NotFound\u0026gt; There can be both a message and an entire html page. Replacing the text of the message I get: ","permalink":"https://mpostument.com/posts/programming/dotnet/blazor-server-error-handling/","summary":"Hello!\nToday we will look at how you can change the error messages in Blazor Server. There are two types of errors page does not exist and exception. You can change the message that will be displayed in both cases. The first thing to consider is how to change the message displayed to the user in the case of exception.\nException Error In the Pages/_Host.cshtml file, this section is responsible for displaying the error:","title":"Blazor Server Error Handling"},{"content":"Hello! Today I want to tell you how routing works in Blazor Server. Routing starts in the Startup.cs file. The following code will responsible for it\napp.UseRouting(); app.UseEndpoints(endpoints =\u0026gt; { endpoints.MapBlazorHub(); endpoints.MapFallbackToPage(\u0026#34;/_Host\u0026#34;); }); app.UseRouting(); came from asp.net and adds route matching to the middleware pipeline. This middleware looks at the set of endpoints defined in the app, and selects the best match based on the request. app.UseEndpoints() adds endpoint execution to the middleware pipeline. It runs the delegate associated with the selected endpoint. In the middle of app.UseEndpoints() indicates endpoints.MapBlazorHub() here is set up SignalR routing. Setup is done automatically and you do not need to change anything here. The next element is endpoints.MapFallbackToPage(\u0026quot;/_ Host\u0026quot;) a backup route, it has the lowest priority, called in the case that no other more suitable routes are found. The parameters for MapFallbackToPage specify /_Host. This is the file to which it will be redirected. This file is located under Pages folder _Host.cshtml. It looks like regular html with some differences. The beginning of the file is indicated\n@page \u0026#34;/\u0026#34; @namespace BlazorLearn.Pages @addTagHelper *, Microsoft.AspNetCore.Mvc.TagHelpers @{ Layout = null; } @page \u0026quot;/\u0026quot; indicate that this file will be opened if the url is accessed /\nThe body renders the pages\n\u0026lt;app\u0026gt; \u0026lt;component type=\u0026#34;typeof(App)\u0026#34; render-mode=\u0026#34;ServerPrerendered\u0026#34; /\u0026gt; \u0026lt;/app\u0026gt; Here it is specified that we render and in what mode. ServerPrerendered renders first on the server and then displays to the client. App points to another App.razor file. This file determines which layout to display. If routeData exists corresponding page displayed, if this page has no layout then MainLayout will be used. If routeData does not exist then MainLayout and the text Sorry, there's nothing at this address.\n\u0026lt;Router AppAssembly=\u0026#34;@typeof(Program).Assembly\u0026#34;\u0026gt; \u0026lt;Found Context=\u0026#34;routeData\u0026#34;\u0026gt; \u0026lt;RouteView RouteData=\u0026#34;@routeData\u0026#34; DefaultLayout=\u0026#34;@typeof(MainLayout)\u0026#34; /\u0026gt; \u0026lt;/Found\u0026gt; \u0026lt;NotFound\u0026gt; \u0026lt;LayoutView Layout=\u0026#34;@typeof(MainLayout)\u0026#34;\u0026gt; \u0026lt;p\u0026gt;Sorry, there\u0026#39;s nothing at this address.\u0026lt;/p\u0026gt; \u0026lt;/LayoutView\u0026gt; \u0026lt;/NotFound\u0026gt; \u0026lt;/Router\u0026gt; In the blazor server project there are several pages created initially. These are Counter, FetchData and Index. Each of them has a @page directive at the beginning. For counter it will be @page \u0026quot;/counter\u0026quot; for FetchData it will be @page \u0026quot;/fetchdata\u0026quot; and for Index @page \u0026quot;/\u0026quot;. So if you open any of these endpoints in the browser, the corresponding page will be opened. Multiple @page values ​​can be specified for a single page. For example for Counter you can add\n@page \u0026#34;/counter\u0026#34; @page \u0026#34;/count\u0026#34; And now the Counter page will open for both urls. You can also pass an initial value to Counter. Now when the pages are open it is always zero. To do this, add another @page that will look like @page \u0026quot;/counter/{InitialValue: int}\u0026quot; you can also add the same for count page. InitialValue will be a public parameter of type int. This will allow you to specify localhost:5001/counter/123 in the url and counter will start from value 123. Next you need to write the code that will set this value. In the @code section you need to add a parameter with the same name public int InitialValue {get; set; } this parameter must have the attribute [Parameter]. In order to pass the value of InitialValue your need to call the method OnParametersSet.\n@page \u0026#34;/counter\u0026#34; @page \u0026#34;/count\u0026#34; @page \u0026#34;/counter/{InitialValue:int}\u0026#34; @page \u0026#34;/count/{InitialValue:int}\u0026#34; \u0026lt;h1\u0026gt;Counter\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Current count: @currentCount\u0026lt;/p\u0026gt; \u0026lt;button class=\u0026#34;btn btn-primary\u0026#34; @onclick=\u0026#34;IncrementCount\u0026#34;\u0026gt;Click me\u0026lt;/button\u0026gt; @code { [Parameter] public int InitialValue {get; set; } private int currentCount = 0; protected override void OnParametersSet() { base.OnParametersSet(); currentCount = InitialValue; } private void IncrementCount() { currentCount++; } } And the result will be as follows This post is based on a course from Tim Corey - Blazor Server: In Depth\n","permalink":"https://mpostument.com/posts/programming/dotnet/blazor-server-routing/","summary":"Hello! Today I want to tell you how routing works in Blazor Server. Routing starts in the Startup.cs file. The following code will responsible for it\napp.UseRouting(); app.UseEndpoints(endpoints =\u0026gt; { endpoints.MapBlazorHub(); endpoints.MapFallbackToPage(\u0026#34;/_Host\u0026#34;); }); app.UseRouting(); came from asp.net and adds route matching to the middleware pipeline. This middleware looks at the set of endpoints defined in the app, and selects the best match based on the request. app.UseEndpoints() adds endpoint execution to the middleware pipeline.","title":"Blazor Server Routing"},{"content":"Hi there!\nI want to talk today about DLP Nightfall. The Nightfall DLP Action scans your code commits upon Pull Request for sensitive information - like credentials \u0026amp; secrets, PII, credit card numbers \u0026amp; more - and posts review comments to your code hosting service automatically. The Nightfall DLP Action is intended to be used as a part of your CI to simplify the development process, improve your security, and ensure you never accidentally leak secrets or other sensitive information via an accidental commit. I tried to implement DLP with Github Actions and want to share my results\nGithub Action Nightfall gives GitHub Action which can be used to scan Pull Request and Push in the branch. First you need to create a .github\\workflows folder in the root of the repository and create a dlp.yml file in this folder. The file name can be any.\nname: nightfalldlp on: push: branches: - master pull_request: jobs: run-nightfalldlp: name: nightfalldlp runs-on: ubuntu-latest steps: - name: Checkout Repo Action uses: actions/checkout@v2 - name: nightfallDLP action step uses: nightfallai/nightfall_dlp_action@v0.0.7 env: NIGHTFALL_API_KEY: ${{ secrets.NIGHTFALL_API_KEY }} GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} EVENT_BEFORE: ${{ github.event.before }} In the on section it is specified that to start scanning on push in master branch and on Pull Request. Action has two steps, the first is to clone the repository the second is to run the scanner. Several env variables are required for the scanner to work. GITHUB_TOKEN is used so that the scanner can add comments to the PR it should have value secrets.GITHUB_TOKEN EVENT_BEFORE is required if the scanner starts at a push event. And its value must be github.event.before NIGHTFALL_API_KEY the last required variable, you can get it by registering at nightfall.ai. Once NIGHTFALL_API_KEY is received it needs to be added to GitHub Secrets Nightfall Configuration Now when i finished with GitHub Action I need to create a configuration file for Nightfall. To do this, create a folder named .nightfalldlp and the file config.json in it. List of detectors that support nightfall.\n{ \u0026#34;detectors\u0026#34;: [ \u0026#34;CREDIT_CARD_NUMBER\u0026#34;, \u0026#34;PHONE_NUMBER\u0026#34;, \u0026#34;API_KEY\u0026#34;, \u0026#34;CRYPTOGRAPHIC_KEY\u0026#34;, \u0026#34;RANDOMLY_GENERATED_TOKEN\u0026#34;, \u0026#34;US_SOCIAL_SECURITY_NUMBER\u0026#34;, \u0026#34;AMERICAN_BANKERS_CUSIP_ID\u0026#34;, \u0026#34;US_BANK_ROUTING_MICR\u0026#34;, \u0026#34;ICD9_CODE\u0026#34;, \u0026#34;ICD10_CODE\u0026#34;, \u0026#34;US_DRIVERS_LICENSE_NUMBER\u0026#34;, \u0026#34;US_PASSPORT\u0026#34;, \u0026#34;EMAIL_ADDRESS\u0026#34;, \u0026#34;IP_ADDRESS\u0026#34; ] } My configuration file looks shorter because the full one generates a lot of false-positive alarms. Example Example of my file:\n{ \u0026#34;detectors\u0026#34;: [ \u0026#34;API_KEY\u0026#34;, \u0026#34;CRYPTOGRAPHIC_KEY\u0026#34;, \u0026#34;RANDOMLY_GENERATED_TOKEN\u0026#34;, \u0026#34;EMAIL_ADDRESS\u0026#34;, \u0026#34;IP_ADDRESS\u0026#34; ] } Now it\u0026rsquo;s all you need to push configuration for scanner to start. Even with this configuration, the scanner gives a lot of false-positive alarm results For example, I get such a error Suspicious content detected (3:***, type IP_ADDRESS) on \u0026quot;arn:aws:s3:::usershome/{{name}}-{{data}}/*\u0026quot; Or Suspicious content detected (ke********, type RANDOMLY_GENERATED_TOKEN) on \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:kms:us-east-1:000000000:key/6a2b4d78-oy69-44ab-ce3d-43faca87fd14\u0026quot;. Despite the false-positive part, the scanner allows you to find cluttered passwords and api keys\n","permalink":"https://mpostument.com/posts/programming/ci/nightfall-dlp-configuration/","summary":"Hi there!\nI want to talk today about DLP Nightfall. The Nightfall DLP Action scans your code commits upon Pull Request for sensitive information - like credentials \u0026amp; secrets, PII, credit card numbers \u0026amp; more - and posts review comments to your code hosting service automatically. The Nightfall DLP Action is intended to be used as a part of your CI to simplify the development process, improve your security, and ensure you never accidentally leak secrets or other sensitive information via an accidental commit.","title":"Nightfall Dlp Configuration"},{"content":"Hi there!\nToday I want to show how you can deploy code on AWS EC2 using CodeShip Basic. To do this, I will use the integration of Codeship with AWS CodeDeploy.\nCodeShip Configuration First you need to add a repository to CodeShip and go to the Deploy page Add a branch from which code will be deployed to ec2. In my case, the branch is master. Once the branch is added you need to scroll down to the Deployment section. And select CodeDeploy The following form will appear immediately Here you need to enter the Access and Secret Keys of the AWS user who will perform the deployment. The user must have access to s3 and CodeDeploy. In the region, specify your AWS Region where the application is located. For me it\u0026rsquo;s us-east-1. Application, GroupName will need to be created in AWS. The value should be chosen so that it is clear which application will be deployed. S3 Bucket can be as a general bucket where all deployments will be carried out or choose an individual name for one application. In my case it\u0026rsquo;s CodeShipEc2Deployment. And last is Config Name you need to specify the configuration name in CodeDeploy. I will take the standard CodeDeployDefault.AllAtOnce. This means that the deployment will occur at once on all ec2 instances\nNow with CodeShip configuration over and you can start setting up AWS.\nCodeDeploy Configuration Open CodeDeploy service in AWS. In the Deployment section, find Applications Click Create Application. Enter the same name as in Codeship in the Application section. Select EC2 as the Compute Platform Open the newly created Application and in Deployment Groups select Create Deployment Group In Deployment group name it is necessary to enter the same name which was specified in CodeShip. In ServiceRole choose a role that will allow to carry out deployment on ec2. A good example of the role can be found here.\nIn Environment configuration, select ec2. Then you need to specify which tag to use for the ec2 filter. For example it can be Key = Name, Value = ProductionApplication. Here you can use any tags that are on your ec2. Ec2 Configuration Another role will be required for ec2. What accesses it should have can be found here\nOnce the role is created, it must be added to the ec2 instance. To do this, select the desired ec2. Right-click and select Attach Role Find the role you created above in the list. Now you need to put the CodeDeploy Agent on the ec2 instance. To do this, connect to the server via ssh ssh -i private_key ec2-user@IP. In my example i am using AmazonLinux. If you have another OS commands can be found here.\nDownload the installation wget https://aws-codedeploy-us-east-1.s3.us-east-1.amazonaws.com/latest/install Provide permissions sudo chmod + x install Install agent sudo ./install auto You can check if the agent is running with the sudo service codedeploy-agent status command. The result should be similar to The AWS CodeDeploy agent is running as PID 32466. You also need to create a folder that will contain the application mkdir /opt/application and give permission to the user. I use ec2-user chown ec2-user:ec2-user /opt/application\nDeployment Script At the root of the repository you need to create a file called appspec.yml. Here you can find all options supported by this file - AWS.\nHere is an example of my file:\nversion: 0.0 os: linux files: - source: / destination: /opt/application permissions: - object: /opt/application owner: ec2-user group: ec2-user hooks: AfterInstall: - location: ops/deploybuild.sh runas: ec2-user version has a default value and does not need to be changed. 0.0 The only value is supported. os can have two values windows or linux, my os is EC2 AmazonLinux, so I choose linux. files indicates which files need to be copied to the server during deployment. source: / means copy all files. destination is where to copy. The permissions specifies which permissions should have the files just copied. In the hooks section is the application configuration. I use the AfterInstall hook. This means that the script will run after the Install step. During the Install step, the files are copied to the server. The location specifies which script to run and from its user. runas means that the script will be called on behalf of the user. By default, the code-deploy agent is started from the root and runas at this stage will switch to ec2-user. You can also change the code-deploy agent to run immediately from ec2-user.\nNext we need a deployment script. In the repository, I created the ops folder because the path to my script is - location: ops/deploybuild.sh. And in this folder I created a script deploybuild.sh. I have a node.js application for which you need to do yarn install and npm start. I will add it to the script\n#!/usr/bin/env bash cd /opt/application yarn --ignore-engines cd examples/demo-app/ (npm run start-prod)\u0026amp; This is very basic script to start application. For production is better to use pm2 package to start js application. Now after every push to master branch code will be deployed.\n","permalink":"https://mpostument.com/posts/programming/ci/deploy-to-ec2-with-codeship/","summary":"Hi there!\nToday I want to show how you can deploy code on AWS EC2 using CodeShip Basic. To do this, I will use the integration of Codeship with AWS CodeDeploy.\nCodeShip Configuration First you need to add a repository to CodeShip and go to the Deploy page Add a branch from which code will be deployed to ec2. In my case, the branch is master. Once the branch is added you need to scroll down to the Deployment section.","title":"Deploy to ec2 with codedship"},{"content":"Hi there!\nToday I want to tell you how with go you can get a list of all AWS Lambda that are in the VPC. First you need to install golang. The latest version can be downloaded from the official website.\nOnce golang is installed, create a folder for the scripts mkdir awsscripts\nNext you need to initialize the go.mod file. This file stores all the dependencies on external packages, and at the same time will make the go code module. And in the future it can be installed with go get. To create a module you need to call the command go mod init github.com/mpostument/awsscripts. Where github\u0026hellip; is the name of the module. If you use another version control system, it could be it, then the user name and module name.\nOnce the module is created we will need aws-sdk to work with aws. You can install it with the command go get github.com/aws/aws-sdk-go@latest. Run it in the directory with go.mod. After that in go mod the version aws-sdk will be added require github.com/aws/aws-sdk-go v1.34.14. In the future, to update the version of aws-sdk. Manually change the version in this file and run go mod tidy.\nThe setting is complete and you can go to the code. Create a lambdaInVpc.go file in the root of the awsscripts folder or in any other folder inside of the awsscripts and open it.\nEach go file starts with package. Since we need to run the go file directly instead of importing package should be main.\nYou also need to add a list of third-party modules that we will use\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/aws/session\u0026#34; \u0026#34;github.com/aws/aws-sdk-go/service/lambda\u0026#34; ) The list of imports is followed by the main method, which is the entry point into the code. In main we will create an aws client.\nfunc main() { mySession := session.Must(session.NewSession()) client := lambda.New(mySession) lambdas := getLambdaFunctions(client) } This creates an aws session and a lambda client, and method getLambdaFunctions executed. As parameter it take newly created client. Now let\u0026rsquo;s start writing getLambdaFunctions.\nfunc getLambdaFunctions(client *lambda.Lambda) []*lambda.FunctionConfiguration { input := \u0026amp;lambda.ListFunctionsInput{} var result []*lambda.FunctionConfiguration err := client.ListFunctionsPages(input, func(page *lambda.ListFunctionsOutput, lastPage bool) bool { result = append(result, page.Functions...) return !lastPage }) if err != nil { log.Fatal(\u0026#34;Not able to get lambdas\u0026#34;, err) return nil } return result } Now let\u0026rsquo;s look at what\u0026rsquo;s going on here. To get the list of lambdas it is necessary to call method ListFunctions. But since this method returns only the first 50 lambdas, it does not suit us. Because if there are 51 or more lambdas on aws, the account will still return only the first 50. But aws has a method with pagination ListFunctionsPages, and it can be used here. The method takes two parameters, the first search parameter ListFunctionsInput in which you can specify the region, version and several other parameters. And the second is the method of pagination.\nIn the beginning we create two variables input of type ListFunctionsInput and result of type[]*lambda.FunctionConfiguration. Input will be passed to the ListFunctionsPages function and result will be used to store the execution result.\nThen comes the call of ListFunctionsPages. This method returns only an error, the result of this method is assigned to the variable err and then there is a check whether this varialbe is not equal to nil. If the method returned nil then the program is terminated and the error is displayed.\nIn the function ListFunctionsPages there is a call of one more function which adds result of execution in a slice result. The internal function is called several times until we reach the last page. And in the end we return result.\nNow that you have received a list of all lambdas, you need to filter out those in the VPC. For this purpose we will return again to the main method and to add there iterations on all lambdas with a condition.\nfor _, l := range lambdas { if l.VpcConfig != nil \u0026amp;\u0026amp; len(l.VpcConfig.SubnetIds) \u0026gt; 0 { fmt.Println(*l.FunctionName) } } Here we iterate on result of execution of getLambdaFunctions and we will check up whether VpcConfig of function is not equal nil and whether the list of subnet is more than zero. Two checks are necessary because for some lambdas VpcConfig will be nil, and for others there will be a map in which there will be empty lists. Example\n\u0026lt;nil\u0026gt; \u0026lt;nil\u0026gt; { SecurityGroupIds: [], SubnetIds: [], VpcId: \u0026#34;\u0026#34; } \u0026lt;nil\u0026gt; Now you can run the code and get the result. go run lambdaInVpc.go\n","permalink":"https://mpostument.com/posts/programming/golang/get-lambdas-in-vpc-with-go/","summary":"Hi there!\nToday I want to tell you how with go you can get a list of all AWS Lambda that are in the VPC. First you need to install golang. The latest version can be downloaded from the official website.\nOnce golang is installed, create a folder for the scripts mkdir awsscripts\nNext you need to initialize the go.mod file. This file stores all the dependencies on external packages, and at the same time will make the go code module.","title":"Get Lambdas in Vpc With Go"},{"content":"Hello!\nToday I want to show how you can build and deploy .net core with Travis. To get started, you need to create a Travis account. This requires you to login with your GitHub account. .travis.yml Configuration The next step is to create a .travis.yml file in the root of the repository. Example of my configuration file:\nlanguage: csharp mono: none dotnet: 3.1.4 solution: awstaghelper.sln install: - dotnet restore script: - dotnet build --configuration Release --runtime win-x64 --output win-x64 - dotnet build --configuration Release --runtime win-x86 --output win-x86 - dotnet build --configuration Release --runtime linux-x64 --output linux-x64 - dotnet build --configuration Release --runtime osx-x64 --output osx-x64 before_deploy: - zip -r9 win-x64.zip win-x64 - zip -r9 win-x86.zip win-x86 - zip -r9 linux-x64.zip linux-x64 - zip -r9 osx-x64.zip osx-x64 deploy: provider: releases api_key: $api_key file: - win-x64.zip - win-x86.zip - linux-x64.zip - osx-x64.zip skip_cleanup: true on: tags: true .Net configuration Now let\u0026rsquo;s review the config. Yml begins with the declaration of the programming language language: csharp. mono is none because the .net core is used. And the last is the .net core version. In my case it is dotnet: 3.1.4. You also need to specify the name of the solution solution: awstaghelper.sln.\nBuild configuration Next are the build steps. Step install used to install the necessary dependencies. dotnet restore is optional as in the latest versions of dotnet restore the command is automatically called when build. Because of this, install can be completely removed.\nIn the script section there is a build of application. I call the build command 4 times because I build for different OS: Windows, Linux, Mac. The operating system is indicated by the --runtime key. A list of all runtime is available here. --output key specifies in which directory to put binaries. You can also add the /p:PublishSingleFile=true key to get only one exe file as a result per OS. You can also add this key in the project settings in Visual Studio.\nDeployment configuration The before_deploy section is called before the deployment to prepare the binary for release. At this stage, I make an archive with the binaries that I received after the builds for the each operating system.\nIn the deploy section I use the releases provider which releases on github. List of all providers - docs.travis-ci.com. A github token is required for deployment. You can generate it on GitHub. The key must be saved in the travis configuration of the project. To do this, go to the project on Travis. Open project/settings: .\nIn Environment Variables you need to add the generated key: In file you need to specify which files should be deployed using the provider. skip_cleanup: true must be true so that travis does not clear whipped files before deployment. on.tags: true means that the deployment run only on github tags.\nOnce the file is ready you need to make run changes to the master and the build will start automatically. But the deployment will not take place, because it trigger only on tags. On GitHub you need to create release for the desired repository. Once the release is done the build will run again, but this time with a deployment.\nIn Travis you can see the following result: ","permalink":"https://mpostument.com/posts/programming/dotnet/build-dotnet-core-with-travis/","summary":"Hello!\nToday I want to show how you can build and deploy .net core with Travis. To get started, you need to create a Travis account. This requires you to login with your GitHub account. .travis.yml Configuration The next step is to create a .travis.yml file in the root of the repository. Example of my configuration file:\nlanguage: csharp mono: none dotnet: 3.1.4 solution: awstaghelper.sln install: - dotnet restore script: - dotnet build --configuration Release --runtime win-x64 --output win-x64 - dotnet build --configuration Release --runtime win-x86 --output win-x86 - dotnet build --configuration Release --runtime linux-x64 --output linux-x64 - dotnet build --configuration Release --runtime osx-x64 --output osx-x64 before_deploy: - zip -r9 win-x64.","title":"Build Dotnet Core With Travis"},{"content":"About project Tags are critical to managing AWS resources at scale. AWSTagHelper provides a command line tool to ease adding and managing tags to and from CSV files across the wide range of AWS resources.\nInstall Download the latest binary from releases\nDocs https://github.com/mpostument/awstaghelper/blob/master/README.MD\n","permalink":"https://mpostument.com/posts/projects/awstaghelper/","summary":"About project Tags are critical to managing AWS resources at scale. AWSTagHelper provides a command line tool to ease adding and managing tags to and from CSV files across the wide range of AWS resources.\nInstall Download the latest binary from releases\nDocs https://github.com/mpostument/awstaghelper/blob/master/README.MD","title":"AWSTagHelper"},{"content":"About project Scrape your steam wishlist in text format\nInstall Download the latest binary from releases\nDocs https://github.com/mpostument/SteamWishlistScraper/blob/master/README.MD\n","permalink":"https://mpostument.com/posts/projects/steamwishlistscraper/","summary":"About project Scrape your steam wishlist in text format\nInstall Download the latest binary from releases\nDocs https://github.com/mpostument/SteamWishlistScraper/blob/master/README.MD","title":"SteamWishlistScraper"}]